{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-0.7314 -0.0077\n",
       " 1.6915  1.6529\n",
       " 0.1061 -0.3005\n",
       "       ⋮        \n",
       "-0.7322 -0.9411\n",
       " 1.0942 -1.2787\n",
       " 1.3752 -0.1952\n",
       "[torch.FloatTensor of size 1000x2]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def disk(nb_points) :\n",
    "    radius = 1/np.sqrt(2*np.pi)\n",
    "    inp = Tensor(nb_points,2).uniform_(0,1)\n",
    "    ratio = torch.floor(torch.norm(inp,p=2,dim=1)/radius)\n",
    "    target = 1-torch.clamp(ratio,min=0,max=1)\n",
    "    return inp, target\n",
    "\n",
    "nb_points = 1000\n",
    "# create train set and respective labels\n",
    "train_input , train_target = disk(nb_points)\n",
    "# create test set and respective labels\n",
    "test_input , test_target = disk(nb_points)\n",
    "\n",
    "#print(train_input, train_target, test_input, test_target)\n",
    "\n",
    "# data normalization\n",
    "mu_train , std_train = train_input.mean() , train_input.std()\n",
    "train_input.sub_(mu_train).div_(std_train)\n",
    "mu_test , std_test = test_input.mean() , test_input.std()\n",
    "test_input.sub_(mu_test).div_(std_test)\n",
    "# later see whether it is convenient to normalize component-wise with broadcasting (pag 66 week5)\n",
    "\n",
    "# then do sanity check to see if the two classes are balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module ( object ) :\n",
    "    def forward ( self , * input ) :\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward ( self , * gradwrtoutput ) :\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def param ( self ) :\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return x.tanh()\n",
    "\n",
    "def dtanh(x):\n",
    "    return 1-torch.tanh(x).pow(2)\n",
    "\n",
    "\n",
    "class Sigma(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self,input):\n",
    "        self.input = input\n",
    "        return tanh(input)\n",
    "    \n",
    "    def backward(self,output):\n",
    "        return dtanh(self.input)*output\n",
    "    \n",
    "    # here you need to add \"def param\" too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return max(0,x)\n",
    "\n",
    "def drelu(x):\n",
    "    if x>0:\n",
    "        out = 1\n",
    "    elif x<0:\n",
    "        out = 0\n",
    "    return out\n",
    "\n",
    "class Relu(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,input):\n",
    "        self.input = input\n",
    "        return relu(input)\n",
    "    \n",
    "    def backward(self,output):\n",
    "        return drelu(output) #to check, probably incorrect\n",
    "    \n",
    "    # here you need to add \"def param\" too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(v, t):\n",
    "    return (v - t).pow(2).sum()\n",
    "\n",
    "def dloss(v, t):\n",
    "    return 2 * (v - t)\n",
    "\n",
    "class Loss(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        self.input = input\n",
    "        return loss(input, target)\n",
    "    \n",
    "    def backward(self, output):\n",
    "        return dloss(output,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of Linear\n",
    "class Linear(Module):\n",
    "    # in_features: size of each input sample\n",
    "    # out_features: size of each output sample\n",
    "    # bias: If set to False, the layer will not learn an additive bias. Default: ``True``\n",
    "\n",
    "    # Attributes:\n",
    "    # weight: the learnable weights of the module of shape (out_features x in_features)`\n",
    "    # bias:   the learnable bias of the module of shape `(out_features)`\n",
    "\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weights = Tensor(out_features,in_features).normal_(0,1e-6)\n",
    "        \n",
    "    def forward(self,input):\n",
    "        self.input = input\n",
    "        #print('ecco i pesi:',self.weights)\n",
    "        self.s = torch.mv(self.weights,input)\n",
    "        return torch.mv(self.weights,input)\n",
    "    \n",
    "    def backward_last(self, output, target, dl_dw, dl_db, x_m): # output would be x3 \n",
    "        dl_dx = dloss(output, target)\n",
    "        dl_ds = Sigma().backward(self.s)*dl_dx\n",
    "        dl_dw.add_(dl_ds.view(-1, 1).mm(x_m.view(1, -1)))  \n",
    "        dl_db.add_(dl_ds)\n",
    "        return dl_dw, dl_db, dl_ds\n",
    "    \n",
    "    def backward(self, dl_dw, dl_db, w_next, dl_ds_next, x_m):  \n",
    "        # serve output negli argomenti, e target?? penso di no perchè li metto nel backward_last e poi non servono\n",
    "        dl_dx = w_next.t().mv(dl_ds_next)\n",
    "        dl_ds = Sigma().backward(self.s)*dl_dx\n",
    "        dl_dw.add_(dl_ds.view(-1, 1).mm(x_m.view(1, -1)))   \n",
    "        dl_db.add_(dl_ds)\n",
    "        return dl_dw, dl_db, dl_ds      \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network structure QUI LA RISCRIVO PASSO PASSO PER VERIFICARE CHE FUNZIONI OGNI STEP\n",
    "class Net(Module):\n",
    "    def __init__(self):\n",
    "        self.fc1 = Linear(input_size,hidden_units)  # first hidden layer\n",
    "        self.fc2 = Linear(hidden_units,hidden_units) # second hidden layer\n",
    "        self.fc3 = Linear(hidden_units,output_size) # third hidden layer\n",
    "    \n",
    "    \"\"\"def forward(self,x):       # this is my original version\n",
    "        x = self.fc1.forward(x)\n",
    "        print('fc1 forward',x)\n",
    "        x = Sigma().forward(x)\n",
    "        print('sigma 1st time',x)\n",
    "        x = self.fc2.forward(x)\n",
    "        print('fc2 forward',x)\n",
    "        x = Sigma().forward(x)\n",
    "        print('sigma 2nd time',x)\n",
    "        x = self.fc3.forward(x)\n",
    "        print('fc3 forward',x)\n",
    "        return x\"\"\"\n",
    "    def forward(self,x):\n",
    "        s1 = self.fc1.forward(x)\n",
    "        print('fc1 forward',x)\n",
    "        x1 = Sigma().forward(s1)\n",
    "        print('sigma 1st time',x)\n",
    "        s2 = self.fc2.forward(x1)\n",
    "        print('fc2 forward',x)\n",
    "        x2 = Sigma().forward(s2)\n",
    "        print('sigma 2nd time',x)\n",
    "        s3 = self.fc3.forward(x2)\n",
    "        print('fc3 forward',x)\n",
    "        x3 = s3\n",
    "        return x1, s1, x2, s2, x3\n",
    "    \n",
    "    def backward(self, w1, b1, w2, b2, w3, b3,\n",
    "                  t,\n",
    "                  x, x1, x2, x3, s1, s2,  \n",
    "                  dl_dw1, dl_db1, dl_dw2, dl_db2, dl_dw3, dl_db3):  # modifica qui gli input non sono tutti necessari(?)\n",
    "        x0 = x\n",
    "        # last layer\n",
    "        dl_dw3 , dl_db3, dl_ds3 = self.fc3.backward_last(output = x3, target = t, dl_dw = dl_dw3, dl_db = dl_db3, x_m = x2)\n",
    "        # previous layers\n",
    "        dl_dw2 , dl_db2 = self.fc2.backward(dl_dw = dl_dw2, dl_db = dl_db2, w_next = w3, dl_ds_next = dl_ds3, x_m = x1)\n",
    "        dl_dw1 , dl_db1 = self.fc1.backward(dl_dw = dl_dw1, dl_db = dl_db1, w_next = w2, dl_ds_next = dl_ds2, x_m = x0)\n",
    "                \n",
    "        #return ? dl_dw , dl_db necessary ? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1 forward \n",
      " 1.0492\n",
      " 1.6063\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "sigma 1st time \n",
      " 1.0492\n",
      " 1.6063\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "fc2 forward \n",
      " 1.0492\n",
      " 1.6063\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "sigma 2nd time \n",
      " 1.0492\n",
      " 1.6063\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "fc3 forward \n",
      " 1.0492\n",
      " 1.6063\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Sigma' object has no attribute 'input'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-c159a85dd5e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                   \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                   \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                   dl_dw1, dl_db1, dl_dw2, dl_db2, dl_dw3, dl_db3)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-94-ee4f680bea18>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, w1, b1, w2, b2, w3, b3, t, x, x1, x2, x3, s1, s2, dl_dw1, dl_db1, dl_dw2, dl_db2, dl_dw3, dl_db3)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mx0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# last layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mdl_dw3\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdl_db3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_ds3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_last\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_dw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl_dw3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl_db3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;31m# previous layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mdl_dw2\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdl_db2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_dw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl_dw2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl_db2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_ds_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl_ds3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-93-cb8a7b6ed5fe>\u001b[0m in \u001b[0;36mbackward_last\u001b[0;34m(self, output, target, dl_dw, dl_db, x_m)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward_last\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_dw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_db\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# output would be x3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mdl_dx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mdl_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSigma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdl_dx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mdl_dw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_m\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mdl_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-9a0597e634dc>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, output)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# here you need to add \"def param\" too\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sigma' object has no attribute 'input'"
     ]
    }
   ],
   "source": [
    "prova = train_input[0]\n",
    "istanza = Linear(in_features = len(train_input[0]), out_features = 2)\n",
    "istanza.forward(prova)\n",
    "###########################################à\n",
    "istanzaNet = Net()  # qui chiami semplicemente __init__ di Net , che a sua volta chiama Linear, __init__ di Linear in particolare\n",
    "prova = train_input[0]\n",
    "#istanzaNet.forward(prova)  # qui sto chiamando il forward di Net. Ma leggi bene il forward di Net: a sua volta chiama un forward, che è quello di Linear\n",
    "x = train_input[0]\n",
    "t = train_target[0]\n",
    "x1, s1, x2, s2, x3 = istanzaNet.forward(x)\n",
    "istanzaNet.backward(w1, b1, w2, b2, w3, b3,\n",
    "                  t,\n",
    "                  x, x1, x2, x3, s1, s2,  \n",
    "                  dl_dw1, dl_db1, dl_dw2, dl_db2, dl_dw3, dl_db3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a network with 2 input units, 2 output units, 3 hidden layers with 25 units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network parameters\n",
    "input_size = 2\n",
    "output_size = 2\n",
    "hidden_units = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network structure I AM NOT USING THIS ONE\n",
    "class Net(Module):\n",
    "    def __init__(self):\n",
    "        self.fc1 = Linear(input_size,hidden_units)  # first hidden layer\n",
    "        self.fc2 = Linear(hidden_units,hidden_units) # second hidden layer\n",
    "        self.fc3 = Linear(hidden_units,output_size) # third hidden layer\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.fc1.forward(x)\n",
    "        x = relu().forward(x)\n",
    "        x = self.fc2(x)\n",
    "        x = relu().forward(x)\n",
    "        x = self.fc3(x)\n",
    "        return x \n",
    "    \n",
    "    def backward(w1, b1, w2, b2, w3, b3,\n",
    "                  t,\n",
    "                  x, s1, x1, s2, x2, s3, x3,\n",
    "                  dl_dw1, dl_db1, dl_dw2, dl_db2, dl_dw3, dl_db3):\n",
    "        x0 = x\n",
    "        dl_dx3 = dloss(x3, t)\n",
    "        dl_ds3 = dsigma(s3) * dl_dx3\n",
    "        dl_dx2 = w3.t().mv(dl_ds3)\n",
    "        dl_ds2 = dsigma(s2) * dl_dx2\n",
    "        dl_dx1 = w2.t().mv(dl_ds2)\n",
    "        dl_ds1 = dsigma(s1) * dl_dx1\n",
    "\n",
    "        dl_dw3.add_(dl_ds3.view(-1, 1).mm(x1.view(1, -1)))\n",
    "        dl_db3.add_(dl_ds3)\n",
    "        dl_dw2.add_(dl_ds2.view(-1, 1).mm(x1.view(1, -1)))\n",
    "        dl_db2.add_(dl_ds2)\n",
    "        dl_dw1.add_(dl_ds1.view(-1, 1).mm(x0.view(1, -1)))\n",
    "        dl_db1.add_(dl_ds1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set\n",
    "train_input = train_input\n",
    "train_target = train_target\n",
    "# network parameters\n",
    "model = Net()\n",
    "input_size = 2\n",
    "output_size = 2\n",
    "hidden_units = 25\n",
    "# training parameters\n",
    "lr = 0.1\n",
    "nb_epochs = 10\n",
    "# create weight and bias vectors\n",
    "eps = 1e-6\n",
    "w1 = Tensor(hidden_units, input_size).normal_(0, eps)\n",
    "b1 = Tensor(hidden_units).normal_(0, eps)\n",
    "w2 = Tensor(hidden_units, hidden_units).normal_(0, eps)\n",
    "b2 = Tensor(hidden_units).normal_(0, eps)\n",
    "w3 = Tensor(output_size, hidden_units).normal_(0,eps)\n",
    "b3 = Tensor(output_size).normal_(0, eps)\n",
    "# create derivatives\n",
    "dl_dw1 = Tensor(w1.size())\n",
    "dl_db1 = Tensor(b1.size())\n",
    "dl_dw2 = Tensor(w2.size())\n",
    "dl_db2 = Tensor(b2.size())\n",
    "dl_dw3 = Tensor(w3.size())\n",
    "dl_db3 = Tensor(b3.size())\n",
    "\n",
    "#prova = .forward(train_input[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "max received an invalid combination of arguments - got (out=NoneType, axis=NoneType, ), but expected one of:\n * no arguments\n * (torch.FloatTensor other, *, torch.FloatTensor out)\n      didn't match because some of the keywords were incorrect: axis\n * (int dim, *, tuple[torch.FloatTensor, torch.LongTensor] out)\n * (int dim, bool keepdim, *, tuple[torch.FloatTensor, torch.LongTensor] out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-b81c443ca64a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   2315\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2317\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2319\u001b[0m     return _methods._amax(a, axis=axis,\n",
      "\u001b[0;31mTypeError\u001b[0m: max received an invalid combination of arguments - got (out=NoneType, axis=NoneType, ), but expected one of:\n * no arguments\n * (torch.FloatTensor other, *, torch.FloatTensor out)\n      didn't match because some of the keywords were incorrect: axis\n * (int dim, *, tuple[torch.FloatTensor, torch.LongTensor] out)\n * (int dim, bool keepdim, *, tuple[torch.FloatTensor, torch.LongTensor] out)\n"
     ]
    }
   ],
   "source": [
    "train_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
