{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-1\n",
      " 1\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def disk(nb_points) :\n",
    "    radius = 1/np.sqrt(2*np.pi)\n",
    "    inp = Tensor(nb_points,2).uniform_(0,1)\n",
    "    ratio = torch.floor(torch.norm(inp,p=2,dim=1)/radius)\n",
    "    target = 1-torch.clamp(ratio,min=0,max=1)\n",
    "    t = Tensor(nb_points,2)\n",
    "    for i in range(nb_points):\n",
    "        if target[i] == 0:\n",
    "            t[i,:] = Tensor([-1,1])\n",
    "        elif target[i] == 1:\n",
    "            t[i,:] = Tensor([1,-1])    \n",
    "    return inp, t\n",
    "\n",
    "nb_points = 1000\n",
    "# create train set and respective labels\n",
    "train_input , train_target = disk(nb_points)\n",
    "# create test set and respective labels\n",
    "test_input , test_target = disk(nb_points)\n",
    "\n",
    "\"\"\"\n",
    "target=torch.zeros(train_target.shape[0],2)-1\n",
    "target[:,0][train_target==0]=1\n",
    "target[:,1][train_target==1]=1\n",
    "\"\"\"\n",
    "#print(train_input, train_target, test_input, test_target)\n",
    "\n",
    "# data normalization\n",
    "# , std_train = train_input.mean() , train_input.std()\n",
    "#train_input.sub_(mu_train).div_(std_train)\n",
    "#mu_test , std_test = test_input.mean() , test_input.std()\n",
    "#test_input.sub_(mu_test).div_(std_test)\n",
    "# later see whether it is convenient to normalize component-wise with broadcasting (pag 66 week5)\n",
    "\n",
    "# then do sanity check to see if the two classes are balanced\n",
    "print(train_target[-1,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module ( object ) :\n",
    "    def forward ( self , * input ) :\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward ( self , * gradwrtoutput ) :\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def param ( self ) :\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return x.tanh()\n",
    "\n",
    "def dtanh(x):\n",
    "    return 1-torch.tanh(x).pow(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return max(0,x)\n",
    "\n",
    "def drelu(x):\n",
    "    if x>0:\n",
    "        out = 1\n",
    "    elif x<=0:\n",
    "        out = 0\n",
    "    return out\n",
    "\n",
    "class Relu(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,input):\n",
    "        self.input = input\n",
    "        return relu(input)\n",
    "    \n",
    "    def backward(self,output):\n",
    "        return drelu(input)*output #to check, probably incorrect\n",
    "    \n",
    "    # here you need to add \"def param\" too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(v, t):\n",
    "    return (v - t).pow(2).sum()\n",
    "\n",
    "def dloss(v, t):\n",
    "    return 2 * (v - t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    # in_features: size of each input sample\n",
    "    # out_features: size of each output sample\n",
    "    # bias: If set to False, the layer will not learn an additive bias. Default: ``True``\n",
    "\n",
    "    # Attributes:\n",
    "    # weight: the learnable weights of the module of shape (out_features x in_features)`\n",
    "    # bias:   the learnable bias of the module of shape `(out_features)`\n",
    "\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weights = Tensor(out_features,in_features).normal_(0,1e-6)\n",
    "        self.bias = Tensor(out_features).zero_()\n",
    "        self.dl_dw = Tensor(out_features,in_features).zero_()\n",
    "        self.dl_db = Tensor(out_features).zero_()\n",
    "                \n",
    "    def forward(self,input):   # input sarebbe la x\n",
    "        self.input = input\n",
    "        self.s = torch.mv(self.weights,input)\n",
    "        return self.s\n",
    "\n",
    "    def backward(self, dl_ds):  \n",
    "        dl_dx = self.weights.t().mv(dl_ds)   # the problem is here! \n",
    "        \n",
    "        self.dl_dw.add_(dl_ds.view(-1, 1).mm(self.input.view(1, -1)))  \n",
    "        self.dl_db.add_(dl_ds)\n",
    "        return dl_dx     \n",
    "    \n",
    "    def update_param(self, eta):\n",
    "        self.weights = self.weights - eta * self.dl_dw\n",
    "        self.bias = self.bias - eta * self.dl_db\n",
    "    \n",
    "    def reset_param(self):\n",
    "        self.dl_dw.zero_()\n",
    "        self.dl_db.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigma(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,input):\n",
    "        self.s = input\n",
    "        return tanh(input)\n",
    "    \n",
    "    def backward(self, dl_dx):\n",
    "        dl_ds = dtanh(self.s)*dl_dx\n",
    "        return dl_ds\n",
    "    \n",
    "    def update_param(self, eta):\n",
    "        pass \n",
    "    \n",
    "    def reset_param(self):\n",
    "        pass\n",
    "        \n",
    "        #return dtanh(input)*output    \n",
    "        #return 4 * (output.exp() + output.mul(-1).exp()).pow(-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    def __init__(self, module_list):\n",
    "        self.modules = module_list\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for module in module_list:\n",
    "            x = module.forward(x)\n",
    "            \n",
    "        return x        \n",
    "        \n",
    "    def backward(self,dl_d):\n",
    "        for module in reversed(module_list):\n",
    "            dl_d = module.backward(dl_d)\n",
    "    \n",
    "    def update_param(self,eta):\n",
    "        for module in module_list:\n",
    "            module.update_param(eta)\n",
    "    \n",
    "    def reset_param(self):\n",
    "        for module in module_list:\n",
    "            module.reset_param()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        self.input = input\n",
    "        self.target = target\n",
    "        return loss(input, target)\n",
    "    \n",
    "    def backward(self):\n",
    "        return dloss(self.input, self.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 acc_train_loss 2000.00 acc_train_error 87.30% , magnitude of last sample 5.1e-17\n",
      "epoch 2 acc_train_loss 2000.00 acc_train_error 86.80% , magnitude of last sample 5e-17\n",
      "epoch 3 acc_train_loss 2000.00 acc_train_error 86.00% , magnitude of last sample 4.8e-17\n",
      "epoch 4 acc_train_loss 2000.00 acc_train_error 84.50% , magnitude of last sample 4.7e-17\n",
      "epoch 5 acc_train_loss 2000.00 acc_train_error 82.70% , magnitude of last sample 4.6e-17\n",
      "epoch 6 acc_train_loss 2000.00 acc_train_error 81.50% , magnitude of last sample 4.5e-17\n",
      "epoch 7 acc_train_loss 2000.00 acc_train_error 80.10% , magnitude of last sample 4.4e-17\n",
      "epoch 8 acc_train_loss 2000.00 acc_train_error 78.40% , magnitude of last sample 4.3e-17\n",
      "epoch 9 acc_train_loss 2000.00 acc_train_error 76.40% , magnitude of last sample 4.1e-17\n",
      "epoch 10 acc_train_loss 2000.00 acc_train_error 74.00% , magnitude of last sample 4e-17\n",
      "epoch 11 acc_train_loss 2000.00 acc_train_error 71.30% , magnitude of last sample 3.9e-17\n",
      "epoch 12 acc_train_loss 2000.00 acc_train_error 68.80% , magnitude of last sample 3.8e-17\n",
      "epoch 13 acc_train_loss 2000.00 acc_train_error 65.00% , magnitude of last sample 3.7e-17\n",
      "epoch 14 acc_train_loss 2000.00 acc_train_error 62.00% , magnitude of last sample 3.6e-17\n",
      "epoch 15 acc_train_loss 2000.00 acc_train_error 55.90% , magnitude of last sample 3.5e-17\n",
      "epoch 16 acc_train_loss 2000.00 acc_train_error 49.70% , magnitude of last sample 3.4e-17\n",
      "epoch 17 acc_train_loss 2000.00 acc_train_error 43.90% , magnitude of last sample 3.2e-17\n",
      "epoch 18 acc_train_loss 2000.00 acc_train_error 39.20% , magnitude of last sample 3.1e-17\n",
      "epoch 19 acc_train_loss 2000.00 acc_train_error 35.40% , magnitude of last sample 3e-17\n",
      "epoch 20 acc_train_loss 2000.00 acc_train_error 31.70% , magnitude of last sample 2.9e-17\n",
      "epoch 21 acc_train_loss 2000.00 acc_train_error 28.80% , magnitude of last sample 2.8e-17\n",
      "epoch 22 acc_train_loss 2000.00 acc_train_error 26.40% , magnitude of last sample 2.7e-17\n",
      "epoch 23 acc_train_loss 2000.00 acc_train_error 23.30% , magnitude of last sample 2.6e-17\n",
      "epoch 24 acc_train_loss 2000.00 acc_train_error 21.10% , magnitude of last sample 2.5e-17\n",
      "epoch 25 acc_train_loss 2000.00 acc_train_error 19.10% , magnitude of last sample 2.4e-17\n",
      "epoch 26 acc_train_loss 2000.00 acc_train_error 18.40% , magnitude of last sample 2.3e-17\n",
      "epoch 27 acc_train_loss 2000.00 acc_train_error 17.10% , magnitude of last sample 2.1e-17\n",
      "epoch 28 acc_train_loss 2000.00 acc_train_error 14.60% , magnitude of last sample 2e-17\n",
      "epoch 29 acc_train_loss 2000.00 acc_train_error 12.70% , magnitude of last sample 1.9e-17\n",
      "epoch 30 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample 1.8e-17\n",
      "epoch 31 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample 1.7e-17\n",
      "epoch 32 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample 1.6e-17\n",
      "epoch 33 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample 1.5e-17\n",
      "epoch 34 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample 1.4e-17\n",
      "epoch 35 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample 1.3e-17\n",
      "epoch 36 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample 1.2e-17\n",
      "epoch 37 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample 1e-17\n",
      "epoch 38 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample 9.4e-18\n",
      "epoch 39 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample 8.3e-18\n",
      "epoch 40 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample 7.2e-18\n",
      "epoch 41 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample 6.1e-18\n",
      "epoch 42 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample 4.9e-18\n",
      "epoch 43 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample 3.8e-18\n",
      "epoch 44 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample 2.7e-18\n",
      "epoch 45 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample 1.6e-18\n",
      "epoch 46 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample 4.6e-19\n",
      "epoch 47 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -6.8e-19\n",
      "epoch 48 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -1.8e-18\n",
      "epoch 49 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -3e-18\n",
      "epoch 50 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -4.1e-18\n",
      "epoch 51 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -5.3e-18\n",
      "epoch 52 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -6.4e-18\n",
      "epoch 53 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -7.6e-18\n",
      "epoch 54 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -8.8e-18\n",
      "epoch 55 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -9.9e-18\n",
      "epoch 56 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -1.1e-17\n",
      "epoch 57 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -1.2e-17\n",
      "epoch 58 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -1.4e-17\n",
      "epoch 59 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -1.5e-17\n",
      "epoch 60 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -1.6e-17\n",
      "epoch 61 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -1.7e-17\n",
      "epoch 62 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -1.8e-17\n",
      "epoch 63 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -2e-17\n",
      "epoch 64 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -2.1e-17\n",
      "epoch 65 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -2.2e-17\n",
      "epoch 66 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -2.4e-17\n",
      "epoch 67 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -2.5e-17\n",
      "epoch 68 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -2.6e-17\n",
      "epoch 69 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -2.7e-17\n",
      "epoch 70 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -2.9e-17\n",
      "epoch 71 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -3e-17\n",
      "epoch 72 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -3.2e-17\n",
      "epoch 73 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -3.3e-17\n",
      "epoch 74 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -3.4e-17\n",
      "epoch 75 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -3.6e-17\n",
      "epoch 76 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -3.7e-17\n",
      "epoch 77 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -3.9e-17\n",
      "epoch 78 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -4e-17\n",
      "epoch 79 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -4.1e-17\n",
      "epoch 80 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -4.3e-17\n",
      "epoch 81 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -4.4e-17\n",
      "epoch 82 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -4.6e-17\n",
      "epoch 83 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -4.8e-17\n",
      "epoch 84 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -4.9e-17\n",
      "epoch 85 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -5.1e-17\n",
      "epoch 86 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -5.2e-17\n",
      "epoch 87 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -5.4e-17\n",
      "epoch 88 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -5.6e-17\n",
      "epoch 89 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -5.7e-17\n",
      "epoch 90 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -5.9e-17\n",
      "epoch 91 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -6.1e-17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 92 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -6.3e-17\n",
      "epoch 93 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -6.4e-17\n",
      "epoch 94 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -6.6e-17\n",
      "epoch 95 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -6.8e-17\n",
      "epoch 96 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -7e-17\n",
      "epoch 97 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -7.2e-17\n",
      "epoch 98 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -7.4e-17\n",
      "epoch 99 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -7.6e-17\n",
      "epoch 100 acc_train_loss 2000.00 acc_train_error 12.60% , magnitude of last sample -7.8e-17\n"
     ]
    }
   ],
   "source": [
    "# Create my sequential list\n",
    "# List of layers\n",
    "input_size = 2\n",
    "hidden_units = 25\n",
    "output_size = 2\n",
    "\n",
    "# three linear layers\n",
    "fc1 = Linear(input_size,hidden_units)\n",
    "fc2 = Linear(hidden_units,hidden_units)\n",
    "fc3 = Linear(hidden_units,output_size)\n",
    "\n",
    "# three activation layers\n",
    "act1 = Sigma()\n",
    "act2 = Sigma()\n",
    "act3 = Sigma()\n",
    "\n",
    "module_list = [fc1, act1, fc2, act2, fc3, act3]\n",
    "\n",
    "# framework\n",
    "model = Sequential(module_list)\n",
    "model_loss = Loss()\n",
    "\n",
    "# training set\n",
    "train_input = train_input\n",
    "train_target = train_target\n",
    "\n",
    "# training parameters\n",
    "lr = 0.001\n",
    "nb_epochs = 100\n",
    "\n",
    "for k in range(0, nb_epochs):\n",
    "\n",
    "    acc_loss = 0   # accumulated loss\n",
    "    nb_train_errors = 0\n",
    "    \n",
    "    # zero the derivatives\n",
    "    model.reset_param()\n",
    "    \n",
    "    # forward pass for all samples\n",
    "    for n in range(train_input.size(0)):\n",
    "        x = train_input[n]\n",
    "        t = train_target[n]\n",
    "        x = model.forward(x)\n",
    "        \n",
    "        # compute the error\n",
    "        pred = x.max(0)[1][0]  # the result is the index (0 or 1) of the position where the max value is\n",
    "        targ = train_target[n,:].max(0)[1][0]\n",
    "        if targ != pred:\n",
    "            nb_train_errors = nb_train_errors + 1 \n",
    "               \n",
    "        #loss\n",
    "        ### forward\n",
    "        loss_sample = model_loss.forward(input = x, target = t)\n",
    "        acc_loss += loss_sample\n",
    "        ### backward\n",
    "        grad_loss = model_loss.backward()\n",
    "        \n",
    "        # backward pass\n",
    "        model.backward(grad_loss)  \n",
    "        \n",
    "        # update the derivatives\n",
    "        model.update_param(eta=lr)\n",
    "        \n",
    "    print('epoch {:d} acc_train_loss {:.02f} acc_train_error {:.02f}% , magnitude of last sample {:.2g}'.format(k+1,acc_loss,\n",
    "                                                                            (100 * nb_train_errors) / train_input.size(0) , x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
