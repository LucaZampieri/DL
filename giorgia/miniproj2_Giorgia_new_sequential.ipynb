{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-1\n",
      " 1\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def disk(nb_points) :\n",
    "    radius = 1/np.sqrt(2*np.pi)\n",
    "    inp = Tensor(nb_points,2).uniform_(0,1)\n",
    "    ratio = torch.floor(torch.norm(inp,p=2,dim=1)/radius)\n",
    "    target = 1-torch.clamp(ratio,min=0,max=1)\n",
    "    t = Tensor(nb_points,2)\n",
    "    for i in range(nb_points):\n",
    "        if target[i] == 0:\n",
    "            t[i,:] = Tensor([-1,1])\n",
    "        elif target[i] == 1:\n",
    "            t[i,:] = Tensor([1,-1])    \n",
    "    return inp, t\n",
    "\n",
    "nb_points = 1000\n",
    "# create train set and respective labels\n",
    "train_input , train_target = disk(nb_points)\n",
    "# create test set and respective labels\n",
    "test_input , test_target = disk(nb_points)\n",
    "\n",
    "\"\"\"\n",
    "target=torch.zeros(train_target.shape[0],2)-1\n",
    "target[:,0][train_target==0]=1\n",
    "target[:,1][train_target==1]=1\n",
    "\"\"\"\n",
    "#print(train_input, train_target, test_input, test_target)\n",
    "\n",
    "# data normalization\n",
    "# , std_train = train_input.mean() , train_input.std()\n",
    "#train_input.sub_(mu_train).div_(std_train)\n",
    "#mu_test , std_test = test_input.mean() , test_input.std()\n",
    "#test_input.sub_(mu_test).div_(std_test)\n",
    "# later see whether it is convenient to normalize component-wise with broadcasting (pag 66 week5)\n",
    "\n",
    "# then do sanity check to see if the two classes are balanced\n",
    "print(train_target[-1,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module ( object ) :\n",
    "    def forward ( self , * input ) :\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward ( self , * gradwrtoutput ) :\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def param ( self ) :\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return x.tanh()\n",
    "\n",
    "def dtanh(x):\n",
    "    return 1-torch.tanh(x).pow(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return max(0,x)\n",
    "\n",
    "def drelu(x):\n",
    "    if x>0:\n",
    "        out = 1\n",
    "    elif x<=0:\n",
    "        out = 0\n",
    "    return out\n",
    "\n",
    "class Relu(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,input):\n",
    "        self.input = input\n",
    "        return relu(input)\n",
    "    \n",
    "    def backward(self,output):\n",
    "        return drelu(input)*output #to check, probably incorrect\n",
    "    \n",
    "    # here you need to add \"def param\" too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(v, t):\n",
    "    return (v - t).pow(2).sum()\n",
    "\n",
    "def dloss(v, t):\n",
    "    return 2 * (v - t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    # in_features: size of each input sample\n",
    "    # out_features: size of each output sample\n",
    "    # bias: If set to False, the layer will not learn an additive bias. Default: ``True``\n",
    "\n",
    "    # Attributes:\n",
    "    # weight: the learnable weights of the module of shape (out_features x in_features)`\n",
    "    # bias:   the learnable bias of the module of shape `(out_features)`\n",
    "\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weights = Tensor(out_features,in_features).normal_(0,1e-1)\n",
    "        self.bias = Tensor(out_features).zero_()\n",
    "        self.dl_dw = Tensor(out_features,in_features).zero_()\n",
    "        self.dl_db = Tensor(out_features).zero_()\n",
    "                \n",
    "    def forward(self,input):   # input sarebbe la x\n",
    "        self.input = input\n",
    "        self.s = torch.mv(self.weights,input)\n",
    "        return self.s\n",
    "\n",
    "    def backward(self, dl_ds):  \n",
    "        dl_dx = self.weights.t().mv(dl_ds)   # the problem is here! \n",
    "        \n",
    "        self.dl_dw.add_(dl_ds.view(-1, 1).mm(self.input.view(1, -1)))  \n",
    "        self.dl_db.add_(dl_ds)\n",
    "        return dl_dx     \n",
    "    \n",
    "    def update_param(self, eta):\n",
    "        self.weights = self.weights - eta * self.dl_dw\n",
    "        self.bias = self.bias - eta * self.dl_db\n",
    "    \n",
    "    def reset_param(self):\n",
    "        self.dl_dw.zero_()\n",
    "        self.dl_db.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigma(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,input):\n",
    "        self.s = input\n",
    "        return tanh(input)\n",
    "    \n",
    "    def backward(self, dl_dx):\n",
    "        dl_ds = dtanh(self.s)*dl_dx\n",
    "        return dl_ds\n",
    "    \n",
    "    def update_param(self, eta):\n",
    "        pass \n",
    "    \n",
    "    def reset_param(self):\n",
    "        pass\n",
    "        \n",
    "        #return dtanh(input)*output    \n",
    "        #return 4 * (output.exp() + output.mul(-1).exp()).pow(-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    def __init__(self, module_list):\n",
    "        self.modules = module_list\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for module in module_list:\n",
    "            x = module.forward(x)\n",
    "            \n",
    "        return x        \n",
    "        \n",
    "    def backward(self,dl_d):\n",
    "        for module in reversed(module_list):\n",
    "            dl_d = module.backward(dl_d)\n",
    "    \n",
    "    def update_param(self,eta):\n",
    "        for module in module_list:\n",
    "            module.update_param(eta)\n",
    "    \n",
    "    def reset_param(self):\n",
    "        for module in module_list:\n",
    "            module.reset_param()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        self.input = input\n",
    "        self.target = target\n",
    "        return loss(input, target)\n",
    "    \n",
    "    def backward(self):\n",
    "        return dloss(self.input, self.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 acc_train_loss 2031.59 acc_train_error 87.10% test_error 89.40%\n",
      "epoch 2 acc_train_loss 1583.34 acc_train_error 87.10% test_error 89.40%\n",
      "epoch 3 acc_train_loss 592.64 acc_train_error 3.60% test_error 4.40%\n",
      "epoch 4 acc_train_loss 583.85 acc_train_error 3.40% test_error 2.80%\n",
      "epoch 5 acc_train_loss 582.60 acc_train_error 2.40% test_error 2.30%\n",
      "epoch 6 acc_train_loss 582.33 acc_train_error 2.50% test_error 2.50%\n",
      "epoch 7 acc_train_loss 582.13 acc_train_error 2.50% test_error 2.50%\n",
      "epoch 8 acc_train_loss 581.95 acc_train_error 2.50% test_error 2.40%\n",
      "epoch 9 acc_train_loss 581.78 acc_train_error 2.50% test_error 2.40%\n",
      "epoch 10 acc_train_loss 581.61 acc_train_error 2.50% test_error 2.40%\n",
      "epoch 11 acc_train_loss 581.46 acc_train_error 2.70% test_error 2.40%\n",
      "epoch 12 acc_train_loss 581.31 acc_train_error 2.70% test_error 2.40%\n",
      "epoch 13 acc_train_loss 581.16 acc_train_error 2.70% test_error 2.40%\n",
      "epoch 14 acc_train_loss 581.03 acc_train_error 2.70% test_error 2.60%\n",
      "epoch 15 acc_train_loss 580.89 acc_train_error 2.70% test_error 2.60%\n",
      "epoch 16 acc_train_loss 580.77 acc_train_error 2.70% test_error 2.60%\n",
      "epoch 17 acc_train_loss 580.64 acc_train_error 2.70% test_error 2.60%\n",
      "epoch 18 acc_train_loss 580.53 acc_train_error 2.70% test_error 2.60%\n",
      "epoch 19 acc_train_loss 580.41 acc_train_error 2.60% test_error 2.60%\n",
      "epoch 20 acc_train_loss 580.30 acc_train_error 2.60% test_error 2.60%\n"
     ]
    }
   ],
   "source": [
    "# Create my sequential list\n",
    "# List of layers\n",
    "input_size = 2\n",
    "hidden_units = 25\n",
    "output_size = 2\n",
    "\n",
    "# three linear layers\n",
    "fc1 = Linear(input_size,hidden_units)\n",
    "fc2 = Linear(hidden_units,hidden_units)\n",
    "fc3 = Linear(hidden_units,output_size)\n",
    "\n",
    "# three activation layers\n",
    "act1 = Sigma()\n",
    "act2 = Sigma()\n",
    "act3 = Sigma()\n",
    "\n",
    "module_list = [fc1, act1, fc2, act2, fc3, act3]\n",
    "\n",
    "# framework\n",
    "model = Sequential(module_list)\n",
    "model_loss = Loss()\n",
    "\n",
    "# training set\n",
    "train_input = train_input\n",
    "train_target = train_target\n",
    "\n",
    "# training parameters\n",
    "lr = 0.001\n",
    "nb_epochs = 20\n",
    "\n",
    "# testing set\n",
    "test_input = test_input\n",
    "test_target = test_target\n",
    "\n",
    "for k in range(0, nb_epochs):\n",
    "\n",
    "    # TRAINING\n",
    "    acc_loss = 0   # accumulated loss\n",
    "    nb_train_errors = 0\n",
    "    nb_test_errors = 0\n",
    "    \n",
    "    # zero the derivatives\n",
    "    model.reset_param()\n",
    "    \n",
    "    # forward pass for all training samples\n",
    "    for n in range(train_input.size(0)):\n",
    "        x = train_input[n]\n",
    "        t = train_target[n]\n",
    "        x = model.forward(x)\n",
    "        \n",
    "        # compute the error\n",
    "        pred = x.max(0)[1][0]  # the result is the index (0 or 1) of the position where the max value is\n",
    "        if x[0]>-0.5:\n",
    "            pred=0\n",
    "        targ = train_target[n,:].max(0)[1][0]\n",
    "        if targ != pred:\n",
    "            nb_train_errors = nb_train_errors + 1 \n",
    "               \n",
    "        #loss\n",
    "        ### forward\n",
    "        loss_sample = model_loss.forward(input = x, target = t)\n",
    "        acc_loss += loss_sample\n",
    "        ### backward\n",
    "        grad_loss = model_loss.backward()\n",
    "        \n",
    "        # backward pass\n",
    "        model.backward(grad_loss)  \n",
    "        \n",
    "        # TESTING\n",
    "        x_test = test_input[n]\n",
    "        t_test = test_target[n]\n",
    "        x_test = model.forward(x_test)\n",
    "        \n",
    "        # compute the error\n",
    "        pred_test = x_test.max(0)[1][0]  # the result is the index (0 or 1) of the position where the max value is\n",
    "        if x_test[0]>-0.5:\n",
    "            pred_test=0\n",
    "        targ_test = test_target[n,:].max(0)[1][0]\n",
    "        if targ_test != pred_test:\n",
    "            nb_test_errors = nb_test_errors + 1 \n",
    "        \n",
    "        \n",
    "    # update the derivatives\n",
    "    model.update_param(eta=lr)\n",
    "    \n",
    "    \n",
    "        \n",
    "    print('epoch {:d} acc_train_loss {:.02f} acc_train_error {:.02f}% test_error {:.02f}%'.format(k+1,acc_loss,\n",
    "                                                                            (100 * nb_train_errors) / train_input.size(0), (100 * nb_test_errors) / test_input.size(0) ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
