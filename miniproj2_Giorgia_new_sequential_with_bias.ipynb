{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disk(nb_points) :\n",
    "    radius = 1/np.sqrt(2*np.pi)\n",
    "    inp = Tensor(nb_points,2).uniform_(0,1)\n",
    "    ratio = torch.floor(torch.norm(inp,p=2,dim=1)/radius)\n",
    "    target = 1-torch.clamp(ratio,min=0,max=1)\n",
    "    t = Tensor(nb_points,2)\n",
    "    for i in range(nb_points):\n",
    "        if target[i] == 0:\n",
    "            t[i,:] = Tensor([-1,1])\n",
    "        elif target[i] == 1:\n",
    "            t[i,:] = Tensor([1,-1])    \n",
    "    return inp, t\n",
    "\n",
    "nb_points = 1000\n",
    "# create train set and respective labels\n",
    "train_input , train_target = disk(nb_points)\n",
    "# create test set and respective labels\n",
    "test_input , test_target = disk(nb_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module ( object ) :\n",
    "    def forward ( self , * input ) :\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward ( self , * gradwrtoutput ) :\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def param ( self ) :\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return x.tanh()\n",
    "\n",
    "def dtanh(x):\n",
    "    return 1-torch.tanh(x).pow(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return max(0,x)\n",
    "\n",
    "def drelu(x):\n",
    "    if x>0:\n",
    "        out = 1\n",
    "    elif x<=0:\n",
    "        out = 0\n",
    "    return out\n",
    "\n",
    "class Relu(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,input):\n",
    "        self.input = input\n",
    "        return relu(input)\n",
    "    \n",
    "    def backward(self,output):\n",
    "        return drelu(input)*output #to check, probably incorrect\n",
    "    \n",
    "    # here you need to add \"def param\" too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(v, t):\n",
    "    return (v - t).pow(2).sum()\n",
    "\n",
    "def dloss(v, t):\n",
    "    return 2 * (v - t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    # in_features: size of each input sample\n",
    "    # out_features: size of each output sample\n",
    "    # bias: If set to False, the layer will not learn an additive bias. Default: ``True``\n",
    "\n",
    "    # Attributes:\n",
    "    # weight: the learnable weights of the module of shape (out_features x in_features)`\n",
    "    # bias:   the learnable bias of the module of shape `(out_features)`\n",
    "\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weights = Tensor(out_features,in_features).normal_(0,1e-1)\n",
    "        self.bias = Tensor(out_features).zero_()\n",
    "        self.dl_dw = Tensor(out_features,in_features).zero_()\n",
    "        self.dl_db = Tensor(out_features).zero_()\n",
    "                \n",
    "    def forward(self,input):   # input sarebbe la x\n",
    "        self.input = input\n",
    "        self.s = torch.mv(self.weights,input) + self.bias\n",
    "        return self.s\n",
    "\n",
    "    def backward(self, dl_ds):  \n",
    "        dl_dx = self.weights.t().mv(dl_ds)   # the problem is here! \n",
    "        \n",
    "        self.dl_dw.add_(dl_ds.view(-1, 1).mm(self.input.view(1, -1)))  \n",
    "        self.dl_db.add_(dl_ds)\n",
    "        return dl_dx     \n",
    "    \n",
    "    def update_param(self, eta):\n",
    "        self.weights = self.weights - eta * self.dl_dw\n",
    "        self.bias = self.bias - eta * self.dl_db\n",
    "    \n",
    "    def reset_param(self):\n",
    "        self.dl_dw.zero_()\n",
    "        self.dl_db.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigma(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,input):\n",
    "        self.s = input\n",
    "        return tanh(input)\n",
    "    \n",
    "    def backward(self, dl_dx):\n",
    "        dl_ds = dtanh(self.s)*dl_dx\n",
    "        return dl_ds\n",
    "    \n",
    "    def update_param(self, eta):\n",
    "        pass \n",
    "    \n",
    "    def reset_param(self):\n",
    "        pass\n",
    "        \n",
    "        #return dtanh(input)*output    \n",
    "        #return 4 * (output.exp() + output.mul(-1).exp()).pow(-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    def __init__(self, module_list):\n",
    "        self.modules = module_list\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for module in module_list:\n",
    "            x = module.forward(x)\n",
    "            \n",
    "        return x        \n",
    "        \n",
    "    def backward(self,dl_d):\n",
    "        for module in reversed(module_list):\n",
    "            dl_d = module.backward(dl_d)\n",
    "    \n",
    "    def update_param(self,eta):\n",
    "        for module in module_list:\n",
    "            module.update_param(eta)\n",
    "    \n",
    "    def reset_param(self):\n",
    "        for module in module_list:\n",
    "            module.reset_param()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        self.input = input\n",
    "        self.target = target\n",
    "        return loss(input, target)\n",
    "    \n",
    "    def backward(self):\n",
    "        return dloss(self.input, self.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 acc_train_loss 1882.36 acc_train_error 86.80% test_error 12.20%\n",
      "epoch 2 acc_train_loss 1038.44 acc_train_error 13.20% test_error 12.20%\n",
      "epoch 3 acc_train_loss 1035.40 acc_train_error 13.20% test_error 12.20%\n",
      "epoch 4 acc_train_loss 1031.37 acc_train_error 13.20% test_error 12.20%\n",
      "epoch 5 acc_train_loss 1025.86 acc_train_error 13.20% test_error 12.20%\n",
      "epoch 6 acc_train_loss 1018.06 acc_train_error 13.20% test_error 12.20%\n",
      "epoch 7 acc_train_loss 1006.58 acc_train_error 13.20% test_error 12.20%\n",
      "epoch 8 acc_train_loss 989.23 acc_train_error 13.20% test_error 12.20%\n",
      "epoch 9 acc_train_loss 963.67 acc_train_error 13.20% test_error 12.20%\n",
      "epoch 10 acc_train_loss 933.80 acc_train_error 13.20% test_error 12.20%\n",
      "epoch 11 acc_train_loss 913.37 acc_train_error 13.20% test_error 12.20%\n",
      "epoch 12 acc_train_loss 896.60 acc_train_error 13.20% test_error 12.20%\n",
      "epoch 13 acc_train_loss 886.09 acc_train_error 13.20% test_error 12.20%\n",
      "epoch 14 acc_train_loss 874.54 acc_train_error 13.20% test_error 12.20%\n",
      "epoch 15 acc_train_loss 858.70 acc_train_error 13.20% test_error 12.20%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-5319d5c4b188>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# TESTING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-fc5becace129>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, dl_d)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdl_d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mdl_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-484dd3f8e89e>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, dl_dx)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_dx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mdl_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdl_dx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdl_ds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-fb6a2f6e130c>\u001b[0m in \u001b[0;36mdtanh\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create my sequential list\n",
    "# List of layers\n",
    "input_size = 2\n",
    "hidden_units = 25\n",
    "output_size = 2\n",
    "\n",
    "# three linear layers\n",
    "fc1 = Linear(input_size,hidden_units)\n",
    "fc2 = Linear(hidden_units,hidden_units)\n",
    "fc3 = Linear(hidden_units,output_size)\n",
    "\n",
    "# three activation layers\n",
    "act1 = Sigma()\n",
    "act2 = Sigma()\n",
    "act3 = Sigma()\n",
    "\n",
    "module_list = [fc1, act1, fc2, act2, fc3, act3]\n",
    "\n",
    "# framework\n",
    "model = Sequential(module_list)\n",
    "model_loss = Loss()\n",
    "\n",
    "# training set\n",
    "train_input = train_input\n",
    "train_target = train_target\n",
    "\n",
    "# training parameters\n",
    "lr = 0.001\n",
    "nb_epochs = 200\n",
    "\n",
    "# testing set\n",
    "test_input = test_input\n",
    "test_target = test_target\n",
    "\n",
    "# store results\n",
    "results = Tensor(train_input.size(0), 2, nb_epochs).zero_()\n",
    "\n",
    "for k in range(0, nb_epochs):\n",
    "\n",
    "    # TRAINING\n",
    "    acc_loss = 0   # accumulated loss\n",
    "    nb_train_errors = 0\n",
    "    nb_test_errors = 0\n",
    "    \n",
    "    # zero the derivatives\n",
    "    model.reset_param()\n",
    "    \n",
    "    # forward pass for all training samples\n",
    "    for n in range(train_input.size(0)):\n",
    "        x = train_input[n]\n",
    "        t = train_target[n]\n",
    "        x = model.forward(x)\n",
    "        \n",
    "        results[n,:,k] = x\n",
    "        \n",
    "        # compute the error\n",
    "        pred = x.max(0)[1][0]  # the result is the index (0 or 1) of the position where the max value is\n",
    "        if x[0]>-0.5:\n",
    "            pred=0\n",
    "        targ = train_target[n,:].max(0)[1][0]\n",
    "        if targ != pred:\n",
    "            nb_train_errors = nb_train_errors + 1 \n",
    "               \n",
    "        #loss\n",
    "        ### forward\n",
    "        loss_sample = model_loss.forward(input = x, target = t)\n",
    "        acc_loss += loss_sample\n",
    "        ### backward\n",
    "        grad_loss = model_loss.backward()\n",
    "        \n",
    "        # backward pass\n",
    "        model.backward(grad_loss)  \n",
    "        \n",
    "        # TESTING\n",
    "        x_test = test_input[n]\n",
    "        t_test = test_target[n]\n",
    "        x_test = model.forward(x_test)\n",
    "        \n",
    "        # compute the error\n",
    "        pred_test = x_test.max(0)[1][0]  # the result is the index (0 or 1) of the position where the max value is\n",
    "        #if x_test[0]>-0.5:\n",
    "            #pred_test=0\n",
    "        targ_test = test_target[n,:].max(0)[1][0]\n",
    "        if targ_test != pred_test:\n",
    "            nb_test_errors = nb_test_errors + 1 \n",
    "        \n",
    "        \n",
    "    # update the derivatives\n",
    "    model.update_param(eta=lr)\n",
    "    \n",
    "    \n",
    "        \n",
    "    print('epoch {:d} acc_train_loss {:.02f} acc_train_error {:.02f}% test_error {:.02f}%'.format(k+1,acc_loss,\n",
    "                                                                            (100 * nb_train_errors) / train_input.size(0), (100 * nb_test_errors) / test_input.size(0) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-0.6980  0.6937\n",
       "-0.7356  0.7264\n",
       "-0.7780  0.7881\n",
       "-0.6739  0.6615\n",
       "-0.5537  0.4859\n",
       "-0.7066  0.7045\n",
       "-0.6354  0.6057\n",
       "-0.7939  0.8064\n",
       "-0.7154  0.6998\n",
       "-0.7791  0.7906\n",
       "-0.7783  0.7868\n",
       "-0.6856  0.6788\n",
       "-0.6580  0.6389\n",
       "-0.5224  0.4432\n",
       "-0.6117  0.5637\n",
       "-0.7450  0.7392\n",
       "-0.7885  0.7985\n",
       "-0.7767  0.7885\n",
       "-0.6220  0.5825\n",
       "-0.6924  0.6730\n",
       "[torch.FloatTensor of size 20x2]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0:20,:,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Tensor(2,3).zero_()\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9400811791419983"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
