{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from torch import Tensor\n",
    "#from torch import LongTensor\n",
    "import torch\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "#import baseline\n",
    "\n",
    "DEBUG = False\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3989422804014327"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/math.sqrt(2*math.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 2]) torch.Size([1000])\n",
      "torch.Size([1000])\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def generate_disc_set(nb):\n",
    "    input_ = Tensor(nb, 2).uniform_(0,1)\n",
    "    disk_center = Tensor(nb, 2).fill_(0.5)\n",
    "    #ones_ = torch.ones(nb,2)\n",
    "    R = 1/math.sqrt(2*math.pi) # Radius of the disk\n",
    "    target = (R - (disk_center - input_).pow(2).sum(1).sqrt()  ).sign()#.long()\n",
    "    target.add_(1).div_(2) # to transform [-1,1] into [0,1]\n",
    "    #target = input.pow(2).sum(1).mul(-1).add(1 / 2/ math.pi).sign().add(1).div(2).long() # prof version\n",
    "    return input_, target\n",
    "\n",
    "\n",
    "train_input, train_target = generate_disc_set(1000)\n",
    "test_input, test_target = generate_disc_set(1000)\n",
    "\n",
    "\n",
    "\n",
    "# one hot?\n",
    "#one_hot_targets = np.eye(2)[train_target]\n",
    "#train_target = one_hot_targets\n",
    "#train_target = Tensor(train_target)\n",
    "\n",
    "\n",
    "mini_batch_size = 100\n",
    "print (train_input.size(), train_target.size())\n",
    "#print(train_input[0:10],train_target[0:10])\n",
    "print(train_target.size())\n",
    "#plt.plot(train_input.where())\n",
    "print(train_target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "dim out of range - got 1 but the tensor is only 1D",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c43346811ab2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_target_ori\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mone_hot_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtrain_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mone_hot_targets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot_targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: dim out of range - got 1 but the tensor is only 1D"
     ]
    }
   ],
   "source": [
    "train_target_ori  = train_target.clone()\n",
    "one_hot_targets = torch.cat((train_target,1-train_target),1)\n",
    "one_hot_targets[0]\n",
    "train_target = one_hot_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d1be754fa7a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'x-large'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Distribution of generated data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mplot_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-d1be754fa7a4>\u001b[0m in \u001b[0;36mplot_data\u001b[0;34m(input_, target_, figure_size)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0minput_false\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtarget_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0minput_false\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_false\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "#third_tensor = torch.cat((first_tensor, second_tensor), 0)\n",
    "def plot_data(input_, target_, figure_size = 6):\n",
    "    input_true = torch.Tensor(0,2)\n",
    "    input_false = torch.Tensor(0,2)\n",
    "    for i,x in enumerate(input_):\n",
    "        if target_[i][0] == 0 :\n",
    "            input_false = torch.cat( (input_false, input_[i,:].view(-1,2)),0 )\n",
    "        else :\n",
    "            input_true = torch.cat( (input_true, input_[i,:].view(-1,2)),0 )\n",
    "    print ('#samples:       ',input_.size())\n",
    "    print ('#true_samples:  ',input_true.size())\n",
    "    print ('#false_samples: ',input_false.size())\n",
    "    p1 = plt.figure(1,figsize=(figure_size,figure_size))\n",
    "    plt.plot(input_true[:,0].numpy(),input_true[:,1].numpy(),'r.',label='1 label')\n",
    "    plt.plot(input_false[:,0].numpy(),input_false[:,1].numpy(),'b.',label='0 labe')\n",
    "    plt.xlim(0,1), plt.ylim(0,1)\n",
    "    plt.legend(fontsize='x-large')\n",
    "    plt.title('Distribution of generated data')\n",
    "plot_data(train_input, train_target)\n",
    "\n",
    "mu, std = train_input.mean(),train_input.std()\n",
    "train_input.sub_(mu).div_(std)\n",
    "mu, std = test_input.mean(),test_input.std()\n",
    "test_input.sub_(mu).div_(std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Between -1,1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_target=(train_target*2-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6 %\n"
     ]
    }
   ],
   "source": [
    "import baseline\n",
    "baseline.baseline_linear_model(train_input, train_target_ori, test_input, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return x.tanh()\n",
    "\n",
    "def d_tanh(x):\n",
    "    return (x.exp() + x.mul(-1).exp()).pow(-2)\n",
    "\n",
    "def relu(x):\n",
    "    if x>0 : return x\n",
    "    else : return x.fill(0)\n",
    "    \n",
    "def d_relu(x):\n",
    "    if x>0: return x.fill(1)\n",
    "    else : return x.fill(0)\n",
    "\n",
    "def mse(x,t):\n",
    "    if DEBUG == True:\n",
    "        print('mse x.size',x.size())\n",
    "        print('mse target size',t.size())\n",
    "    return (x - t).pow(2).sum()\n",
    "\n",
    "#def dsigma_relu(x):\n",
    "\n",
    "sigma = tanh\n",
    "dsigma = d_tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xavier_init(weight): \n",
    "    size = weight.size()\n",
    "    std = np.sqrt(2.0/(size[0] + size[1]))\n",
    "    return weight.normal_(0.0, std)\n",
    "\n",
    "def standard_init(weight):\n",
    "    stdv = 1. / math.sqrt(weight.size(1)) / math.sqrt(3) \n",
    "    weight.uniform_(-stdv, stdv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### suggested structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Module ( object ) :\n",
    "    def forward ( self , * input ) :\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward ( self , * gradwrtoutput ) :\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def param ( self ) :\n",
    "        return [] \n",
    "    \n",
    "    def zero_grads ( self ) :\n",
    "        pass\n",
    "    \n",
    "    def reset_params( self ) :\n",
    "        pass\n",
    "    \n",
    "    def update_params( self, eta ):\n",
    "        pass\n",
    "    \n",
    "\n",
    "class Tanh(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.params = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return 0.5*(1+x.tanh())\n",
    "    \n",
    "    def backward(self, d_dx):\n",
    "        return 0.5*d_dx*(1-torch.tanh(self.x)**2)\n",
    "    \n",
    "class Sigmoid(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return torch.sigmoid(x)\n",
    "    \n",
    "    def backward(self, d_dx):\n",
    "        return d_dx * (torch.sigmoid(self.x*(1-torch.sigmoid(self.x))))\n",
    "\n",
    "\"\"\"class Tanh(Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input_):\n",
    "        self.input = input_.clone()\n",
    "        return (input_.tanh()+1)/2\n",
    "    \n",
    "    def backward(self, d_output):\n",
    "        x = self.input\n",
    "        return (1 - self.input.tanh()**2) * d_output\"\"\"\n",
    "    \n",
    "class Relu (Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.params = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return x*(x>0).type(torch.FloatTensor)\n",
    "    \n",
    "    def backward(self, d_dx):\n",
    "        return d_dx*(self.x>0).type(torch.FloatTensor)\n",
    "        \n",
    "    \n",
    "class LossMSE(Module):\n",
    "    def __init__(self,p=2):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "    def forward(self,y,t):\n",
    "        self.y = y\n",
    "        self.t = t\n",
    "        return torch.dist(y, t, p=2)\n",
    "        \n",
    "    def backward(self):\n",
    "        #print (self.p)\n",
    "        return 2*(self.y-self.t)#self.p * torch.pow((self.x - self.t),self.p-1)\n",
    "    \n",
    "    \n",
    "    \n",
    "class Sequential(Module):\n",
    "\n",
    "    def __init__(self,modules, loss): \n",
    "        super().__init__()\n",
    "        self.modules = modules\n",
    "        self.loss = loss\n",
    "        self.params = []\n",
    "        self.param()\n",
    "        \n",
    "    #done by luca\n",
    "    def add(self, module):\n",
    "        self.modules.append(module)\n",
    "\n",
    "    def forward(self, input_): # , target\n",
    "        for module in self.modules:\n",
    "            input_ = module.forward(input_) # module.forward?\n",
    "        return input_ \n",
    "    \n",
    "    def backward(self):\n",
    "        out = self.loss.backward()\n",
    "        for x in reversed(self.modules):\n",
    "            out = x.backward(out)\n",
    "        return out\n",
    "    \n",
    "    def zero_grads(self):\n",
    "        for x in self.modules:\n",
    "            x.zero_grads()\n",
    "            \n",
    "    \"\"\"def param ( self ):\n",
    "        params = []\n",
    "        for x in self.modules:\n",
    "            params = params + x.param()\n",
    "        return params\"\"\"\n",
    "    def param(self):\n",
    "        self.params = []\n",
    "        for module in self.modules:\n",
    "            if(module.params):\n",
    "                self.params.append(module.param())\n",
    "            \n",
    "    def update_params( self, eta ):\n",
    "        for module in self.modules:\n",
    "            module.update_params(eta)\n",
    "            \n",
    "            \n",
    "            \n",
    "class Linear(Module):\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        # num features\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        # weigths\n",
    "        self.weights = torch.Tensor(out_features, in_features).normal_(0,1)\n",
    "        self.bias   = torch.Tensor(out_features).uniform_(0,0)\n",
    "        # gradients\n",
    "        self.dl_dw = torch.Tensor(out_features, in_features)\n",
    "        self.dl_db   = torch.Tensor(out_features)\n",
    "        self.zero_grads()\n",
    "        self.params = [(self.weights, self.bias),(self.dl_dw, self.dl_db)]\n",
    "        # initialize the parameters\n",
    "        #self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weights.size(1)) / math.sqrt(3) #\n",
    "        self.weights.uniform_(-stdv, stdv)\n",
    "        self.bias.uniform_(-stdv, stdv)\n",
    "        \n",
    "    def zero_grads(self):\n",
    "        \"\"\"resets the gradients of the module\"\"\"\n",
    "        self.dl_dw.zero_()\n",
    "        self.dl_db.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        if(x.size()[1]!=self.in_features):\n",
    "            raise TypeError('Size of x should correspond to size of linear module')\n",
    "        return torch.mm(x, self.weights.t()) + self.bias.expand(x.size(0),self.out_features)\n",
    "        \n",
    "    def backward ( self ,d_dx ) :\n",
    "        self.dl_db = torch.mean(d_dx,0)  \n",
    "        self.dl_dw = torch.mm(d_dx.t(), self.x)\n",
    "        dl_ds = torch.mm(d_dx,self.weights)\n",
    "        return dl_ds\n",
    "    \n",
    "    \"\"\"def param ( self ) :\n",
    "        return [[self.weight,self.dl_dws], [self.bias, self.dl_dbias]] \"\"\"\n",
    "    def param(self):\n",
    "        self.params = [(self.weights, self.bias),(self.dl_dw, self.dl_db)]\n",
    "        return self.params\n",
    "    \n",
    "    def update_params ( self, eta ):\n",
    "        self.weights.sub_(eta * self.dl_dw)\n",
    "        self.bias.sub_(eta * self.dl_db)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SGDOptimizer():\n",
    "    def __init__(self, Sequential, lr):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.Sequential = Sequential\n",
    "        self.lr = lr\n",
    "        \n",
    "    def step(self):\n",
    "        for param in self.Sequential.params:\n",
    "            param[0][0].add_(- self.lr * param[1][0])\n",
    "            param[0][1].add_(- self.lr * param[1][1])\n",
    "        \n",
    "        self.Sequential.zero_grads()\n",
    "\n",
    "class SGD(Module):\n",
    "    \n",
    "    def __init__(self, model_params, eta):\n",
    "        super().__init__()\n",
    "        self.model_params = model_params\n",
    "        self.eta = eta\n",
    "        \n",
    "    def step(self):\n",
    "        for x in self.model_params:\n",
    "            x[0].sub_(self.eta*x[1])\n",
    "            \n",
    "class SGD_mom(Module):\n",
    "    \n",
    "    def __init__(self, model_params, eta, gamma):\n",
    "        super().__init__()\n",
    "        self.model_params = model_params\n",
    "        self.eta = eta\n",
    "        self.gamma = gamma\n",
    "        self.u = []\n",
    "        self.count= 0\n",
    "        \n",
    "    def _initial_step(self):\n",
    "        self.count = 1\n",
    "        for i,x in enumerate(self.model_params):\n",
    "            self.u.append(self.eta*x[1])\n",
    "            x[0].sub_(self.u[i])\n",
    "        \n",
    "    def step(self):\n",
    "        if self.count == 0:\n",
    "            self._initial_step()\n",
    "        else:\n",
    "            for i,x in enumerate(self.model_params):\n",
    "                self.u[i] = self.gamma*self.u[i] + self.eta*x[1]\n",
    "                x[0].sub_(self.u[i])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "hidden = 20\n",
    "Lin1 = Linear (2,hidden)\n",
    "Lin2 = Linear (hidden,hidden)\n",
    "Lin3 = Linear (hidden,2)\n",
    "act1 = Relu()\n",
    "act2 = Relu()\n",
    "act3 = Tanh()\n",
    "act4 = Sigmoid()\n",
    "\n",
    "\n",
    "layers = [Lin1,act1,Lin3,act3]\n",
    "loss = LossMSE()\n",
    "# network parameters\n",
    "model = Sequential(modules = layers,loss = loss )\n",
    "\n",
    "# training parameters\n",
    "mini_batch_size = 50\n",
    "\n",
    "lr = 0.01\n",
    "momentum = 0.001\n",
    "#optimizer = SGD(model.param(), eta= lr)\n",
    "optimizer = SGDOptimizer(model, 0.01)\n",
    "#optimizer = SGD_mom(model.param(), eta= lr, gamma = momentum)\n",
    "nb_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 1 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 2 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 3 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 4 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 5 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 6 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 7 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 8 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 9 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 10 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 11 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 12 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 13 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 14 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 15 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 16 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 17 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 18 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 19 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 20 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 21 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 22 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 23 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 24 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 25 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 26 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 27 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 28 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 29 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 30 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 31 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 32 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 33 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 34 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 35 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 36 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 37 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 38 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 39 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 40 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 41 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 42 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 43 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 44 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 45 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 46 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 47 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 48 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 49 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 50 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 51 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 52 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 53 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 54 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 55 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 56 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 57 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 58 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 59 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 60 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 61 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 62 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 63 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 64 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 65 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 66 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 67 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 68 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 69 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 70 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 71 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 72 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 73 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 74 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 75 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 76 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 77 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 78 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 79 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 80 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 81 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 82 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 83 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 84 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 85 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 86 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 87 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 88 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 89 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 90 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 91 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 92 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 93 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 94 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 95 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 96 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 97 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 98 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 99 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 100 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 101 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 102 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 103 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 104 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 105 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 106 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 107 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 108 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 109 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 110 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 111 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 112 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 113 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 114 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 115 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 116 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 117 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 118 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 119 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 120 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 121 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 122 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 123 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 124 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 125 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 126 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 127 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 128 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 129 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 130 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 131 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 132 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 133 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 134 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 135 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 136 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 137 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 138 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 139 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 140 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 141 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 142 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 143 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 144 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 145 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 146 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 147 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 148 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 149 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 150 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 151 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 152 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 153 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 154 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 155 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 156 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 157 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 158 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 159 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 160 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 161 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 162 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 163 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 164 acc_train_loss 284.29 acc_train_error 50.80%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 165 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 166 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 167 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 168 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 169 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 170 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 171 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 172 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 173 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 174 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 175 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 176 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 177 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 178 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 179 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 180 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 181 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 182 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 183 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 184 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 185 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 186 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 187 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 188 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 189 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 190 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 191 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 192 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 193 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 194 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 195 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 196 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 197 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 198 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 199 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 200 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 201 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 202 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 203 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 204 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 205 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 206 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 207 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 208 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 209 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 210 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 211 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 212 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 213 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 214 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 215 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 216 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 217 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 218 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 219 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 220 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 221 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 222 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 223 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 224 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 225 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 226 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 227 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 228 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 229 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 230 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 231 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 232 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 233 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 234 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 235 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 236 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 237 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 238 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 239 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 240 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 241 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 242 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 243 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 244 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 245 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 246 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 247 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 248 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 249 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 250 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 251 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 252 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 253 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 254 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 255 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 256 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 257 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 258 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 259 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 260 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 261 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 262 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 263 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 264 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 265 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 266 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 267 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 268 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 269 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 270 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 271 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 272 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 273 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 274 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 275 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 276 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 277 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 278 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 279 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 280 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 281 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 282 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 283 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 284 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 285 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 286 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 287 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 288 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 289 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 290 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 291 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 292 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 293 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 294 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 295 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 296 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 297 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 298 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 299 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 300 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 301 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 302 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 303 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 304 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 305 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 306 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 307 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 308 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 309 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 310 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 311 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 312 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 313 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 314 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 315 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 316 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 317 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 318 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 319 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 320 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 321 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 322 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 323 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 324 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 325 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 326 acc_train_loss 284.29 acc_train_error 50.80%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 327 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 328 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 329 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 330 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 331 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 332 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 333 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 334 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 335 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 336 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 337 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 338 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 339 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 340 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 341 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 342 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 343 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 344 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 345 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 346 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 347 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 348 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 349 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 350 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 351 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 352 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 353 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 354 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 355 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 356 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 357 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 358 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 359 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 360 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 361 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 362 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 363 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 364 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 365 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 366 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 367 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 368 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 369 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 370 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 371 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 372 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 373 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 374 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 375 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 376 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 377 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 378 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 379 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 380 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 381 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 382 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 383 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 384 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 385 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 386 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 387 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 388 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 389 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 390 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 391 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 392 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 393 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 394 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 395 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 396 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 397 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 398 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 399 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 400 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 401 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 402 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 403 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 404 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 405 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 406 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 407 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 408 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 409 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 410 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 411 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 412 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 413 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 414 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 415 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 416 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 417 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 418 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 419 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 420 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 421 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 422 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 423 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 424 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 425 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 426 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 427 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 428 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 429 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 430 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 431 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 432 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 433 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 434 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 435 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 436 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 437 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 438 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 439 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 440 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 441 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 442 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 443 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 444 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 445 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 446 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 447 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 448 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 449 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 450 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 451 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 452 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 453 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 454 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 455 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 456 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 457 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 458 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 459 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 460 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 461 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 462 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 463 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 464 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 465 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 466 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 467 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 468 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 469 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 470 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 471 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 472 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 473 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 474 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 475 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 476 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 477 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 478 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 479 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 480 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 481 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 482 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 483 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 484 acc_train_loss 284.29 acc_train_error 50.80%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 485 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 486 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 487 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 488 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 489 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 490 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 491 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 492 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 493 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 494 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 495 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 496 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 497 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 498 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 499 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "\n",
      "\n",
      "\n",
      " FINISHED!!! \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_errors = []\n",
    "loss_list = []\n",
    "\n",
    "for epoch in range(nb_epochs):\n",
    "    acc_loss = 0\n",
    "    nb_train_errors = 0\n",
    "    model.zero_grads()\n",
    "    \n",
    "    for b in range(0,train_input.size(0), mini_batch_size):\n",
    "        output = model.forward(train_input[b:b+mini_batch_size,:])\n",
    "        batch_loss = loss.forward(train_input[b:b+mini_batch_size,:],train_target[b:b+mini_batch_size,:])\n",
    "        acc_loss += batch_loss\n",
    "        ###################  update of weights\n",
    "        \n",
    "        model.backward()\n",
    "        optimizer.step()\n",
    "        #model.update_params(lr)\n",
    "        ###################  make predictions\n",
    "        max_pred, argmax_pred = torch.max(output,1)\n",
    "        max_target, argmax_target = torch.max(train_target[b:b+mini_batch_size],1)\n",
    "        nb_train_errors += torch.sum(argmax_target == argmax_pred)\n",
    "    ############# logg errors and print it\n",
    "    train_errors.append(nb_train_errors)\n",
    "    loss_list.append(acc_loss)\n",
    "    print('epoch {:d} acc_train_loss {:.02f} acc_train_error {:.02f}%'.format(epoch,acc_loss,\n",
    "                              (100 * nb_train_errors) / train_input.size(0)))\n",
    "print('\\n\\n\\n FINISHED!!! \\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEICAYAAABbOlNNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAEE1JREFUeJzt3X+s3XV9x/Hny1ZK/EVbaBUotThY\nTHGJxjOI2VyY/ComWiJkq3OzcbpumfwxDYslzCBoNmAajJFt6cSkMZnAMM4mbiEVZFmWBbkFnFbF\nXiuECwTq2uCQCaLv/XG/3c7n5pR7e8+59/Ti85GcnO/383mf73l/epO+7vf7PadNVSFJ0mEvGXcD\nkqRji8EgSWoYDJKkhsEgSWoYDJKkhsEgSWoYDJKkhsEgvYAkDyU5f9x9SIvJYJAkNQwGaR6S/FGS\nySQHk+xKcko3niQ3JnkyyVNJ/jPJG7q5tyf5TpL/TvJokivGuwppMINBOkpJ3gb8FfA7wMnAw8At\n3fSFwG8BvwqsBH4X+K9u7mbgj6vqlcAbgLsWsW1pzpaPuwFpCXoP8Pmqug8gyZXAoSQbgJ8BrwRe\nD3yjqr7b97qfARuTfLOqDgGHFrVraY48Y5CO3ilMnyUAUFVPM31WcGpV3QV8FrgJeCLJjiSv6kov\nBd4OPJzkX5O8ZZH7lubEYJCO3mPAaw/vJHk5cCLwKEBVfaaq3gycxfQlpT/vxu+tqs3AWuCfgNsW\nuW9pTgwGaXYvTXL84QfTf6G/L8kbk6wA/hK4p6oeSvLrSc5J8lLgJ8BPgZ8nOS7Je5KcUFU/A34M\n/HxsK5JegMEgze6fgf/pe7wV+CjwJeBx4FeALV3tq4C/Z/r+wcNMX2L6ZDf3B8BDSX4M/Anw+4vU\nv3RU4n/UI0nq5xmDJKlhMEiSGgaDJKlhMEiSGkvym88nnXRSbdiwYdxtSNKSsmfPnh9V1ZrZ6pZk\nMGzYsIGJiYlxtyFJS0qSh2ev8lKSJGkGg0GS1DAYJEkNg0GS1DAYJEkNg0GS1DAYJEkNg0GS1DAY\nJEkNg0GS1DAYJEkNg0GS1DAYJEkNg0GS1DAYJEkNg0GS1DAYJEkNg0GS1DAYJEkNg0GS1DAYJEkN\ng0GS1DAYJEkNg0GS1DAYJEmNkQRDkk1JHkwymWT7gPkVSW7t5u9JsmHG/PokTye5YhT9SJLmb+hg\nSLIMuAm4GNgIvDvJxhll7wcOVdUZwI3A9TPmbwT+ZdheJEnDG8UZw9nAZFXtr6rngFuAzTNqNgM7\nu+3bgfOSBCDJJcB+YO8IepEkDWkUwXAq8Ejf/lQ3NrCmqp4HngJOTPJy4CPANbO9SZJtSSaSTBw4\ncGAEbUuSBhlFMGTAWM2x5hrgxqp6erY3qaodVdWrqt6aNWvm0aYkaS6Wj+AYU8BpffvrgMeOUDOV\nZDlwAnAQOAe4LMkNwErgF0l+WlWfHUFfkqR5GEUw3AucmeR04FFgC/B7M2p2AVuB/wAuA+6qqgLe\nerggyceApw0FSRqvoYOhqp5PcjlwB7AM+HxV7U1yLTBRVbuAm4EvJJlk+kxhy7DvK0laGJn+xX1p\n6fV6NTExMe42JGlJSbKnqnqz1fnNZ0lSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUM\nBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklS\nw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDVGEgxJNiV5MMlkku0D5lck\nubWbvyfJhm78giR7knyre37bKPqRJM3f0MGQZBlwE3AxsBF4d5KNM8reDxyqqjOAG4Hru/EfAe+o\nql8DtgJfGLYfSdJwRnHGcDYwWVX7q+o54BZg84yazcDObvt24Lwkqar7q+qxbnwvcHySFSPoSZI0\nT6MIhlOBR/r2p7qxgTVV9TzwFHDijJpLgfur6tkR9CRJmqflIzhGBozV0dQkOYvpy0sXHvFNkm3A\nNoD169cffZeSpDkZxRnDFHBa3/464LEj1SRZDpwAHOz21wFfBt5bVT840ptU1Y6q6lVVb82aNSNo\nW5I0yCiC4V7gzCSnJzkO2ALsmlGzi+mbywCXAXdVVSVZCXwVuLKq/n0EvUiShjR0MHT3DC4H7gC+\nC9xWVXuTXJvknV3ZzcCJSSaBDwOHP9J6OXAG8NEkD3SPtcP2JEmav1TNvB1w7Ov1ejUxMTHuNiRp\nSUmyp6p6s9X5zWdJUsNgkCQ1DAZJUsNgkCQ1DAZJUsNgkCQ1DAZJUsNgkCQ1DAZJUsNgkCQ1DAZJ\nUsNgkCQ1DAZJUsNgkCQ1DAZJUsNgkCQ1DAZJUsNgkCQ1DAZJUsNgkCQ1DAZJUsNgkCQ1DAZJUsNg\nkCQ1DAZJUsNgkCQ1DAZJUsNgkCQ1DAZJUsNgkCQ1DAZJUmMkwZBkU5IHk0wm2T5gfkWSW7v5e5Js\n6Ju7sht/MMlFo+hHkjR/QwdDkmXATcDFwEbg3Uk2zih7P3Coqs4AbgSu7167EdgCnAVsAv6mO54k\naUxGccZwNjBZVfur6jngFmDzjJrNwM5u+3bgvCTpxm+pqmer6ofAZHc8SdKYjCIYTgUe6duf6sYG\n1lTV88BTwIlzfC0ASbYlmUgyceDAgRG0LUkaZBTBkAFjNceaubx2erBqR1X1qqq3Zs2ao2xRkjRX\nowiGKeC0vv11wGNHqkmyHDgBODjH10qSFtEoguFe4Mwkpyc5jumbybtm1OwCtnbblwF3VVV141u6\nTy2dDpwJfGMEPUmS5mn5sAeoqueTXA7cASwDPl9Ve5NcC0xU1S7gZuALSSaZPlPY0r12b5LbgO8A\nzwMfrKqfD9uTJGn+Mv2L+9LS6/VqYmJi3G1I0pKSZE9V9War85vPkqSGwSBJahgMkqSGwSBJahgM\nkqSGwSBJahgMkqSGwSBJahgMkqSGwSBJahgMkqSGwSBJahgMkqSGwSBJahgMkqSGwSBJahgMkqSG\nwSBJahgMkqSGwSBJahgMkqSGwSBJahgMkqSGwSBJahgMkqSGwSBJahgMkqSGwSBJahgMkqSGwSBJ\nagwVDElWJ9mdZF/3vOoIdVu7mn1JtnZjL0vy1STfS7I3yXXD9CJJGo1hzxi2A3dW1ZnAnd1+I8lq\n4GrgHOBs4Oq+APlkVb0eeBPwG0kuHrIfSdKQhg2GzcDObnsncMmAmouA3VV1sKoOAbuBTVX1TFV9\nHaCqngPuA9YN2Y8kaUjDBsOrq+pxgO557YCaU4FH+vanurH/k2Ql8A6mzzokSWO0fLaCJF8DXjNg\n6qo5vkcGjFXf8ZcDXwQ+U1X7X6CPbcA2gPXr18/xrSVJR2vWYKiq8480l+SJJCdX1eNJTgaeHFA2\nBZzbt78OuLtvfwewr6o+PUsfO7paer1evVCtJGn+hr2UtAvY2m1vBb4yoOYO4MIkq7qbzhd2YyT5\nBHAC8GdD9iFJGpFhg+E64IIk+4ALun2S9JJ8DqCqDgIfB+7tHtdW1cEk65i+HLURuC/JA0k+MGQ/\nkqQhpWrpXZXp9Xo1MTEx7jYkaUlJsqeqerPV+c1nSVLDYJAkNQwGSVLDYJAkNQwGSVLDYJAkNQwG\nSVLDYJAkNQwGSVLDYJAkNQwGSVLDYJAkNQwGSVLDYJAkNQwGSVLDYJAkNQwGSVLDYJAkNQwGSVLD\nYJAkNQwGSVLDYJAkNQwGSVLDYJAkNQwGSVLDYJAkNQwGSVLDYJAkNQwGSVLDYJAkNQwGSVJjqGBI\nsjrJ7iT7uudVR6jb2tXsS7J1wPyuJN8ephdJ0mgMe8awHbizqs4E7uz2G0lWA1cD5wBnA1f3B0iS\ndwFPD9mHJGlEhg2GzcDObnsncMmAmouA3VV1sKoOAbuBTQBJXgF8GPjEkH1IkkZk2GB4dVU9DtA9\nrx1QcyrwSN/+VDcG8HHgU8Azs71Rkm1JJpJMHDhwYLiuJUlHtHy2giRfA14zYOqqOb5HBoxVkjcC\nZ1TVh5JsmO0gVbUD2AHQ6/Vqju8tSTpKswZDVZ1/pLkkTyQ5uaoeT3Iy8OSAsing3L79dcDdwFuA\nNyd5qOtjbZK7q+pcJEljM+ylpF3A4U8ZbQW+MqDmDuDCJKu6m84XAndU1d9W1SlVtQH4TeD7hoIk\njd+wwXAdcEGSfcAF3T5Jekk+B1BVB5m+l3Bv97i2G5MkHYNStfQu1/d6vZqYmBh3G5K0pCTZU1W9\n2er85rMkqWEwSJIaBoMkqWEwSJIaBoMkqWEwSJIaBoMkqWEwSJIaBoMkqWEwSJIaBoMkqWEwSJIa\nBoMkqWEwSJIaBoMkqWEwSJIaBoMkqWEwSJIaBoMkqWEwSJIaBoMkqWEwSJIaBoMkqWEwSJIaqapx\n93DUkhwAHh53H0fpJOBH425ikbnmXw6ueel4bVWtma1oSQbDUpRkoqp64+5jMbnmXw6u+cXHS0mS\npIbBIElqGAyLZ8e4GxgD1/zLwTW/yHiPQZLU8IxBktQwGCRJDYNhhJKsTrI7yb7uedUR6rZ2NfuS\nbB0wvyvJtxe+4+ENs+YkL0vy1STfS7I3yXWL2/3RSbIpyYNJJpNsHzC/Ismt3fw9STb0zV3ZjT+Y\n5KLF7HsY811zkguS7Enyre75bYvd+3wM8zPu5tcneTrJFYvV84KoKh8jegA3ANu77e3A9QNqVgP7\nu+dV3faqvvl3Af8AfHvc61noNQMvA367qzkO+Dfg4nGv6QjrXAb8AHhd1+s3gY0zav4U+Ltuewtw\na7e9satfAZzeHWfZuNe0wGt+E3BKt/0G4NFxr2ch19s3/yXgH4Erxr2eYR6eMYzWZmBnt70TuGRA\nzUXA7qo6WFWHgN3AJoAkrwA+DHxiEXodlXmvuaqeqaqvA1TVc8B9wLpF6Hk+zgYmq2p/1+stTK+9\nX/+fxe3AeUnSjd9SVc9W1Q+Bye54x7p5r7mq7q+qx7rxvcDxSVYsStfzN8zPmCSXMP1Lz95F6nfB\nGAyj9eqqehyge147oOZU4JG+/aluDODjwKeAZxayyREbds0AJFkJvAO4c4H6HNasa+ivqarngaeA\nE+f42mPRMGvudylwf1U9u0B9jsq815vk5cBHgGsWoc8Ft3zcDSw1Sb4GvGbA1FVzPcSAsUryRuCM\nqvrQzOuW47ZQa+47/nLgi8Bnqmr/0Xe4KF5wDbPUzOW1x6Jh1jw9mZwFXA9cOMK+Fsow670GuLGq\nnu5OIJY0g+EoVdX5R5pL8kSSk6vq8SQnA08OKJsCzu3bXwfcDbwFeHOSh5j+uaxNcndVncuYLeCa\nD9sB7KuqT4+g3YUyBZzWt78OeOwINVNd2J0AHJzja49Fw6yZJOuALwPvraofLHy7QxtmvecAlyW5\nAVgJ/CLJT6vqswvf9gIY902OF9MD+GvaG7E3DKhZDfyQ6Zuvq7rt1TNqNrB0bj4PtWam76d8CXjJ\nuNcyyzqXM339+HT+/8bkWTNqPkh7Y/K2bvss2pvP+1kaN5+HWfPKrv7Sca9jMdY7o+ZjLPGbz2Nv\n4MX0YPra6p3Avu758F9+PeBzfXV/yPQNyEngfQOOs5SCYd5rZvo3sgK+CzzQPT4w7jW9wFrfDnyf\n6U+uXNWNXQu8s9s+nulPpEwC3wBe1/faq7rXPcgx+smrUa4Z+AvgJ30/1weAteNez0L+jPuOseSD\nwX8SQ5LU8FNJkqSGwSBJahgMkqSGwSBJahgMkqSGwSBJahgMkqTG/wKNDJVJBsj2LgAAAABJRU5E\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7eff4bdec4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEICAYAAABbOlNNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7eff4bda85c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_list)\n",
    "plt.title('Loss')\n",
    "plt.show()\n",
    "plt.plot(train_errors)\n",
    "plt.title('Train error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -6.2135  -3.6557\n",
      "  2.4919  17.0744\n",
      "-20.2456   0.1606\n",
      " -1.3837  -0.0463\n",
      "  0.1625   0.6191\n",
      " 14.8800  -4.2982\n",
      "  8.9325   0.5230\n",
      "  1.1019  -0.1001\n",
      "-12.7274  -3.1620\n",
      " -1.2513  -4.9012\n",
      " -6.7639   3.7345\n",
      "  6.6544  -7.9926\n",
      "  2.2174  -1.2183\n",
      "  4.1442  -0.0851\n",
      "  7.5383   0.9199\n",
      "  0.2062   6.4622\n",
      "  9.0093  -0.7424\n",
      "  0.3805  -0.3944\n",
      " -3.1593   5.6455\n",
      " -9.6319  -3.0651\n",
      "[torch.FloatTensor of size 20x2]\n",
      "\n",
      "\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "[torch.FloatTensor of size 20x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.param()[0][0])\n",
    "print(model.param()[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\n",
       "    -7.6250   -4.9432\n",
       "   -21.2920   -8.4659\n",
       "     4.9776   -0.3756\n",
       "   -20.2504  -10.3480\n",
       "    17.7162    1.1736\n",
       "   -19.6728   12.6312\n",
       "   -14.0407   -4.0811\n",
       "    -4.8308  -11.6672\n",
       "     9.9235   23.6013\n",
       "   -27.1013   -2.7988\n",
       "   -51.8492   32.5478\n",
       "   135.7014    5.9190\n",
       "   -20.1224   13.3914\n",
       "     2.0349   22.0325\n",
       "   -44.1463   17.0851\n",
       "  -100.6973  -22.7698\n",
       "    47.4853  -12.9090\n",
       "   -64.4381   38.7549\n",
       "   -98.8742  -53.8455\n",
       "     0.1425  -12.1399\n",
       "  [torch.FloatTensor of size 20x2], \n",
       "      0     0\n",
       "      0     0\n",
       "      0     0\n",
       "      0     0\n",
       "      0     0\n",
       "      0     0\n",
       "      0     0\n",
       "      0     0\n",
       "      0     0\n",
       "      0     0\n",
       "      0     0\n",
       "      0     0\n",
       "      0     0\n",
       "      0     0\n",
       "      0     0\n",
       "      0     0\n",
       "      0     0\n",
       "      0     0\n",
       "      0     0\n",
       "      0     0\n",
       "  [torch.FloatTensor of size 20x2]], [\n",
       "  -0.3604\n",
       "  -0.5856\n",
       "  -0.6024\n",
       "  -0.5608\n",
       "  -0.4740\n",
       "  -0.8237\n",
       "  -0.6027\n",
       "  -0.1783\n",
       "  -0.4973\n",
       "  -0.6575\n",
       "  -0.6659\n",
       "  -0.5848\n",
       "  -0.3573\n",
       "  -0.4927\n",
       "  -0.2568\n",
       "  -0.8237\n",
       "  -0.5793\n",
       "  -0.8929\n",
       "  -0.8529\n",
       "  -0.7207\n",
       "  [torch.FloatTensor of size 20], \n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "  [torch.FloatTensor of size 20]], [\n",
       "  \n",
       "  Columns 0 to 7 \n",
       "   -4.4151   5.3017  14.4080   0.1017   3.5791   2.1245   4.7096   2.8842\n",
       "   -2.3667  -8.6420 -31.5903 -10.1685  -0.7483  -0.8958 -17.7323 -11.8779\n",
       "   -2.6668   3.3140  37.8768  11.8308 -17.5431  -4.9965   7.2506   8.7701\n",
       "   -8.0147   2.2842  10.9644  -0.7341  -1.8080  -0.5518  -4.5632   1.4294\n",
       "    4.1007   1.0936  18.7900   9.8126  -9.8671  -4.5069   7.3378   6.9885\n",
       "  -10.1683  16.9439  67.5359   9.5841  -3.6393   8.0577  24.7381  12.9414\n",
       "   -2.3570  -3.0885 -14.8000  -4.4805   2.0635   0.0038  -9.1659  -4.2173\n",
       "   -3.0034   1.8279   3.0687  -1.5059   3.7172  -0.6416  -0.8441   1.5903\n",
       "    2.3026  11.0049  21.5727   0.6731  13.3663  11.5286  23.2959   4.7001\n",
       "   -9.1952   4.9031  14.4333  -1.6786   1.8627   3.4669  -0.5229   1.2035\n",
       "    7.1359   0.1352   0.3001   2.2812   1.2487   2.5762   9.6958  -0.0577\n",
       "   -3.1816 -10.3739 -24.0759  -9.4301 -12.9455  -4.9686 -26.3842  -7.1173\n",
       "    8.8292   0.5736  15.3007  18.9240   0.2653  -3.9822  19.2466   4.2669\n",
       "    0.5710  -0.9014   0.0361  -1.2110  -1.3869  -0.0863  -0.9129  -0.4393\n",
       "   14.6510   1.3949   8.5125  19.0461   6.5106  -1.7455  25.5598   4.7683\n",
       "   -0.4435   6.3690  22.3860  -3.0524   0.3149   8.8316  14.8082   0.4605\n",
       "   -2.9236  -4.8561 -18.1147 -11.5995  -3.6110  -0.0465 -17.1717  -4.9331\n",
       "    2.6390  -0.5009   2.8031   2.2561  -4.1588  -0.2436   2.3655   1.7441\n",
       "  -11.6094   9.2361  18.3491 -12.5975   2.6527  10.3279   1.5086   0.7331\n",
       "   -5.5772  -1.9196  -6.0561 -13.0375  -5.1610   2.4725 -13.2153  -4.4151\n",
       "  \n",
       "  Columns 8 to 15 \n",
       "   -0.7733   2.2575  -5.5436  -0.8214   5.1504  -2.0316 -12.0886   7.0571\n",
       "   17.6417   1.9077 -24.3980  11.3447 -16.1904  -3.1164  10.8742 -15.1090\n",
       "  -31.6072  -7.1148   7.9644 -15.4699  17.9316   5.4946  -0.5832   9.7024\n",
       "   -1.9643  -0.2581 -16.3804   0.1549   0.6388  -1.0791  -5.7250   2.4797\n",
       "  -20.1731  -7.0068  19.6936  -9.7844  10.3725   6.1440   4.3415   5.4921\n",
       "  -29.9241   7.7856   0.0719 -18.5642  33.2793  -4.8594 -33.4917  27.3368\n",
       "   10.1314   1.0067 -12.3905   5.6255  -9.2469  -1.4924   3.9894  -6.6219\n",
       "    3.8295  -0.7623  -6.3465   2.7083  -1.5355  -0.1886  -4.6066   1.8637\n",
       "   -1.2689  12.2644  15.6689  -5.2550  16.2452  -6.8042 -23.8564  15.3738\n",
       "    0.2676   4.7991 -19.0797  -1.0029   3.6196  -4.2501 -12.5655   5.2867\n",
       "   -5.7702   1.9543  25.0733  -4.2703   5.7697  -0.2100  -0.7996   2.2230\n",
       "    2.2700  -4.4000 -17.6269  11.3728 -17.4313   1.7497  23.3967 -17.5153\n",
       "  -18.2292  -5.6907  47.9070 -21.3862  15.9071   5.9575  -8.3498   8.5129\n",
       "   -4.2296  -0.5065   8.9175   0.9314  -0.0176   0.0498   0.4182  -0.3820\n",
       "  -12.5837  -3.6648  58.8467 -20.7612  15.4677   5.9000  -7.5285   9.1380\n",
       "   -9.8193   9.3457   4.2776  -2.0799  15.5710  -6.8851 -16.4275  10.6291\n",
       "    7.9495   0.3132 -17.7630  13.7614 -13.8898  -1.8382  12.1143 -10.2759\n",
       "   -7.7337  -0.8353   9.5890  -4.1418   2.9080   1.7299   4.8053   0.7146\n",
       "    8.8484  12.8101 -50.3028   9.9475   5.0774 -10.4715 -14.5673   7.7777\n",
       "    3.0926   3.3008 -22.9423  13.2939  -7.4531  -4.3876   4.8061  -5.6826\n",
       "  \n",
       "  Columns 16 to 19 \n",
       "  -19.4385  -5.9784   2.2533  -1.2232\n",
       "   15.6639  23.6663 -27.9786  -2.0886\n",
       "    3.0760 -16.5473  24.0420   2.7840\n",
       "   -6.4794  -0.5490  -7.6660   0.2644\n",
       "    9.3701 -11.4943  21.2349   4.4211\n",
       "  -51.4745 -36.2211  35.0509  -7.4700\n",
       "    6.6120   9.2507 -15.8707  -0.3925\n",
       "   -8.2373   0.6431  -5.3380   1.7775\n",
       "  -42.0570 -20.7989  23.7598  -8.1335\n",
       "  -19.3985  -5.0810  -6.0208  -3.5834\n",
       "    3.8461  -5.2228  18.1130  -2.4377\n",
       "   45.4547  24.4071 -26.2889   4.2411\n",
       "    4.3644 -19.4972  38.6400  -1.5656\n",
       "    9.7541   2.8491   3.9436   0.0834\n",
       "   -0.5021 -23.1107  43.0527  -1.8107\n",
       "  -27.1745  -8.5036  19.8763  -7.8106\n",
       "   19.9143  19.0774 -22.6232   2.1115\n",
       "    9.1350  -4.7552   8.1347   0.5618\n",
       "  -50.6781  -2.1993 -17.0934  -5.6982\n",
       "    7.2533  16.6551 -17.6320  -0.3847\n",
       "  [torch.FloatTensor of size 20x20], \n",
       "  \n",
       "  Columns 0 to 12 \n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "  \n",
       "  Columns 13 to 19 \n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "  [torch.FloatTensor of size 20x20]], [\n",
       "  -0.2356\n",
       "  -0.1929\n",
       "  -0.2172\n",
       "  -0.2633\n",
       "  -0.2644\n",
       "  -0.1564\n",
       "  -0.1660\n",
       "  -0.1829\n",
       "  -0.0549\n",
       "  -0.2721\n",
       "  -0.1829\n",
       "  -0.0612\n",
       "  -0.0584\n",
       "  -0.1735\n",
       "  -0.1553\n",
       "  -0.2475\n",
       "  -0.0451\n",
       "  -0.2624\n",
       "  -0.2704\n",
       "  -0.1137\n",
       "  [torch.FloatTensor of size 20], \n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "  [torch.FloatTensor of size 20]], [\n",
       "  \n",
       "  Columns 0 to 7 \n",
       "   -5.0685   5.9972   3.0630  12.6083   0.1204   3.4245   1.6493   7.2296\n",
       "   -4.5327  -0.8474  -3.7461  10.3793  -7.9849   2.1343 -13.5918  12.2265\n",
       "  \n",
       "  Columns 8 to 15 \n",
       "   -4.3448  -3.3751   6.7437   0.1486   6.0442  -0.0410  -6.2425   5.3975\n",
       "   -2.7844 -11.2225 -11.9644  18.8420  -7.5498  10.3131  10.7458 -12.2104\n",
       "  \n",
       "  Columns 16 to 19 \n",
       "    1.9975  -9.1478   9.1679   1.7911\n",
       "   21.3333  -2.4221 -13.6647  21.1745\n",
       "  [torch.FloatTensor of size 2x20], \n",
       "  \n",
       "  Columns 0 to 12 \n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "  \n",
       "  Columns 13 to 19 \n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "  [torch.FloatTensor of size 2x20]], [\n",
       "  -0.7429\n",
       "  -0.6696\n",
       "  [torch.FloatTensor of size 2], \n",
       "   0\n",
       "   0\n",
       "  [torch.FloatTensor of size 2]]]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## test as giorgia\n",
    "class Linear2(Module):\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        # num features\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        # weigths\n",
    "        self.weight = torch.Tensor(out_features, in_features).fill_(0.0)\n",
    "        self.bias   = torch.Tensor(out_features).fill_(0.0)\n",
    "        # gradients\n",
    "        self.dl_dws = torch.Tensor(out_features, in_features).fill_(1.0)\n",
    "        self.dl_dbias   = torch.Tensor(out_features).fill_(0.0)\n",
    "        self.params = [(self.weight,self.dl_dws), (self.bias, self.dl_dbias)] \n",
    "        if DEBUG == True:\n",
    "            print('weights size: ',self.weight.size())\n",
    "        # initialize the parameters\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1)) # *sqrt(3)?\n",
    "        self.weight.uniform_(-stdv, stdv)\n",
    "        self.bias.uniform_(0, 0)\n",
    "        \n",
    "    def zero_grads(self):\n",
    "        \"\"\"resets the gradients of the module\"\"\"\n",
    "        self.dl_dws.zero_()\n",
    "        self.dl_dbias.zero_()\n",
    "\n",
    "    def forward(self, input_):\n",
    "        self.input = input_\n",
    "        self.s = torch.mm(input_, self.weight.t())#.add(self.bias)\n",
    "        return torch.mm(input_, self.weight.t())\n",
    "        \n",
    "    def backward_last ( self ,d_output ) :\n",
    "        dl_ds = d_output\n",
    "        dl_dw = torch.mm(dl_ds.view(-1,1), self.input.view(-1,1).t())\n",
    "       \n",
    "        self.dl_dws.add_(dl_dw) \n",
    "        self.dl_dbias.add(d_output)\n",
    "        \n",
    "        #d_input = torch.mm(self.weight.t(), d_output.t())\n",
    "        return dl_ds, self.weight\n",
    "    \n",
    "    def backward(self, dl_ds_next):  \n",
    "        #dl_dx = self.weights.t().mv(dl_ds_next)   # the problem is here! \n",
    "        dl_dx = w_next.t().mv(dl_ds_next)\n",
    "        dl_ds = Sigma().backward(self.s)*dl_dx \n",
    "        self.dl_dw.add_(dl_ds.view(-1, 1).mm(self.input.view(1, -1)))   \n",
    "        self.dl_db.add_(dl_ds)\n",
    "        return dl_ds\n",
    "    \n",
    "    def backward ( self , dl_ds_next) :\n",
    "        dl_ds = dl_ds_next\n",
    "        dl_dw = dl_ds_next.view(-1,1).mm(self.input.view(-1,1).t())\n",
    "        \n",
    "        self.dl_dws.add_(dl_dw) \n",
    "        self.dl_dbias.add(d_output)\n",
    "        #d_input = torch.mm(self.weight.t(), d_output.t())\n",
    "        return dl_ds\n",
    "    \n",
    "    def param ( self ) :\n",
    "        return [[self.weight,self.dl_dws], [self.bias, self.dl_dbias]] \n",
    "    \n",
    "    def update_params ( self, eta ):\n",
    "        self.weight = self.weight - eta * self.dl_dws\n",
    "        self.bias   = self.bias - eta * self.dl_dbias\n",
    "    \n",
    "\n",
    "\n",
    "class MyNet(Module):\n",
    "\n",
    "    def __init__(self): # , loss\n",
    "        super().__init__()\n",
    "        self.fc1 = Linear2(2,100)\n",
    "        self.fc2 = Linear2(100,100)\n",
    "        self.fc3 = Linear2(100,2)\n",
    "        self.act1 = Tanh()\n",
    "        self.act2 = Tanh()\n",
    "        self.act3 = Tanh()\n",
    "        #self.loss = loss\n",
    "        \n",
    "    #done by luca\n",
    "    def add(self, module):\n",
    "        self.modules.append(module)\n",
    "\n",
    "    def forward(self, input_): # , target\n",
    "        \n",
    "        s1 = self.fc1.forward(input_)\n",
    "        x1 = self.act1.forward(s1)\n",
    "        \n",
    "        s2 = self.fc2.forward(x1)\n",
    "        x2 = self.act2.forward(s2)\n",
    "        \n",
    "        s3 = self.fc3.forward(x2)\n",
    "        x3 = self.act3.forward(s3)\n",
    "        \n",
    "        return x3\n",
    "    \n",
    "    def backward(self, out):\n",
    "        \n",
    "        sigma3_p = self.act3.backward(out)\n",
    "        dl_ds3 = self.fc3.backward_last(sigma3_p)\n",
    "        \n",
    "        sigma2_p = self.act2.backward(dl_ds3)\n",
    "        dl_ds2  = self.fc2.backward(sigma2_p)\n",
    "        \n",
    "        sigma1_p = self.act1.backward(dl_ds2)\n",
    "        dl_ds1 = self.fc1.backward(sigma1_p)\n",
    "    \n",
    "    def zero_grads(self):\n",
    "        self.fc1.zero_grads()\n",
    "        self.fc2.zero_grads()\n",
    "        self.fc3.zero_grads()\n",
    "        \n",
    "            \n",
    "    def param ( self ):\n",
    "        params = []\n",
    "        params = params + self.fc1.param() + self.fc2.param() + self.fc3.param()\n",
    "        return params\n",
    "            \n",
    "    def update_params( self, eta ):\n",
    "        self.fc1.update_params(eta)\n",
    "        self.fc2.update_params(eta)\n",
    "        self.fc3.update_params(eta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prof version\n",
    "\n",
    "######################################################################\n",
    "\n",
    "def loss(v, t):\n",
    "    return (v - t).pow(2).sum()\n",
    "\n",
    "def dloss(v, t):\n",
    "    return 2 * (v - t)\n",
    "\n",
    "######################################################################\n",
    "# from F\n",
    "class Model(Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5)\n",
    "        self.conv2 = nn.Conv2d(20, 20, 5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        return F.relu(self.conv2(x))\n",
    "\n",
    "def linear(input, weight, bias=None):\n",
    "    \n",
    "    output = input.matmul(weight.t())\n",
    "    if bias is not None:\n",
    "        output += bias\n",
    "    return output\n",
    "\n",
    "\n",
    "class Linear(Module):\n",
    "    \n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.linear(input, self.weight, self.bias)\n",
    "\n",
    "######################################################################\n",
    "################# A first model ######################################\n",
    "def linear_forward(x,w,b):\n",
    "    s = w.mv(x) + b\n",
    "    x = sigma(s)\n",
    "    return s,x\n",
    "\n",
    "def linear_backward(x0, x, dl_ds_prev, dl_dw, dl_db, initial = False):\n",
    "    \n",
    "    dl_dx = w.t().mv(dl_ds_prev)\n",
    "    dl_ds = dsigma(s1) * dl_dx   \n",
    "    dl_dw.add_(dl_ds.view(-1, 1).mm(x0.view(1, -1)))\n",
    "    dl_db.add_(dl_ds)\n",
    "\n",
    "def forward_pass(ws, bs, x, test = False):\n",
    "    x0 = x\n",
    "    \n",
    "    s1, x1 = linear_forward(x0,ws[0],bs[0])\n",
    "    s2, x2 = linear_forward(x1,ws[1],bs[1])\n",
    "    \n",
    "    xs = [x1, x2]\n",
    "    ss = [s1, s2]\n",
    "    if test: \n",
    "        return xs[-1]\n",
    "    return x0, xs, ss\n",
    "\n",
    "\n",
    "def backward_pass(ws, bs,\n",
    "                  t,\n",
    "                  x, xs, ss,\n",
    "                  dl_dws, dl_dbs):\n",
    "    x0 = x\n",
    "    \n",
    "    dl_dx2 = dloss(xs[-1], t)\n",
    "    dl_ds2 = dsigma(ss[2-1]) * dl_dx2\n",
    "    dl_dws[2-1].add_(dl_ds2.view(-1, 1).mm(xs[1-1].view(1, -1)))\n",
    "    dl_dbs[2-1].add_(dl_ds2)\n",
    "    \n",
    "    dl_dx1 = ws[2-1].t().mv(dl_ds2) # w2\n",
    "    dl_ds1 = dsigma(ss[1-1]) * dl_dx1   \n",
    "    \n",
    "    dl_dws[1-1].add_(dl_ds1.view(-1, 1).mm(x0.view(1, -1)))\n",
    "    dl_dbs[1-1].add_(dl_ds1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_input.size():  torch.Size([1000, 2])\n",
      "-6.00436487729894e-06\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "bool value of non-empty torch.ByteTensor objects is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-33190ad6df85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# check wether the target was 1 or -1 --> verify if positif\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0mnb_train_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb_train_errors\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# if == -1 lets say :p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0macc_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         raise RuntimeError(\"bool value of non-empty \" + torch.typename(self) +\n\u001b[0;32m--> 140\u001b[0;31m                            \" objects is ambiguous\")\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0m__nonzero__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: bool value of non-empty torch.ByteTensor objects is ambiguous"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "zeta = 0.9\n",
    "train_input = train_input\n",
    "test_input = test_input\n",
    "\n",
    "nb_hidden=50\n",
    "print ('train_input.size(): ', train_input.size())\n",
    "nb_classes = 2 #train_target.size(1) \n",
    "nb_train_samples = train_input.size(0)\n",
    "\n",
    "eta = 0.1 / train_target.size(0)\n",
    "\n",
    "eps = 1e-6\n",
    "\n",
    "# weights and biases\n",
    "w1 = Tensor(nb_hidden, train_input.size(1)).normal_(0,1)\n",
    "b1 = Tensor(nb_hidden).normal_(0,1)\n",
    "w2 = Tensor(nb_classes, nb_hidden).normal_(0,eps)\n",
    "b2 = Tensor(nb_classes).normal_(0,eps)\n",
    "\n",
    "# lists\n",
    "ws = [w1, w2]\n",
    "bs = [b1, b2]\n",
    "# derivatives of the losse wrt weights and biases\n",
    "dl_dws = []\n",
    "dl_dbs = []\n",
    "for w in ws:\n",
    "    dl_dws.append(Tensor(w.size()))\n",
    "for b in bs:\n",
    "    dl_dbs.append(Tensor(b.size()))\n",
    "\n",
    "\n",
    "epochs = 250\n",
    "for k in range (0,epochs):\n",
    "    \n",
    "    acc_loss = 0\n",
    "    nb_train_errors = 0\n",
    "    \n",
    "    # set the storage to 0\n",
    "    for i in range(0, len(dl_dws)):\n",
    "        dl_dws[i].zero_()\n",
    "        dl_dbs[i].zero_()\n",
    "    \n",
    "    # for each sample run forward and backward pass\n",
    "    for n in range(0, nb_train_samples):\n",
    "        \n",
    "        # run forward pass\n",
    "        x0, xs, ss = forward_pass(ws, bs, train_input[n])\n",
    "        \n",
    "        # prediction is the maximum predicted class\n",
    "        \n",
    "        predicted = xs[-1].max(dim = 0)[1] # dim is the axis, 1 for taking index, 0 to just select the value\n",
    "        pred = predicted [0]\n",
    "        #print(predicted)\n",
    "        pred = xs[-1][0]\n",
    "        #print(xs[-1])\n",
    "        print(pred)\n",
    "        # check wether the target was 1 or -1 --> verify if positif\n",
    "        if train_target[n] != int(pred) : \n",
    "            nb_train_errors = nb_train_errors + 1 # if == -1 lets say :p \n",
    "        acc_loss += loss(Tensor(1).fill_(pred), train_target[n])\n",
    "        #acc_loss += loss(pred, train_target[n])\n",
    "\n",
    "        # run backward pass\n",
    "        backward_pass(ws, bs,\n",
    "                      train_target[n],\n",
    "                      x0, xs, ss,\n",
    "                      dl_dws, dl_dbs)\n",
    "    \n",
    "    # Gradient step\n",
    "    for i in range(0, len(ws)):\n",
    "        ws[i] = ws[i] - eta * dl_dws[i]\n",
    "        bs[i] = bs[i] - eta * dl_dbs[i]\n",
    "\n",
    "    # Test error\n",
    "    nb_test_errors = 0\n",
    "    for n in range(0, test_input.size(0)):\n",
    "        output = forward_pass(ws, bs, test_input[n], test=True)\n",
    "\n",
    "        pred = output.max(0)[1][0]\n",
    "        if test_target[n] != int(output[0]) : nb_test_errors = nb_test_errors + 1  \n",
    "\n",
    "\n",
    "    print(k,' --> acc_train_loss {:.02f} acc_train_error {:.02f}% test_error {:.02f}%'\n",
    "          .format(acc_loss,\n",
    "                  (100 * nb_train_errors) / train_input.size(0),\n",
    "                  (100 * nb_test_errors) / test_input.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
