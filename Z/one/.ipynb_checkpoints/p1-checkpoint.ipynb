{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO'S\n",
    "\n",
    " - Data augmentation\n",
    "     - shift\n",
    "     - mirror?? (maybe not)\n",
    "     - PCA? (why not)\n",
    " - reduce overfitting\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "from torch import Tensor\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from models import *\n",
    "\n",
    "import numpy as np\n",
    "# to load the data\n",
    "import dlc_bci as bci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test the imported data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.FloatTensor'> torch.Size([316, 28, 50])\n",
      "<class 'torch.LongTensor'> torch.Size([316])\n",
      "<class 'torch.FloatTensor'> torch.Size([100, 28, 50])\n",
      "<class 'torch.LongTensor'> torch.Size([100])\n",
      "train --- mean 24.03, std: 41.49 target mean:  0.496835443038\n",
      "test --- mean 21.69, std: 40.83 target mean:  0.51\n",
      "<class 'torch.autograd.variable.Variable'> torch.Size([316, 28, 50])\n",
      "<class 'torch.autograd.variable.Variable'> torch.Size([316])\n"
     ]
    }
   ],
   "source": [
    "#CRIT = 'MSE'\n",
    "CRIT = 'cross'\n",
    "\n",
    "HD_SIGNAL = False\n",
    "\n",
    "if HD_SIGNAL == True : SIGNAL_LENGTH = 500\n",
    "elif HD_SIGNAL == False : SIGNAL_LENGTH = 50\n",
    "    \n",
    "\n",
    "train_input, train_target = bci.load ( root = './data_bci',train=True, download=False, one_khz=HD_SIGNAL)\n",
    "print( str( type(train_input) ), train_input.size () )\n",
    "print( str( type(train_target) ), train_target.size () )\n",
    "\n",
    "test_input , test_target = bci.load ( root = './data_bci' , train = False, download=False, one_khz=HD_SIGNAL)\n",
    "print( str( type(test_input) ) , test_input.size () )\n",
    "print( str( type(test_target) ) , test_target.size () )\n",
    "\n",
    "print('train --- mean {:0.2f}, std: {:0.2f}'.format(train_input.mean(),train_input.std()),\\\n",
    "     'target mean: ',train_target.numpy().mean())\n",
    "print('test --- mean {:0.2f}, std: {:0.2f}'.format(test_input.mean(),test_input.std()),\\\n",
    "     'target mean: ',test_target.numpy().mean())\n",
    "\n",
    "\n",
    "# transform into one-hot-encoding\n",
    "def convert_to_one_hot(data):\n",
    "    return np.eye(2)[data]\n",
    "\n",
    "#one_hot_targets = np.eye(2)[train_target]\n",
    "#train_target = one_hot_targets\n",
    "if CRIT=='MSE':\n",
    "    train_target = convert_to_one_hot(train_target)\n",
    "    test_target = convert_to_one_hot(test_target)\n",
    "\n",
    "\n",
    "# Convert them into Variables\n",
    "train_input = Variable(Tensor(train_input))\n",
    "test_input = Variable(test_input)\n",
    "if CRIT == 'MSE':\n",
    "    train_target = Variable(torch.Tensor(train_target))\n",
    "    test_target = Variable(torch.Tensor(test_target))\n",
    "elif CRIT == 'cross':\n",
    "    train_target = Variable(torch.LongTensor(train_target))\n",
    "    test_target = Variable(torch.LongTensor(test_target))\n",
    "\n",
    "# normalise the data (note that the channels differ sometimes a lot)\n",
    "def normalize_data(dataset):\n",
    "    mu, std = dataset.mean(0), dataset.std(0) # normalisation is done component-wise\n",
    "    return dataset.sub(mu).div(std)\n",
    "    \n",
    "#train_input = normalize_data(train_input)\n",
    "#test_input = normalize_data(test_input)\n",
    "\n",
    "print( str( type(train_input) ), train_input.size () )\n",
    "print( str( type(train_target) ), train_target.size () )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"def create_model_1():\n",
    "    return Net(nb_hidden=100, nb_init_filters = 16, nb_convs=2, kernel_size=5, length_signal = SIGNAL_LENGTH )\"\"\"\n",
    "\n",
    "class Net_seq(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden = 100, num_classes=2, nl='relu',iterations = 0,length_signal = SIGNAL_LENGTH ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.dropping_prob = 0.5\n",
    "        if nl == 'leaky': f_non_linearity = nn.LeakyReLU(negative_slope=0.01,inplace=True)\n",
    "        elif nl == 'tanh' : f_non_linearity =  nn.Tanh()\n",
    "        elif nl == 'relu': f_non_linearity = nn.ReLU(inplace=True)\n",
    "            \n",
    "        filter_size = 3\n",
    "        my_dropout = nn.Dropout(p=0.6)\n",
    "        ################### layers ################\n",
    "        \n",
    "        f_layers = []\n",
    "        f_layers.append(nn.Conv1d(28, 8, kernel_size=filter_size)) # 7 \n",
    "        f_layers.append(f_non_linearity)\n",
    "        \n",
    "        f_layers.append(nn.Conv1d(8, 8, kernel_size=filter_size))\n",
    "        f_layers.append(f_non_linearity)\n",
    "        f_layers.append(nn.BatchNorm1d(8, affine=True))\n",
    "        f_layers.append(my_dropout)\n",
    "        #f_layers.append(nn.MaxPool1d(kernel_size=2))\n",
    "        \n",
    "        \n",
    "        for i in range (iterations):\n",
    "            f_layers.append(nn.Conv1d(4, 4, kernel_size=3))\n",
    "            f_layers.append(f_non_linearity)\n",
    "            f_layers.append(my_dropout)\n",
    "        \n",
    "        f_layers.append(nn.Conv1d(8, 16, kernel_size=filter_size))\n",
    "        f_layers.append(f_non_linearity)\n",
    "        f_layers.append(nn.BatchNorm1d(16, affine=True))\n",
    "        f_layers.append(my_dropout)\n",
    "        \n",
    "        self.features = nn.Sequential(*f_layers)\n",
    "        \n",
    "        self.final_filters = length_signal - 3*(filter_size//2)*2 #-iterations*2\n",
    "        self.final_length = 16\n",
    "        \n",
    "        c_layers = [] \n",
    "        c_layers.append(nn.Linear(self.final_filters*self.final_length, hidden))\n",
    "        c_layers.append(nn.ReLU(inplace=True))\n",
    "        #c_layers.append(nn.BatchNorm1d(hidden, eps=1e-05, momentum=0.1, affine=False))\n",
    "        #c_layers.append(nn.Dropout(inplace=True))\n",
    "        \"\"\"c_layers.append(nn.Linear(hidden, hidden))\n",
    "        c_layers.append(nn.ReLU(inplace=True))\n",
    "        c_layers.append(my_dropout)\n",
    "        c_layers.append(nn.Linear(hidden, hidden))\n",
    "        c_layers.append(nn.ReLU(inplace=True))\"\"\"\n",
    "        c_layers.append(nn.Linear(hidden, num_classes))\n",
    "        #c_layers.append(nn.Softmax(dim=1))\n",
    "        \n",
    "        self.classifier = nn.Sequential(* c_layers)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(-1, self.final_filters*self.final_length)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "# 14% with just 2 conv layers 26,16 filter 5 ;; 16,32, filter 5, 2 fcl with hidden = 100\n",
    "\n",
    "def create_net_seq():\n",
    "    return Net_seq()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mini_batch_size = 70\n",
    "epochs = 400\n",
    "\n",
    "eta = 0.008\n",
    "\n",
    "if CRIT == 'MSE':\n",
    "    criterion = nn.MSELoss()\n",
    "elif CRIT == 'cross':\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#torch.norm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, data_input, data_target, mini_batch_size = mini_batch_size):\n",
    "    nb_errors = 0\n",
    "    for i in range(0, data_input.size(0)):\n",
    "        model.train(False)\n",
    "        if model.training == False:\n",
    "            output = model.forward(data_input[i].view(1,28,SIGNAL_LENGTH))\n",
    "        else : print('DROPING OUT IN TESTING')\n",
    "        #print(output)\n",
    "        _, predicted = torch.max(output.data, 1)                            \n",
    "        true_val = data_target.data[i]\n",
    "        if CRIT == 'MSE':\n",
    "            true_val = true_val[0]\n",
    "        predicted = predicted[0]\n",
    "        #print(true_val)\n",
    "        #print(predicted)\n",
    "        if (true_val!= predicted):\n",
    "            nb_errors += 1\n",
    "        #print('LALALALALALA')\n",
    "    return nb_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target, mini_batch_size):\n",
    "    \n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr = eta)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0)\n",
    "    #optimizer = torch.optim.LBFGS(model.parameters(), lr=1, max_iter=20, max_eval=None, tolerance_grad=1e-05)\n",
    "    #optimizer = torch.optim.SparseAdam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n",
    "    \n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    for e in range(epochs):\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            model.train(True)\n",
    "            if model.training == True:\n",
    "                output = model.forward(train_input[b:b+mini_batch_size])\n",
    "            else : print('NOT TRAINING')\n",
    "                \n",
    "            loss = criterion(output, train_target[b:b+mini_batch_size])\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \"\"\"# for L1 normalisation\n",
    "            for p in model.parameters():\n",
    "                p.data -= p.data.sign() * p.data.abs().clamp(max = 0.001)\"\"\"\n",
    "\n",
    "        if (e%10==0):\n",
    "            train_errors = compute_nb_errors(model, data_input=train_input, data_target=train_target)\n",
    "            test_errors = compute_nb_errors(model, data_input=test_input, data_target=test_target)\n",
    "            train_list.append(train_errors)\n",
    "            test_list.append(test_errors)\n",
    "            print('epoch #',e,\\\n",
    "                  'Train: {:0.2f}%'.format( train_errors/train_input.size(0)*100),\\\n",
    "                  'Test:  {:0.2f}%'.format(test_errors / test_input.size(0)*100),\\\n",
    "                  \n",
    "            )\n",
    "            #print('Loss:  {:0.5f} '.format(loss.data[0]))\n",
    "    return train_list, test_list\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Zone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_net_seq\n",
      "epoch # 0 Train: 50.32% Test:  50.00%\n",
      "epoch # 10 Train: 43.35% Test:  54.00%\n",
      "epoch # 20 Train: 42.72% Test:  43.00%\n",
      "epoch # 30 Train: 37.97% Test:  47.00%\n",
      "epoch # 40 Train: 36.08% Test:  50.00%\n",
      "epoch # 50 Train: 35.44% Test:  47.00%\n",
      "epoch # 60 Train: 34.81% Test:  47.00%\n",
      "epoch # 70 Train: 32.28% Test:  51.00%\n",
      "epoch # 80 Train: 30.38% Test:  53.00%\n",
      "epoch # 90 Train: 31.65% Test:  52.00%\n",
      "epoch # 100 Train: 29.11% Test:  54.00%\n",
      "epoch # 110 Train: 29.75% Test:  49.00%\n",
      "epoch # 120 Train: 26.27% Test:  48.00%\n",
      "epoch # 130 Train: 26.58% Test:  48.00%\n",
      "epoch # 140 Train: 23.73% Test:  49.00%\n",
      "epoch # 150 Train: 24.68% Test:  50.00%\n",
      "epoch # 160 Train: 22.47% Test:  50.00%\n",
      "epoch # 170 Train: 20.89% Test:  47.00%\n",
      "epoch # 180 Train: 19.94% Test:  48.00%\n",
      "epoch # 190 Train: 18.67% Test:  47.00%\n",
      "epoch # 200 Train: 18.35% Test:  46.00%\n",
      "epoch # 210 Train: 15.82% Test:  45.00%\n",
      "epoch # 220 Train: 15.51% Test:  47.00%\n",
      "epoch # 230 Train: 16.46% Test:  46.00%\n",
      "epoch # 240 Train: 13.92% Test:  43.00%\n",
      "epoch # 250 Train: 15.19% Test:  41.00%\n",
      "epoch # 260 Train: 12.66% Test:  40.00%\n",
      "epoch # 270 Train: 11.71% Test:  42.00%\n",
      "epoch # 280 Train: 13.29% Test:  40.00%\n",
      "epoch # 290 Train: 10.44% Test:  40.00%\n",
      "epoch # 300 Train: 9.49% Test:  40.00%\n",
      "epoch # 310 Train: 7.59% Test:  42.00%\n",
      "epoch # 320 Train: 9.49% Test:  41.00%\n",
      "epoch # 330 Train: 8.86% Test:  39.00%\n",
      "epoch # 340 Train: 7.91% Test:  37.00%\n",
      "epoch # 350 Train: 7.59% Test:  37.00%\n",
      "epoch # 360 Train: 8.54% Test:  38.00%\n",
      "epoch # 370 Train: 7.91% Test:  36.00%\n",
      "epoch # 380 Train: 7.91% Test:  36.00%\n",
      "epoch # 390 Train: 6.96% Test:  39.00%\n"
     ]
    }
   ],
   "source": [
    "#model_list = [create_model_1, create_net_seq]\n",
    "for m in [create_net_seq]:\n",
    "    print(m.__name__)\n",
    "    model = m()\n",
    "    train_list, test_list = train_model(model, train_input, train_target, mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d1e7b1084d08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_list\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_list\u001b[0m\u001b[0;34m/\u001b[0m \u001b[0mtest_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "plt.plot(train_list/train_input.size(0)*100)\n",
    "plt.plot(test_list/ test_input.size(0)*100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
