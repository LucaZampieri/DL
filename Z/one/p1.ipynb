{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO'S\n",
    "\n",
    " - Data augmentation\n",
    "     - shift\n",
    "     - mirror?? (maybe not)\n",
    "     - PCA? (why not)\n",
    " - reduce overfitting\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "from torch import Tensor\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "\n",
    "from models import *\n",
    "\n",
    "import numpy as np\n",
    "# to load the data\n",
    "import dlc_bci as bci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test the imported data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.FloatTensor'> torch.Size([316, 28, 50])\n",
      "<class 'torch.LongTensor'> torch.Size([316])\n",
      "<class 'torch.FloatTensor'> torch.Size([100, 28, 50])\n",
      "<class 'torch.LongTensor'> torch.Size([100])\n",
      "train --- mean 24.03, std: 41.49 target mean:  0.496835443038\n",
      "test --- mean 21.69, std: 40.83 target mean:  0.51\n",
      "<class 'torch.autograd.variable.Variable'> torch.Size([316, 28, 50])\n",
      "<class 'torch.autograd.variable.Variable'> torch.Size([316])\n"
     ]
    }
   ],
   "source": [
    "#CRIT = 'MSE'\n",
    "CRIT = 'cross'\n",
    "\n",
    "HD_SIGNAL = False\n",
    "\n",
    "if HD_SIGNAL == True : SIGNAL_LENGTH = 500\n",
    "elif HD_SIGNAL == False : SIGNAL_LENGTH = 50\n",
    "    \n",
    "\n",
    "train_input, train_target = bci.load ( root = './data_bci',train=True, download=False, one_khz=HD_SIGNAL)\n",
    "print( str( type(train_input) ), train_input.size () )\n",
    "print( str( type(train_target) ), train_target.size () )\n",
    "\n",
    "test_input , test_target = bci.load ( root = './data_bci' , train = False, download=False, one_khz=HD_SIGNAL)\n",
    "print( str( type(test_input) ) , test_input.size () )\n",
    "print( str( type(test_target) ) , test_target.size () )\n",
    "\n",
    "print('train --- mean {:0.2f}, std: {:0.2f}'.format(train_input.mean(),train_input.std()),\\\n",
    "     'target mean: ',train_target.numpy().mean())\n",
    "print('test --- mean {:0.2f}, std: {:0.2f}'.format(test_input.mean(),test_input.std()),\\\n",
    "     'target mean: ',test_target.numpy().mean())\n",
    "\n",
    "\n",
    "# transform into one-hot-encoding\n",
    "def convert_to_one_hot(data):\n",
    "    return np.eye(2)[data]\n",
    "\n",
    "#one_hot_targets = np.eye(2)[train_target]\n",
    "#train_target = one_hot_targets\n",
    "if CRIT=='MSE':\n",
    "    train_target = convert_to_one_hot(train_target)\n",
    "    test_target = convert_to_one_hot(test_target)\n",
    "\n",
    "\n",
    "# Convert them into Variables\n",
    "train_input = Variable(Tensor(train_input))\n",
    "test_input = Variable(test_input)\n",
    "if CRIT == 'MSE':\n",
    "    train_target = Variable(torch.Tensor(train_target))\n",
    "    test_target = Variable(torch.Tensor(test_target))\n",
    "elif CRIT == 'cross':\n",
    "    train_target = Variable(torch.LongTensor(train_target))\n",
    "    test_target = Variable(torch.LongTensor(test_target))\n",
    "\n",
    "# normalise the data (note that the channels differ sometimes a lot)\n",
    "def normalize_data(dataset):\n",
    "    mu, std = dataset.mean(0), dataset.std(0) # normalisation is done component-wise\n",
    "    return dataset.sub(mu).div(std)\n",
    "    \n",
    "#train_input = normalize_data(train_input)\n",
    "#test_input = normalize_data(test_input)\n",
    "\n",
    "print( str( type(train_input) ), train_input.size () )\n",
    "print( str( type(train_target) ), train_target.size () )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def create_model_1():\n",
    "    return Net(nb_hidden=100, nb_init_filters = 16, nb_convs=2, kernel_size=5, length_signal = SIGNAL_LENGTH )\"\"\"\n",
    "\n",
    "class Net_seq(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden = 30, num_classes=2, nl='relu',length_signal = SIGNAL_LENGTH ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.dropping_prob = 0.5\n",
    "        if nl == 'leaky': f_non_linearity = nn.LeakyReLU(negative_slope=0.01,inplace=True)\n",
    "        elif nl == 'softmax' : f_non_linearity = nn.Softmax(dim=None) # not working\n",
    "        elif nl == 'tanh' : f_non_linearity =  nn.Tanh()\n",
    "        elif nl == 'relu': f_non_linearity = nn.ReLU(inplace=True)\n",
    "            \n",
    "        filter_size = 5\n",
    "            \n",
    "        f_layers = []\n",
    "        f_layers.append(nn.Conv1d(28, 8, kernel_size=filter_size)) # 7 \n",
    "        f_layers.append(f_non_linearity)\n",
    "        \n",
    "        f_layers.append(nn.Conv1d(8, 4, kernel_size=filter_size))\n",
    "        f_layers.append(f_non_linearity)\n",
    "        f_layers.append(nn.Dropout())\n",
    "        #f_layers.append(nn.MaxPool1d(kernel_size=2))\n",
    "        \n",
    "        for i in range (10):\n",
    "            f_layers.append(nn.Conv1d(4, 4, kernel_size=3))\n",
    "            f_layers.append(f_non_linearity)\n",
    "            f_layers.append(nn.Dropout())\n",
    "            \n",
    "        \n",
    "        f_layers.append(nn.Conv1d(4, 8, kernel_size=filter_size))\n",
    "        f_layers.append(f_non_linearity)\n",
    "        f_layers.append(nn.Dropout())\n",
    "        \n",
    "        f_layers.append(nn.Conv1d(8, 8, kernel_size=filter_size))\n",
    "        f_layers.append(f_non_linearity)\n",
    "        f_layers.append(nn.Dropout())\n",
    "        \n",
    "        self.features = nn.Sequential(*f_layers)\n",
    "        \n",
    "        self.final_filters = length_signal - 4*(filter_size//2)*2-10*2\n",
    "        self.final_length = 8\n",
    "        \n",
    "        c_layers = [] \n",
    "        c_layers.append(nn.Linear(self.final_filters*self.final_length, hidden))\n",
    "        c_layers.append(nn.ReLU(inplace=True))\n",
    "        #c_layers.append(nn.BatchNorm1d(hidden, eps=1e-05, momentum=0.1, affine=False))\n",
    "        #c_layers.append(nn.Dropout(inplace=True))\n",
    "        #c_layers.append(nn.Linear(hidden, hidden))\n",
    "        #c_layers.append(nn.ReLU(inplace=True))\n",
    "        c_layers.append(nn.Linear(hidden, num_classes))\n",
    "                        \n",
    "        self.classifier = nn.Sequential(* c_layers)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(-1, self.final_filters*self.final_length)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "# 14% with just 2 conv layers 26,16 filter 5 ;; 16,32, filter 5, 2 fcl with hidden = 100\n",
    "\n",
    "def create_net_seq():\n",
    "    return Net_seq()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mini_batch_size = 30\n",
    "epochs = 1000\n",
    "\n",
    "eta = 0.008\n",
    "\n",
    "if CRIT == 'MSE':\n",
    "    criterion = nn.MSELoss()\n",
    "elif CRIT == 'cross':\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#torch.norm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, data_input, data_target, mini_batch_size = mini_batch_size):\n",
    "    nb_errors = 0\n",
    "    for i in range(0, data_input.size(0)):\n",
    "        model.train(False)\n",
    "        if model.training == False:\n",
    "            output = model.forward(data_input[i].view(1,28,SIGNAL_LENGTH))\n",
    "        else : print('DROPING OUT IN TESTING')\n",
    "        #print(output)\n",
    "        _, predicted = torch.max(output.data, 1)                            \n",
    "        true_val = data_target.data[i]\n",
    "        if CRIT == 'MSE':\n",
    "            true_val = true_val[0]\n",
    "        predicted = predicted[0]\n",
    "        #print(true_val)\n",
    "        #print(predicted)\n",
    "        if (true_val!= predicted):\n",
    "            nb_errors += 1\n",
    "        #print('LALALALALALA')\n",
    "    return nb_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target, mini_batch_size):\n",
    "    \n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr = eta)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=eta, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "    #optimizer = torch.optim.LBFGS(model.parameters(), lr=1, max_iter=20, max_eval=None, tolerance_grad=1e-05)\n",
    "    #optimizer = torch.optim.SparseAdam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n",
    "    for e in range(epochs):\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            model.train(True)\n",
    "            if model.training == True:\n",
    "                output = model.forward(train_input[b:b+mini_batch_size])\n",
    "            else : print('NOT TRAINING')\n",
    "                \n",
    "            loss = criterion(output, train_target[b:b+mini_batch_size])\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \"\"\"# for L1 normalisation\n",
    "            for p in model.parameters():\n",
    "                p.data -= p.data.sign() * p.data.abs().clamp(max = 0.001)\"\"\"\n",
    "\n",
    "        if (e%10==0):\n",
    "            train_errors = compute_nb_errors(model, data_input=train_input, data_target=train_target)\n",
    "            test_errors = compute_nb_errors(model, data_input=test_input, data_target=test_target)\n",
    "            print('epoch #',e,\\\n",
    "                  'Train: {:0.2f}%'.format( train_errors/train_input.size(0)*100),\\\n",
    "                  'Test:  {:0.2f}%'.format(test_errors / test_input.size(0)*100),\\\n",
    "                  \n",
    "            )\n",
    "            #print('Loss:  {:0.5f} '.format(loss.data[0]))\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Zone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_net_seq\n",
      "epoch # 0 Train: 50.32% Test:  49.00%\n",
      "epoch # 10 Train: 49.68% Test:  51.00%\n",
      "epoch # 20 Train: 49.68% Test:  51.00%\n",
      "epoch # 30 Train: 49.68% Test:  51.00%\n",
      "epoch # 40 Train: 49.68% Test:  51.00%\n",
      "epoch # 50 Train: 49.68% Test:  51.00%\n",
      "epoch # 60 Train: 49.68% Test:  51.00%\n",
      "epoch # 70 Train: 49.68% Test:  51.00%\n",
      "epoch # 80 Train: 49.68% Test:  51.00%\n",
      "epoch # 90 Train: 49.68% Test:  51.00%\n",
      "epoch # 100 Train: 49.68% Test:  51.00%\n",
      "epoch # 110 Train: 49.68% Test:  51.00%\n",
      "epoch # 120 Train: 49.68% Test:  51.00%\n",
      "epoch # 130 Train: 49.68% Test:  51.00%\n",
      "epoch # 140 Train: 49.68% Test:  51.00%\n",
      "epoch # 150 Train: 49.68% Test:  51.00%\n",
      "epoch # 160 Train: 49.68% Test:  51.00%\n",
      "epoch # 170 Train: 49.68% Test:  51.00%\n",
      "epoch # 180 Train: 49.68% Test:  51.00%\n",
      "epoch # 190 Train: 49.68% Test:  51.00%\n",
      "epoch # 200 Train: 49.68% Test:  51.00%\n",
      "epoch # 210 Train: 49.68% Test:  51.00%\n",
      "epoch # 220 Train: 49.68% Test:  51.00%\n",
      "epoch # 230 Train: 49.68% Test:  51.00%\n",
      "epoch # 240 Train: 49.68% Test:  51.00%\n",
      "epoch # 250 Train: 49.68% Test:  51.00%\n",
      "epoch # 260 Train: 49.68% Test:  51.00%\n",
      "epoch # 270 Train: 49.68% Test:  51.00%\n",
      "epoch # 280 Train: 49.68% Test:  51.00%\n",
      "epoch # 290 Train: 49.68% Test:  51.00%\n",
      "epoch # 300 Train: 49.68% Test:  51.00%\n",
      "epoch # 310 Train: 49.68% Test:  51.00%\n",
      "epoch # 320 Train: 49.68% Test:  51.00%\n",
      "epoch # 330 Train: 49.68% Test:  51.00%\n",
      "epoch # 340 Train: 49.68% Test:  51.00%\n",
      "epoch # 350 Train: 49.68% Test:  51.00%\n",
      "epoch # 360 Train: 49.68% Test:  51.00%\n",
      "epoch # 370 Train: 49.68% Test:  51.00%\n",
      "epoch # 380 Train: 49.68% Test:  51.00%\n",
      "epoch # 390 Train: 49.68% Test:  51.00%\n",
      "epoch # 400 Train: 49.68% Test:  51.00%\n",
      "epoch # 410 Train: 49.68% Test:  51.00%\n",
      "epoch # 420 Train: 49.68% Test:  51.00%\n",
      "epoch # 430 Train: 49.68% Test:  51.00%\n",
      "epoch # 440 Train: 49.68% Test:  51.00%\n",
      "epoch # 450 Train: 49.68% Test:  51.00%\n",
      "epoch # 460 Train: 49.68% Test:  51.00%\n",
      "epoch # 470 Train: 49.68% Test:  51.00%\n",
      "epoch # 480 Train: 49.68% Test:  51.00%\n",
      "epoch # 490 Train: 49.68% Test:  51.00%\n",
      "epoch # 500 Train: 49.68% Test:  51.00%\n",
      "epoch # 510 Train: 49.68% Test:  51.00%\n",
      "epoch # 520 Train: 49.68% Test:  51.00%\n",
      "epoch # 530 Train: 49.68% Test:  51.00%\n",
      "epoch # 540 Train: 49.68% Test:  51.00%\n",
      "epoch # 550 Train: 49.68% Test:  51.00%\n",
      "epoch # 560 Train: 49.68% Test:  51.00%\n",
      "epoch # 570 Train: 49.68% Test:  51.00%\n",
      "epoch # 580 Train: 49.68% Test:  51.00%\n",
      "epoch # 590 Train: 49.68% Test:  51.00%\n",
      "epoch # 600 Train: 49.68% Test:  51.00%\n",
      "epoch # 610 Train: 49.68% Test:  51.00%\n",
      "epoch # 620 Train: 49.68% Test:  51.00%\n",
      "epoch # 630 Train: 49.68% Test:  51.00%\n",
      "epoch # 640 Train: 49.68% Test:  51.00%\n",
      "epoch # 650 Train: 49.68% Test:  51.00%\n",
      "epoch # 660 Train: 49.68% Test:  51.00%\n",
      "epoch # 670 Train: 49.68% Test:  51.00%\n",
      "epoch # 680 Train: 49.68% Test:  51.00%\n",
      "epoch # 690 Train: 49.68% Test:  51.00%\n",
      "epoch # 700 Train: 49.68% Test:  51.00%\n",
      "epoch # 710 Train: 49.68% Test:  51.00%\n",
      "epoch # 720 Train: 49.68% Test:  51.00%\n",
      "epoch # 730 Train: 49.68% Test:  51.00%\n",
      "epoch # 740 Train: 49.68% Test:  51.00%\n",
      "epoch # 750 Train: 49.68% Test:  51.00%\n",
      "epoch # 760 Train: 49.68% Test:  51.00%\n",
      "epoch # 770 Train: 49.68% Test:  51.00%\n",
      "epoch # 780 Train: 49.68% Test:  51.00%\n",
      "epoch # 790 Train: 49.68% Test:  51.00%\n",
      "epoch # 800 Train: 49.68% Test:  51.00%\n",
      "epoch # 810 Train: 49.68% Test:  51.00%\n",
      "epoch # 820 Train: 49.68% Test:  51.00%\n",
      "epoch # 830 Train: 49.68% Test:  51.00%\n",
      "epoch # 840 Train: 49.68% Test:  51.00%\n",
      "epoch # 850 Train: 49.68% Test:  51.00%\n",
      "epoch # 860 Train: 49.68% Test:  51.00%\n",
      "epoch # 870 Train: 49.68% Test:  51.00%\n",
      "epoch # 880 Train: 49.68% Test:  51.00%\n",
      "epoch # 890 Train: 49.68% Test:  51.00%\n",
      "epoch # 900 Train: 49.68% Test:  51.00%\n",
      "epoch # 910 Train: 49.68% Test:  51.00%\n",
      "epoch # 920 Train: 49.68% Test:  51.00%\n",
      "epoch # 930 Train: 49.68% Test:  51.00%\n",
      "epoch # 940 Train: 49.68% Test:  51.00%\n",
      "epoch # 950 Train: 49.68% Test:  51.00%\n",
      "epoch # 960 Train: 49.68% Test:  51.00%\n",
      "epoch # 970 Train: 49.68% Test:  51.00%\n",
      "epoch # 980 Train: 49.68% Test:  51.00%\n",
      "epoch # 990 Train: 49.68% Test:  51.00%\n"
     ]
    }
   ],
   "source": [
    "#model_list = [create_model_1, create_net_seq]\n",
    "for m in [create_net_seq]:\n",
    "    print(m.__name__)\n",
    "    model = m()\n",
    "    train_model(model, train_input, train_target, mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
