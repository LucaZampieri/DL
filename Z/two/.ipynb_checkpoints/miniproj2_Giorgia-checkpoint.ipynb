{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1.12\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "print (torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ciao \n",
      "    1\n",
      "    0\n",
      "    0\n",
      "  â‹®   \n",
      "    0\n",
      "    0\n",
      "    1\n",
      "[torch.FloatTensor of size 1000x1]\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "bool value of non-empty torch.ByteTensor objects is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6d76765c17b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mnb_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# create train set and respective labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtrain_input\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;31m# create test set and respective labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mtest_input\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtest_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-6d76765c17b7>\u001b[0m in \u001b[0;36mdisk\u001b[0;34m(nb_points)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ciao'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         raise RuntimeError(\"bool value of non-empty \" + torch.typename(self) +\n\u001b[0;32m--> 140\u001b[0;31m                            \" objects is ambiguous\")\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0m__nonzero__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: bool value of non-empty torch.ByteTensor objects is ambiguous"
     ]
    }
   ],
   "source": [
    "def disk(nb_points) :\n",
    "    radius = 1/np.sqrt(2*np.pi)\n",
    "    inp = Tensor(nb_points,2).uniform_(0,1)\n",
    "    ratio = torch.floor(torch.norm(inp,p=2,dim=1)/radius)\n",
    "    target = 1-torch.clamp(ratio,min=0,max=1)\n",
    "    t = Tensor(nb_points,2)\n",
    "    print('ciao',target)\n",
    "    for i in range(nb_points):\n",
    "        if target[i] == 0:\n",
    "            t[i,:] = Tensor([-1,1])\n",
    "        elif target[i] == 1:\n",
    "            t[i,:] = Tensor([1,-1])\n",
    "    print(t)    \n",
    "    return inp, t\n",
    "\n",
    "nb_points = 1000\n",
    "# create train set and respective labels\n",
    "train_input , train_target = disk(nb_points)\n",
    "# create test set and respective labels\n",
    "test_input , test_target = disk(nb_points)\n",
    "\n",
    "\"\"\"\n",
    "target=torch.zeros(train_target.shape[0],2)-1\n",
    "target[:,0][train_target==0]=1\n",
    "target[:,1][train_target==1]=1\n",
    "\"\"\"\n",
    "#print(train_input, train_target, test_input, test_target)\n",
    "\n",
    "# data normalization\n",
    "# , std_train = train_input.mean() , train_input.std()\n",
    "#train_input.sub_(mu_train).div_(std_train)\n",
    "#mu_test , std_test = test_input.mean() , test_input.std()\n",
    "#test_input.sub_(mu_test).div_(std_test)\n",
    "# later see whether it is convenient to normalize component-wise with broadcasting (pag 66 week5)\n",
    "\n",
    "# then do sanity check to see if the two classes are balanced\n",
    "print(train_target[-1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Module ( object ) :\n",
    "    def forward ( self , * input ) :\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward ( self , * gradwrtoutput ) :\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def param ( self ) :\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return x.tanh()\n",
    "\n",
    "def dtanh(x):\n",
    "    return 1-torch.tanh(x).pow(2)\n",
    "\n",
    "\n",
    "class Sigma(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,input):\n",
    "        return tanh(input)\n",
    "    \n",
    "    def backward(self, output):\n",
    "        #return dtanh(input)*output\n",
    "        return 4 * (output.exp() + output.mul(-1).exp()).pow(-2)\n",
    "    \n",
    "    # here you need to add \"def param\" too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return max(0,x)\n",
    "\n",
    "def drelu(x):\n",
    "    if x>0:\n",
    "        out = 1\n",
    "    elif x<0:\n",
    "        out = 0\n",
    "    return out\n",
    "\n",
    "class Relu(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,input):\n",
    "        self.input = input\n",
    "        return relu(input)\n",
    "    \n",
    "    def backward(self,output):\n",
    "        return drelu(output) #to check, probably incorrect\n",
    "    \n",
    "    # here you need to add \"def param\" too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(v, t):\n",
    "    return (v - t).pow(2).sum()\n",
    "\n",
    "def dloss(v, t):\n",
    "    return 2 * (v - t)\n",
    "\n",
    "class Loss(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        self.input = input\n",
    "        return loss(input, target)\n",
    "    \n",
    "    def backward(self, output):\n",
    "        return dloss(output,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# definition of Linear\n",
    "class Linear(Module):\n",
    "    # in_features: size of each input sample\n",
    "    # out_features: size of each output sample\n",
    "    # bias: If set to False, the layer will not learn an additive bias. Default: ``True``\n",
    "\n",
    "    # Attributes:\n",
    "    # weight: the learnable weights of the module of shape (out_features x in_features)`\n",
    "    # bias:   the learnable bias of the module of shape `(out_features)`\n",
    "\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weights = Tensor(out_features,in_features).normal_(0,1e-6)\n",
    "        self.bias = Tensor(out_features).zero_()\n",
    "        self.dl_dw = Tensor(out_features,in_features).zero_()\n",
    "        self.dl_db = Tensor(out_features).zero_()\n",
    "                \n",
    "    def forward(self,input):\n",
    "        self.input = input\n",
    "        #print('ecco i pesi:',self.weights)\n",
    "        self.s = torch.mv(self.weights,input)\n",
    "        return torch.mv(self.weights,input)\n",
    "    \n",
    "    def backward_last(self, output, target): # output would be x3 \n",
    "        dl_dx = dloss(output, target)\n",
    "        dl_ds = Sigma().backward(self.s)*dl_dx \n",
    "        self.dl_dw.add_(dl_ds.view(-1, 1).mm(self.input.view(1, -1)))  \n",
    "        self.dl_db.add_(dl_ds)\n",
    "        #print('last layer',self.dl_dw,'dl_ds', dl_ds)\n",
    "        return dl_ds, self.weights\n",
    "    \n",
    "    def backward(self, dl_ds_next, w_next):  \n",
    "        #dl_dx = self.weights.t().mv(dl_ds_next)   # the problem is here! \n",
    "        dl_dx = w_next.t().mv(dl_ds_next)\n",
    "        dl_ds = Sigma().backward(self.s)*dl_dx \n",
    "        self.dl_dw.add_(dl_ds.view(-1, 1).mm(self.input.view(1, -1)))   \n",
    "        self.dl_db.add_(dl_ds)\n",
    "        return dl_ds, self.weights      \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(Module):\n",
    "    def __init__(self):\n",
    "        self.fc1 = Linear(input_size,hidden_units)  # first hidden layer\n",
    "        self.fc2 = Linear(hidden_units,hidden_units) # second hidden layer\n",
    "        self.fc3 = Linear(hidden_units,output_size) # third hidden layer\n",
    "   \n",
    "    def forward(self,x):\n",
    "        s1 = self.fc1.forward(x)\n",
    "        #print('fc1 forward',x)\n",
    "        x1 = Sigma().forward(s1)\n",
    "        #print('sigma 1st time',x)\n",
    "        s2 = self.fc2.forward(x1)\n",
    "        #print('fc2 forward',x)\n",
    "        x2 = Sigma().forward(s2)\n",
    "        #print('sigma 2nd time',x)\n",
    "        s3 = self.fc3.forward(x2)\n",
    "        #print('fc3 forward',x)\n",
    "        x3 = Sigma().forward(s3)\n",
    "        return x3\n",
    "    \n",
    "    def backward(self, t, x3):  \n",
    "        # last layer\n",
    "        dl_ds3, w3 = self.fc3.backward_last(output = x3, target = t)\n",
    "        # previous layers\n",
    "        dl_ds2, w2 = self.fc2.backward(dl_ds_next = dl_ds3, w_next = w3)\n",
    "        dl_ds1, w1 = self.fc1.backward(dl_ds_next = dl_ds2, w_next = w2)\n",
    "        #print('dl_dw3',dl_dw3, 'dl_dw2', dl_dw2, 'dl_dw1', dl_dw1)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a network with 2 input units, 2 output units, 3 hidden layers with 25 units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# network parameters\n",
    "input_size = 2\n",
    "output_size = 2\n",
    "hidden_units = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 acc_train_loss 2000.00 acc_train_error 12.00% , magnitude x3 -5.3e-17\n",
      "epoch 1 acc_train_loss 2000.00 acc_train_error 12.00% , magnitude x3 -8.4e-17\n",
      "epoch 2 acc_train_loss 2000.00 acc_train_error 12.00% , magnitude x3 -1.2e-16\n",
      "epoch 3 acc_train_loss 2000.00 acc_train_error 12.00% , magnitude x3 -1.7e-16\n",
      "epoch 4 acc_train_loss 2000.00 acc_train_error 12.00% , magnitude x3 -2.5e-16\n",
      "epoch 5 acc_train_loss 2000.00 acc_train_error 12.00% , magnitude x3 -3.6e-16\n",
      "epoch 6 acc_train_loss 2000.00 acc_train_error 12.00% , magnitude x3 -5.4e-16\n",
      "epoch 7 acc_train_loss 2000.00 acc_train_error 12.00% , magnitude x3 -8.4e-16\n",
      "epoch 8 acc_train_loss 2000.00 acc_train_error 12.00% , magnitude x3 -1.4e-15\n",
      "epoch 9 acc_train_loss 2000.00 acc_train_error 12.00% , magnitude x3 -2.6e-15\n",
      "epoch 10 acc_train_loss 2000.00 acc_train_error 12.00% , magnitude x3 -5.2e-15\n",
      "epoch 11 acc_train_loss 2000.00 acc_train_error 12.00% , magnitude x3 -1.3e-14\n",
      "epoch 12 acc_train_loss 2000.00 acc_train_error 12.00% , magnitude x3 -3.9e-14\n",
      "epoch 13 acc_train_loss 2000.00 acc_train_error 12.00% , magnitude x3 -1.9e-13\n",
      "epoch 14 acc_train_loss 2000.00 acc_train_error 12.00% , magnitude x3 -1.8e-12\n",
      "epoch 15 acc_train_loss 2000.00 acc_train_error 12.00% , magnitude x3 -7.6e-11\n",
      "epoch 16 acc_train_loss 2000.00 acc_train_error 12.00% , magnitude x3 -6.7e-08\n",
      "epoch 17 acc_train_loss 1906.22 acc_train_error 12.00% , magnitude x3 -0.039\n",
      "epoch 18 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 19 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 20 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 21 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 22 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 23 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 24 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 25 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 26 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 27 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 28 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 29 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 30 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 31 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 32 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 33 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 34 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 35 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 36 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 37 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 38 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 39 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 40 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 41 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 42 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 43 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 44 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 45 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 46 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 47 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 48 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 49 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 50 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 51 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 52 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 53 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 54 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 55 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 56 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 57 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 58 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 59 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 60 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 61 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 62 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 63 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 64 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 65 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 66 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 67 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 68 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 69 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 70 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 71 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 72 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 73 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 74 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 75 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 76 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 77 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 78 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 79 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 80 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 81 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 82 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 83 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 84 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 85 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 86 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 87 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 88 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 89 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 90 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 91 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 92 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 93 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 94 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 95 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 96 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 97 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 98 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n",
      "epoch 99 acc_train_loss 960.00 acc_train_error 12.00% , magnitude x3 -1\n"
     ]
    }
   ],
   "source": [
    "# training set\n",
    "train_input = train_input\n",
    "train_target = train_target\n",
    "# network parameters\n",
    "model = Net()\n",
    "input_size = 2\n",
    "output_size = 2\n",
    "hidden_units = 25\n",
    "# training parameters\n",
    "lr = 10\n",
    "nb_epochs = 100\n",
    "\n",
    "for k in range(0, nb_epochs):\n",
    "\n",
    "    # Back-prop\n",
    "\n",
    "    acc_loss = 0\n",
    "    nb_train_errors = 0\n",
    "\n",
    "    model.fc1.dl_dw.zero_()\n",
    "    model.fc1.dl_db.zero_()\n",
    "    model.fc2.dl_dw.zero_()\n",
    "    model.fc2.dl_db.zero_()\n",
    "    model.fc3.dl_dw.zero_()\n",
    "    model.fc3.dl_db.zero_()\n",
    "    \n",
    "    #print('cycle',k,'before',model.fc1.dl_dw)\n",
    "    for n in range(0, train_input.size(0)):\n",
    "    #for n in range(0, 25):\n",
    "        x = train_input[n]\n",
    "        t = train_target[n]\n",
    "        x3 = model.forward(x)\n",
    "                       \n",
    "        pred = x3.max(0)[1][0]  # the result is the index (0 or 1) of the position where the max value is\n",
    "        targ = train_target[n,:].max(0)[1][0]\n",
    "        if targ != pred:\n",
    "            nb_train_errors = nb_train_errors + 1 \n",
    "            \n",
    "        #print('sample num',n,'x3',x3,'pred x3',pred,'targ',targ,'err',nb_train_errors)\n",
    "        \n",
    "        acc_loss += loss(x3, t)  \n",
    "        #if k > 35 : \n",
    "            #myloss = loss(x3,t)\n",
    "            #print('sample',n,'x3',x3,'target',t,'myloss',myloss,'acc_loss',acc_loss)\n",
    "        #acc_loss += (x3 - t).pow(2).sum()\n",
    "        \n",
    "        \n",
    "        model.backward(t, x3)\n",
    "    #print('dl_dw1',dl_dw1)\n",
    "    \n",
    "    # Gradient step\n",
    "    #print('cycle',k,'after',model.fc1.dl_dw)\n",
    "    model.fc1.weights = model.fc1.weights - lr * model.fc1.dl_dw\n",
    "    model.fc1.bias = model.fc1.bias - lr * model.fc1.dl_db\n",
    "    model.fc2.weights = model.fc2.weights - lr * model.fc2.dl_dw\n",
    "    model.fc2.bias = model.fc2.bias - lr * model.fc2.dl_db\n",
    "    model.fc3.weights = model.fc3.weights - lr * model.fc3.dl_dw\n",
    "    model.fc3.bias = model.fc3.bias - lr * model.fc3.dl_db\n",
    "    #print('cycle',k,'weights for layer 1 are', model.fc1.weights)\n",
    "    \n",
    "    print('epoch {:d} acc_train_loss {:.02f} acc_train_error {:.02f}% , magnitude x3 {:.2g}'.format(k,acc_loss,\n",
    "                                                                            (100 * nb_train_errors) / train_input.size(0) , x3[0]))\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"   \\n# Test error\\n\\n    nb_test_errors = 0\\n\\n    for n in range(0, test_input.size(0)):\\n        _, _, _, _, x3 = model.forward(x)\\n\\n        pred = x3.max(0)[1][0]\\n        if test_target[n, pred] < 0: nb_test_errors = nb_test_errors + 1\\n\\n    print('{:d} acc_train_loss {:.02f} acc_train_error {:.02f}% test_error {:.02f}%'\\n          .format(k,\\n                  acc_loss,\\n                  (100 * nb_train_errors) / train_input.size(0),\\n                  (100 * nb_test_errors) / test_input.size(0)))\\n\""
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"   \n",
    "# Test error\n",
    "\n",
    "    nb_test_errors = 0\n",
    "\n",
    "    for n in range(0, test_input.size(0)):\n",
    "        _, _, _, _, x3 = model.forward(x)\n",
    "\n",
    "        pred = x3.max(0)[1][0]\n",
    "        if test_target[n, pred] < 0: nb_test_errors = nb_test_errors + 1\n",
    "\n",
    "    print('{:d} acc_train_loss {:.02f} acc_train_error {:.02f}% test_error {:.02f}%'\n",
    "          .format(k,\n",
    "                  acc_loss,\n",
    "                  (100 * nb_train_errors) / train_input.size(0),\n",
    "                  (100 * nb_test_errors) / test_input.size(0)))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
