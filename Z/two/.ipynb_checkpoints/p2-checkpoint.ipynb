{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "#from torch import LongTensor\n",
    "import torch\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "#import baseline\n",
    "\n",
    "DEBUG = False\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3989422804014327"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/math.sqrt(2*math.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 2]) torch.Size([1000, 1])\n",
      "torch.Size([1000, 1])\n",
      "\n",
      " 0\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_disc_set(nb):\n",
    "    input_ = Tensor(nb, 2).uniform_(0,1)\n",
    "    disk_center = Tensor(nb, 2).fill_(0.5)\n",
    "    #ones_ = torch.ones(nb,2)\n",
    "    R = 1/math.sqrt(2*math.pi) # Radius of the disk\n",
    "    target = (R - (disk_center - input_).pow(2).sum(1).sqrt()  ).sign()#.long()\n",
    "    target.add_(1).div_(2) # to transform [-1,1] into [0,1]\n",
    "    #target = input.pow(2).sum(1).mul(-1).add(1 / 2/ math.pi).sign().add(1).div(2).long() # prof version\n",
    "    return input_, target\n",
    "\n",
    "\n",
    "train_input, train_target = generate_disc_set(1000)\n",
    "test_input, test_target = generate_disc_set(1000)\n",
    "\n",
    "\n",
    "\n",
    "# one hot?\n",
    "#one_hot_targets = np.eye(2)[train_target]\n",
    "#train_target = one_hot_targets\n",
    "#train_target = Tensor(train_target)\n",
    "\n",
    "\n",
    "mini_batch_size = 100\n",
    "print (train_input.size(), train_target.size())\n",
    "#print(train_input[0:10],train_target[0:10])\n",
    "print(train_target.size())\n",
    "#plt.plot(train_input.where())\n",
    "print(train_target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_ori  = train_target.clone()\n",
    "one_hot_targets = torch.cat((train_target,1-train_target),1)\n",
    "one_hot_targets[0]\n",
    "train_target = one_hot_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples:        torch.Size([1000, 2])\n",
      "#true_samples:   torch.Size([497, 2])\n",
      "#false_samples:  torch.Size([503, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       " 1.0607  1.4739\n",
       " 0.2844  1.2710\n",
       "-0.9604  1.2583\n",
       "       â‹®        \n",
       " 0.4195  0.3627\n",
       "-1.3550 -0.5029\n",
       "-0.7227  0.4231\n",
       "[torch.FloatTensor of size 1000x2]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAF1CAYAAADr6FECAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2a4ce7ac50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#third_tensor = torch.cat((first_tensor, second_tensor), 0)\n",
    "def plot_data(input_, target_, figure_size = 6):\n",
    "    input_true = torch.Tensor(0,2)\n",
    "    input_false = torch.Tensor(0,2)\n",
    "    for i,x in enumerate(input_):\n",
    "        if target_[i][0] == 0 :\n",
    "            input_false = torch.cat( (input_false, input_[i,:].view(-1,2)),0 )\n",
    "        else :\n",
    "            input_true = torch.cat( (input_true, input_[i,:].view(-1,2)),0 )\n",
    "    print ('#samples:       ',input_.size())\n",
    "    print ('#true_samples:  ',input_true.size())\n",
    "    print ('#false_samples: ',input_false.size())\n",
    "    p1 = plt.figure(1,figsize=(figure_size,figure_size))\n",
    "    plt.plot(input_true[:,0].numpy(),input_true[:,1].numpy(),'r.',label='1 label')\n",
    "    plt.plot(input_false[:,0].numpy(),input_false[:,1].numpy(),'b.',label='0 labe')\n",
    "    plt.xlim(0,1), plt.ylim(0,1)\n",
    "    plt.legend(fontsize='x-large')\n",
    "    plt.title('Distribution of generated data')\n",
    "plot_data(train_input, train_target)\n",
    "\n",
    "mu, std = train_input.mean(),train_input.std()\n",
    "train_input.sub_(mu).div_(std)\n",
    "mu, std = test_input.mean(),test_input.std()\n",
    "test_input.sub_(mu).div_(std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Between -1,1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target=(train_target*2-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6 %\n"
     ]
    }
   ],
   "source": [
    "import baseline\n",
    "baseline.baseline_linear_model(train_input, train_target_ori, test_input, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return x.tanh()\n",
    "\n",
    "def d_tanh(x):\n",
    "    return (x.exp() + x.mul(-1).exp()).pow(-2)\n",
    "\n",
    "def relu(x):\n",
    "    if x>0 : return x\n",
    "    else : return x.fill(0)\n",
    "    \n",
    "def d_relu(x):\n",
    "    if x>0: return x.fill(1)\n",
    "    else : return x.fill(0)\n",
    "\n",
    "def mse(x,t):\n",
    "    if DEBUG == True:\n",
    "        print('mse x.size',x.size())\n",
    "        print('mse target size',t.size())\n",
    "    return (x - t).pow(2).sum()\n",
    "\n",
    "#def dsigma_relu(x):\n",
    "\n",
    "sigma = tanh\n",
    "dsigma = d_tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xavier_init(weight): \n",
    "    size = weight.size()\n",
    "    std = np.sqrt(2.0/(size[0] + size[1]))\n",
    "    return weight.normal_(0.0, std)\n",
    "\n",
    "def standard_init(weight):\n",
    "    stdv = 1. / math.sqrt(weight.size(1)) / math.sqrt(3) \n",
    "    weight.uniform_(-stdv, stdv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### suggested structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module ( object ) :\n",
    "    def forward ( self , * input ) :\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward ( self , * gradwrtoutput ) :\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def param ( self ) :\n",
    "        return [] \n",
    "    \n",
    "    def zero_grads ( self ) :\n",
    "        pass\n",
    "    \n",
    "    def reset_params( self ) :\n",
    "        pass\n",
    "    \n",
    "    def update_params( self, eta ):\n",
    "        pass\n",
    "    \n",
    "\n",
    "class Tanh(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.params = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return 0.5*(1+x.tanh())\n",
    "    \n",
    "    def backward(self, d_dx):\n",
    "        return 0.5*d_dx*(1-torch.tanh(self.x)**2)\n",
    "    \n",
    "class Sigmoid(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return torch.sigmoid(x)\n",
    "    \n",
    "    def backward(self, d_dx):\n",
    "        return d_dx * (torch.sigmoid(self.x*(1-torch.sigmoid(self.x))))\n",
    "\n",
    "\"\"\"class Tanh(Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input_):\n",
    "        self.input = input_.clone()\n",
    "        return (input_.tanh()+1)/2\n",
    "    \n",
    "    def backward(self, d_output):\n",
    "        x = self.input\n",
    "        return (1 - self.input.tanh()**2) * d_output\"\"\"\n",
    "    \n",
    "class Relu (Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.params = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return x*(x>0).type(torch.FloatTensor)\n",
    "    \n",
    "    def backward(self, d_dx):\n",
    "        return d_dx*(self.x>0).type(torch.FloatTensor)\n",
    "        \n",
    "    \n",
    "class LossMSE(Module):\n",
    "    def __init__(self,p=2):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "    def forward(self,y,t):\n",
    "        self.y = y\n",
    "        self.t = t\n",
    "        return torch.dist(y, t, p=2)\n",
    "        \n",
    "    def backward(self):\n",
    "        #print (self.p)\n",
    "        return 2*(self.y-self.t)#self.p * torch.pow((self.x - self.t),self.p-1)\n",
    "    \n",
    "    \n",
    "    \n",
    "class Sequential(Module):\n",
    "\n",
    "    def __init__(self,modules, loss): \n",
    "        super().__init__()\n",
    "        self.modules = modules\n",
    "        self.loss = loss\n",
    "        self.params = []\n",
    "        self.param()\n",
    "        \n",
    "    #done by luca\n",
    "    def add(self, module):\n",
    "        self.modules.append(module)\n",
    "\n",
    "    def forward(self, input_): # , target\n",
    "        for module in self.modules:\n",
    "            input_ = module.forward(input_) # module.forward?\n",
    "        return input_ \n",
    "    \n",
    "    def backward(self):\n",
    "        out = self.loss.backward()\n",
    "        for x in reversed(self.modules):\n",
    "            out = x.backward(out)\n",
    "        return out\n",
    "    \n",
    "    def zero_grads(self):\n",
    "        for x in self.modules:\n",
    "            x.zero_grads()\n",
    "            \n",
    "    \"\"\"def param ( self ):\n",
    "        params = []\n",
    "        for x in self.modules:\n",
    "            params = params + x.param()\n",
    "        return params\"\"\"\n",
    "    def param(self):\n",
    "        self.params = []\n",
    "        for module in self.modules:\n",
    "            if(module.params):\n",
    "                self.params.append(module.param())\n",
    "            \n",
    "    def update_params( self, eta ):\n",
    "        for module in self.modules:\n",
    "            module.update_params(eta)\n",
    "            \n",
    "            \n",
    "            \n",
    "class Linear(Module):\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        # num features\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        # weigths\n",
    "        self.weights = torch.Tensor(out_features, in_features).normal_(0,1)\n",
    "        self.bias   = torch.Tensor(out_features).uniform_(0,0)\n",
    "        # gradients\n",
    "        self.dl_dw = torch.Tensor(out_features, in_features)\n",
    "        self.dl_db   = torch.Tensor(out_features)\n",
    "        self.zero_grads()\n",
    "        self.params = [(self.weights, self.bias),(self.dl_dw, self.dl_db)]\n",
    "        # initialize the parameters\n",
    "        #self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weights.size(1)) / math.sqrt(3) #\n",
    "        self.weights.uniform_(-stdv, stdv)\n",
    "        self.bias.uniform_(-stdv, stdv)\n",
    "        \n",
    "    def zero_grads(self):\n",
    "        \"\"\"resets the gradients of the module\"\"\"\n",
    "        self.dl_dw.zero_()\n",
    "        self.dl_db.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        if(x.size()[1]!=self.in_features):\n",
    "            raise TypeError('Size of x should correspond to size of linear module')\n",
    "        return torch.mm(x, self.weights.t()) + self.bias.expand(x.size(0),self.out_features)\n",
    "        \n",
    "    def backward ( self ,d_dx ) :\n",
    "        self.dl_db = torch.mean(d_dx,0)  \n",
    "        self.dl_dw = torch.mm(d_dx.t(), self.x)\n",
    "        dl_ds = torch.mm(d_dx,self.weights)\n",
    "        return dl_ds\n",
    "    \n",
    "    \"\"\"def param ( self ) :\n",
    "        return [[self.weight,self.dl_dws], [self.bias, self.dl_dbias]] \"\"\"\n",
    "    def param(self):\n",
    "        self.params = [(self.weights, self.bias),(self.dl_dw, self.dl_db)]\n",
    "        return self.params\n",
    "    \n",
    "    def update_params ( self, eta ):\n",
    "        self.weights.sub_(eta * self.dl_dw)\n",
    "        self.bias.sub_(eta * self.dl_db)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDOptimizer():\n",
    "    def __init__(self, Sequential, lr):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.Sequential = Sequential\n",
    "        self.lr = lr\n",
    "        \n",
    "    def step(self):\n",
    "        for param in self.Sequential.params:\n",
    "            param[0][0].add_(- self.lr * param[1][0])\n",
    "            param[0][1].add_(- self.lr * param[1][1])\n",
    "        \n",
    "        self.Sequential.zero_grads()\n",
    "\n",
    "class SGD(Module):\n",
    "    \n",
    "    def __init__(self, model_params, eta):\n",
    "        super().__init__()\n",
    "        self.model_params = model_params\n",
    "        self.eta = eta\n",
    "        \n",
    "    def step(self):\n",
    "        for x in self.model_params:\n",
    "            x[0].sub_(self.eta*x[1])\n",
    "            \n",
    "class SGD_mom(Module):\n",
    "    \n",
    "    def __init__(self, model_params, eta, gamma):\n",
    "        super().__init__()\n",
    "        self.model_params = model_params\n",
    "        self.eta = eta\n",
    "        self.gamma = gamma\n",
    "        self.u = []\n",
    "        self.count= 0\n",
    "        \n",
    "    def _initial_step(self):\n",
    "        self.count = 1\n",
    "        for i,x in enumerate(self.model_params):\n",
    "            self.u.append(self.eta*x[1])\n",
    "            x[0].sub_(self.u[i])\n",
    "        \n",
    "    def step(self):\n",
    "        if self.count == 0:\n",
    "            self._initial_step()\n",
    "        else:\n",
    "            for i,x in enumerate(self.model_params):\n",
    "                self.u[i] = self.gamma*self.u[i] + self.eta*x[1]\n",
    "                x[0].sub_(self.u[i])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "hidden = 20\n",
    "Lin1 = Linear (2,hidden)\n",
    "Lin2 = Linear (hidden,hidden)\n",
    "Lin3 = Linear (hidden,2)\n",
    "act1 = Relu()\n",
    "act2 = Relu()\n",
    "act3 = Tanh()\n",
    "act4 = Sigmoid()\n",
    "\n",
    "\n",
    "layers = [Lin1,act1,Lin3,act3]\n",
    "loss = LossMSE()\n",
    "# network parameters\n",
    "model = Sequential(modules = layers,loss = loss )\n",
    "\n",
    "# training parameters\n",
    "mini_batch_size = 50\n",
    "\n",
    "lr = 0.01\n",
    "momentum = 0.001\n",
    "#optimizer = SGD(model.param(), eta= lr)\n",
    "optimizer = SGDOptimizer(model, 0.01)\n",
    "#optimizer = SGD_mom(model.param(), eta= lr, gamma = momentum)\n",
    "nb_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 1 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 2 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 3 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 4 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 5 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 6 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 7 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 8 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 9 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 10 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 11 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 12 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 13 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 14 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 15 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 16 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 17 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 18 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 19 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 20 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 21 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 22 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 23 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 24 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 25 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 26 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 27 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 28 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 29 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 30 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 31 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 32 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 33 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 34 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 35 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 36 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 37 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 38 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 39 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 40 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 41 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 42 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 43 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 44 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 45 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 46 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 47 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 48 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 49 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 50 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 51 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 52 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 53 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 54 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 55 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 56 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 57 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 58 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 59 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 60 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 61 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 62 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 63 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 64 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 65 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 66 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 67 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 68 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 69 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 70 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 71 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 72 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 73 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 74 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 75 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 76 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 77 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 78 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 79 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 80 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 81 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 82 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 83 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 84 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 85 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 86 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 87 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 88 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 89 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 90 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 91 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 92 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 93 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 94 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 95 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 96 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 97 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 98 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 99 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 100 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 101 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 102 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 103 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 104 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 105 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 106 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 107 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 108 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 109 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 110 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 111 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 112 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 113 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 114 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 115 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 116 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 117 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 118 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 119 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 120 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 121 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 122 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 123 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 124 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 125 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 126 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 127 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 128 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 129 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 130 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 131 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 132 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 133 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 134 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 135 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 136 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 137 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 138 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 139 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 140 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 141 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 142 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 143 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 144 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 145 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 146 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 147 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 148 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 149 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 150 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 151 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 152 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 153 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 154 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 155 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 156 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 157 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 158 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 159 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 160 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 161 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 162 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 163 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 164 acc_train_loss 284.29 acc_train_error 50.80%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 165 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 166 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 167 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 168 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 169 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 170 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 171 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 172 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 173 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 174 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 175 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 176 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 177 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 178 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 179 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 180 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 181 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 182 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 183 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 184 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 185 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 186 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 187 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 188 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 189 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 190 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 191 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 192 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 193 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 194 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 195 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 196 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 197 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 198 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 199 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 200 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 201 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 202 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 203 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 204 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 205 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 206 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 207 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 208 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 209 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 210 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 211 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 212 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 213 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 214 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 215 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 216 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 217 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 218 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 219 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 220 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 221 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 222 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 223 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 224 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 225 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 226 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 227 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 228 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 229 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 230 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 231 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 232 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 233 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 234 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 235 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 236 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 237 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 238 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 239 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 240 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 241 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 242 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 243 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 244 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 245 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 246 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 247 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 248 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 249 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 250 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 251 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 252 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 253 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 254 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 255 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 256 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 257 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 258 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 259 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 260 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 261 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 262 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 263 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 264 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 265 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 266 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 267 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 268 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 269 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 270 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 271 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 272 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 273 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 274 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 275 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 276 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 277 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 278 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 279 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 280 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 281 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 282 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 283 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 284 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 285 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 286 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 287 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 288 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 289 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 290 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 291 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 292 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 293 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 294 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 295 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 296 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 297 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 298 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 299 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 300 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 301 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 302 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 303 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 304 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 305 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 306 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 307 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 308 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 309 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 310 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 311 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 312 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 313 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 314 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 315 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 316 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 317 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 318 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 319 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 320 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 321 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 322 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 323 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 324 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 325 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 326 acc_train_loss 284.29 acc_train_error 50.80%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 327 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 328 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 329 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 330 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 331 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 332 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 333 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 334 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 335 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 336 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 337 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 338 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 339 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 340 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 341 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 342 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 343 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 344 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 345 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 346 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 347 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 348 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 349 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 350 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 351 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 352 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 353 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 354 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 355 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 356 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 357 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 358 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 359 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 360 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 361 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 362 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 363 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 364 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 365 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 366 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 367 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 368 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 369 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 370 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 371 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 372 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 373 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 374 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 375 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 376 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 377 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 378 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 379 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 380 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 381 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 382 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 383 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 384 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 385 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 386 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 387 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 388 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 389 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 390 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 391 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 392 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 393 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 394 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 395 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 396 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 397 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 398 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 399 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 400 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 401 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 402 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 403 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 404 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 405 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 406 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 407 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 408 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 409 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 410 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 411 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 412 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 413 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 414 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 415 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 416 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 417 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 418 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 419 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 420 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 421 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 422 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 423 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 424 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 425 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 426 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 427 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 428 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 429 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 430 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 431 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 432 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 433 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 434 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 435 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 436 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 437 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 438 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 439 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 440 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 441 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 442 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 443 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 444 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 445 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 446 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 447 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 448 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 449 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 450 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 451 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 452 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 453 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 454 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 455 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 456 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 457 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 458 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 459 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 460 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 461 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 462 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 463 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 464 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 465 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 466 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 467 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 468 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 469 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 470 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 471 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 472 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 473 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 474 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 475 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 476 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 477 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 478 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 479 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 480 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 481 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 482 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 483 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 484 acc_train_loss 284.29 acc_train_error 50.80%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 485 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 486 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 487 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 488 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 489 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 490 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 491 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 492 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 493 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 494 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 495 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 496 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 497 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 498 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "epoch 499 acc_train_loss 284.29 acc_train_error 50.80%\n",
      "\n",
      "\n",
      "\n",
      " FINISHED!!! \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_errors = []\n",
    "loss_list = []\n",
    "\n",
    "for epoch in range(nb_epochs):\n",
    "    acc_loss = 0\n",
    "    nb_train_errors = 0\n",
    "    model.zero_grads()\n",
    "    \n",
    "    for b in range(0,train_input.size(0), mini_batch_size):\n",
    "        output = model.forward(train_input[b:b+mini_batch_size,:])\n",
    "        batch_loss = loss.forward(train_input[b:b+mini_batch_size,:],train_target[b:b+mini_batch_size,:])\n",
    "        acc_loss += batch_loss\n",
    "        ###################  update of weights\n",
    "        \n",
    "        model.backward()\n",
    "        optimizer.step()\n",
    "        #model.update_params(lr)\n",
    "        ###################  make predictions\n",
    "        max_pred, argmax_pred = torch.max(output,1)\n",
    "        max_target, argmax_target = torch.max(train_target[b:b+mini_batch_size],1)\n",
    "        nb_train_errors += torch.sum(argmax_target == argmax_pred)\n",
    "    ############# logg errors and print it\n",
    "    train_errors.append(nb_train_errors)\n",
    "    loss_list.append(acc_loss)\n",
    "    print('epoch {:d} acc_train_loss {:.02f} acc_train_error {:.02f}%'.format(epoch,acc_loss,\n",
    "                              (100 * nb_train_errors) / train_input.size(0)))\n",
    "print('\\n\\n\\n FINISHED!!! \\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEICAYAAABbOlNNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7eff4bdec4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEICAYAAABbOlNNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7eff4bda85c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_list)\n",
    "plt.title('Loss')\n",
    "plt.show()\n",
    "plt.plot(train_errors)\n",
    "plt.title('Train error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -6.2135  -3.6557\n",
      "  2.4919  17.0744\n",
      "-20.2456   0.1606\n",
      " -1.3837  -0.0463\n",
      "  0.1625   0.6191\n",
      " 14.8800  -4.2982\n",
      "  8.9325   0.5230\n",
      "  1.1019  -0.1001\n",
      "-12.7274  -3.1620\n",
      " -1.2513  -4.9012\n",
      " -6.7639   3.7345\n",
      "  6.6544  -7.9926\n",
      "  2.2174  -1.2183\n",
      "  4.1442  -0.0851\n",
      "  7.5383   0.9199\n",
      "  0.2062   6.4622\n",
      "  9.0093  -0.7424\n",
      "  0.3805  -0.3944\n",
      " -3.1593   5.6455\n",
      " -9.6319  -3.0651\n",
      "[torch.FloatTensor of size 20x2]\n",
      "\n",
      "\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "[torch.FloatTensor of size 20x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.param()[0][0])\n",
    "print(model.param()[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\n",
       "    -7.6250   -4.9432\n",
       "   -21.2920   -8.4659\n",
       "     4.9776   -0.3756\n",
       "   -20.2504  -10.3480\n",
       "    17.7162    1.1736\n",
       "   -19.6728   12.6312\n",
       "   -14.0407   -4.0811\n",
       "    -4.8308  -11.6672\n",
       "     9.9235   23.6013\n",
       "   -27.1013   -2.7988\n",
       "   -51.8492   32.5478\n",
       "   135.7014    5.9190\n",
       "   -20.1224   13.3914\n",
       "     2.0349   22.0325\n",
       "   -44.1463   17.0851\n",
       "  -100.6973  -22.7698\n",
       "    47.4853  -12.9090\n",
       "   -64.4381   38.7549\n",
       "   -98.8742  -53.8455\n",
       "     0.1425  -12.1399\n",
       "  [torch.FloatTensor of size 20x2], \n",
       "      0     0\n",
       "      0     0\n",
       "      0     0\n",
       "      0     0\n",
       "      0     0\n",
       "      0     0\n",
       "      0     0\n",
       "      0     0\n",
       "      0     0\n",
       "      0     0\n",
       "      0     0\n",
       "      0     0\n",
       "      0     0\n",
       "      0     0\n",
       "      0     0\n",
       "      0     0\n",
       "      0     0\n",
       "      0     0\n",
       "      0     0\n",
       "      0     0\n",
       "  [torch.FloatTensor of size 20x2]], [\n",
       "  -0.3604\n",
       "  -0.5856\n",
       "  -0.6024\n",
       "  -0.5608\n",
       "  -0.4740\n",
       "  -0.8237\n",
       "  -0.6027\n",
       "  -0.1783\n",
       "  -0.4973\n",
       "  -0.6575\n",
       "  -0.6659\n",
       "  -0.5848\n",
       "  -0.3573\n",
       "  -0.4927\n",
       "  -0.2568\n",
       "  -0.8237\n",
       "  -0.5793\n",
       "  -0.8929\n",
       "  -0.8529\n",
       "  -0.7207\n",
       "  [torch.FloatTensor of size 20], \n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "  [torch.FloatTensor of size 20]], [\n",
       "  \n",
       "  Columns 0 to 7 \n",
       "   -4.4151   5.3017  14.4080   0.1017   3.5791   2.1245   4.7096   2.8842\n",
       "   -2.3667  -8.6420 -31.5903 -10.1685  -0.7483  -0.8958 -17.7323 -11.8779\n",
       "   -2.6668   3.3140  37.8768  11.8308 -17.5431  -4.9965   7.2506   8.7701\n",
       "   -8.0147   2.2842  10.9644  -0.7341  -1.8080  -0.5518  -4.5632   1.4294\n",
       "    4.1007   1.0936  18.7900   9.8126  -9.8671  -4.5069   7.3378   6.9885\n",
       "  -10.1683  16.9439  67.5359   9.5841  -3.6393   8.0577  24.7381  12.9414\n",
       "   -2.3570  -3.0885 -14.8000  -4.4805   2.0635   0.0038  -9.1659  -4.2173\n",
       "   -3.0034   1.8279   3.0687  -1.5059   3.7172  -0.6416  -0.8441   1.5903\n",
       "    2.3026  11.0049  21.5727   0.6731  13.3663  11.5286  23.2959   4.7001\n",
       "   -9.1952   4.9031  14.4333  -1.6786   1.8627   3.4669  -0.5229   1.2035\n",
       "    7.1359   0.1352   0.3001   2.2812   1.2487   2.5762   9.6958  -0.0577\n",
       "   -3.1816 -10.3739 -24.0759  -9.4301 -12.9455  -4.9686 -26.3842  -7.1173\n",
       "    8.8292   0.5736  15.3007  18.9240   0.2653  -3.9822  19.2466   4.2669\n",
       "    0.5710  -0.9014   0.0361  -1.2110  -1.3869  -0.0863  -0.9129  -0.4393\n",
       "   14.6510   1.3949   8.5125  19.0461   6.5106  -1.7455  25.5598   4.7683\n",
       "   -0.4435   6.3690  22.3860  -3.0524   0.3149   8.8316  14.8082   0.4605\n",
       "   -2.9236  -4.8561 -18.1147 -11.5995  -3.6110  -0.0465 -17.1717  -4.9331\n",
       "    2.6390  -0.5009   2.8031   2.2561  -4.1588  -0.2436   2.3655   1.7441\n",
       "  -11.6094   9.2361  18.3491 -12.5975   2.6527  10.3279   1.5086   0.7331\n",
       "   -5.5772  -1.9196  -6.0561 -13.0375  -5.1610   2.4725 -13.2153  -4.4151\n",
       "  \n",
       "  Columns 8 to 15 \n",
       "   -0.7733   2.2575  -5.5436  -0.8214   5.1504  -2.0316 -12.0886   7.0571\n",
       "   17.6417   1.9077 -24.3980  11.3447 -16.1904  -3.1164  10.8742 -15.1090\n",
       "  -31.6072  -7.1148   7.9644 -15.4699  17.9316   5.4946  -0.5832   9.7024\n",
       "   -1.9643  -0.2581 -16.3804   0.1549   0.6388  -1.0791  -5.7250   2.4797\n",
       "  -20.1731  -7.0068  19.6936  -9.7844  10.3725   6.1440   4.3415   5.4921\n",
       "  -29.9241   7.7856   0.0719 -18.5642  33.2793  -4.8594 -33.4917  27.3368\n",
       "   10.1314   1.0067 -12.3905   5.6255  -9.2469  -1.4924   3.9894  -6.6219\n",
       "    3.8295  -0.7623  -6.3465   2.7083  -1.5355  -0.1886  -4.6066   1.8637\n",
       "   -1.2689  12.2644  15.6689  -5.2550  16.2452  -6.8042 -23.8564  15.3738\n",
       "    0.2676   4.7991 -19.0797  -1.0029   3.6196  -4.2501 -12.5655   5.2867\n",
       "   -5.7702   1.9543  25.0733  -4.2703   5.7697  -0.2100  -0.7996   2.2230\n",
       "    2.2700  -4.4000 -17.6269  11.3728 -17.4313   1.7497  23.3967 -17.5153\n",
       "  -18.2292  -5.6907  47.9070 -21.3862  15.9071   5.9575  -8.3498   8.5129\n",
       "   -4.2296  -0.5065   8.9175   0.9314  -0.0176   0.0498   0.4182  -0.3820\n",
       "  -12.5837  -3.6648  58.8467 -20.7612  15.4677   5.9000  -7.5285   9.1380\n",
       "   -9.8193   9.3457   4.2776  -2.0799  15.5710  -6.8851 -16.4275  10.6291\n",
       "    7.9495   0.3132 -17.7630  13.7614 -13.8898  -1.8382  12.1143 -10.2759\n",
       "   -7.7337  -0.8353   9.5890  -4.1418   2.9080   1.7299   4.8053   0.7146\n",
       "    8.8484  12.8101 -50.3028   9.9475   5.0774 -10.4715 -14.5673   7.7777\n",
       "    3.0926   3.3008 -22.9423  13.2939  -7.4531  -4.3876   4.8061  -5.6826\n",
       "  \n",
       "  Columns 16 to 19 \n",
       "  -19.4385  -5.9784   2.2533  -1.2232\n",
       "   15.6639  23.6663 -27.9786  -2.0886\n",
       "    3.0760 -16.5473  24.0420   2.7840\n",
       "   -6.4794  -0.5490  -7.6660   0.2644\n",
       "    9.3701 -11.4943  21.2349   4.4211\n",
       "  -51.4745 -36.2211  35.0509  -7.4700\n",
       "    6.6120   9.2507 -15.8707  -0.3925\n",
       "   -8.2373   0.6431  -5.3380   1.7775\n",
       "  -42.0570 -20.7989  23.7598  -8.1335\n",
       "  -19.3985  -5.0810  -6.0208  -3.5834\n",
       "    3.8461  -5.2228  18.1130  -2.4377\n",
       "   45.4547  24.4071 -26.2889   4.2411\n",
       "    4.3644 -19.4972  38.6400  -1.5656\n",
       "    9.7541   2.8491   3.9436   0.0834\n",
       "   -0.5021 -23.1107  43.0527  -1.8107\n",
       "  -27.1745  -8.5036  19.8763  -7.8106\n",
       "   19.9143  19.0774 -22.6232   2.1115\n",
       "    9.1350  -4.7552   8.1347   0.5618\n",
       "  -50.6781  -2.1993 -17.0934  -5.6982\n",
       "    7.2533  16.6551 -17.6320  -0.3847\n",
       "  [torch.FloatTensor of size 20x20], \n",
       "  \n",
       "  Columns 0 to 12 \n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "  \n",
       "  Columns 13 to 19 \n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "  [torch.FloatTensor of size 20x20]], [\n",
       "  -0.2356\n",
       "  -0.1929\n",
       "  -0.2172\n",
       "  -0.2633\n",
       "  -0.2644\n",
       "  -0.1564\n",
       "  -0.1660\n",
       "  -0.1829\n",
       "  -0.0549\n",
       "  -0.2721\n",
       "  -0.1829\n",
       "  -0.0612\n",
       "  -0.0584\n",
       "  -0.1735\n",
       "  -0.1553\n",
       "  -0.2475\n",
       "  -0.0451\n",
       "  -0.2624\n",
       "  -0.2704\n",
       "  -0.1137\n",
       "  [torch.FloatTensor of size 20], \n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "  [torch.FloatTensor of size 20]], [\n",
       "  \n",
       "  Columns 0 to 7 \n",
       "   -5.0685   5.9972   3.0630  12.6083   0.1204   3.4245   1.6493   7.2296\n",
       "   -4.5327  -0.8474  -3.7461  10.3793  -7.9849   2.1343 -13.5918  12.2265\n",
       "  \n",
       "  Columns 8 to 15 \n",
       "   -4.3448  -3.3751   6.7437   0.1486   6.0442  -0.0410  -6.2425   5.3975\n",
       "   -2.7844 -11.2225 -11.9644  18.8420  -7.5498  10.3131  10.7458 -12.2104\n",
       "  \n",
       "  Columns 16 to 19 \n",
       "    1.9975  -9.1478   9.1679   1.7911\n",
       "   21.3333  -2.4221 -13.6647  21.1745\n",
       "  [torch.FloatTensor of size 2x20], \n",
       "  \n",
       "  Columns 0 to 12 \n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "  \n",
       "  Columns 13 to 19 \n",
       "      0     0     0     0     0     0     0\n",
       "      0     0     0     0     0     0     0\n",
       "  [torch.FloatTensor of size 2x20]], [\n",
       "  -0.7429\n",
       "  -0.6696\n",
       "  [torch.FloatTensor of size 2], \n",
       "   0\n",
       "   0\n",
       "  [torch.FloatTensor of size 2]]]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## test as giorgia\n",
    "class Linear2(Module):\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        # num features\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        # weigths\n",
    "        self.weight = torch.Tensor(out_features, in_features).fill_(0.0)\n",
    "        self.bias   = torch.Tensor(out_features).fill_(0.0)\n",
    "        # gradients\n",
    "        self.dl_dws = torch.Tensor(out_features, in_features).fill_(1.0)\n",
    "        self.dl_dbias   = torch.Tensor(out_features).fill_(0.0)\n",
    "        self.params = [(self.weight,self.dl_dws), (self.bias, self.dl_dbias)] \n",
    "        if DEBUG == True:\n",
    "            print('weights size: ',self.weight.size())\n",
    "        # initialize the parameters\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1)) # *sqrt(3)?\n",
    "        self.weight.uniform_(-stdv, stdv)\n",
    "        self.bias.uniform_(0, 0)\n",
    "        \n",
    "    def zero_grads(self):\n",
    "        \"\"\"resets the gradients of the module\"\"\"\n",
    "        self.dl_dws.zero_()\n",
    "        self.dl_dbias.zero_()\n",
    "\n",
    "    def forward(self, input_):\n",
    "        self.input = input_\n",
    "        self.s = torch.mm(input_, self.weight.t())#.add(self.bias)\n",
    "        return torch.mm(input_, self.weight.t())\n",
    "        \n",
    "    def backward_last ( self ,d_output ) :\n",
    "        dl_ds = d_output\n",
    "        dl_dw = torch.mm(dl_ds.view(-1,1), self.input.view(-1,1).t())\n",
    "       \n",
    "        self.dl_dws.add_(dl_dw) \n",
    "        self.dl_dbias.add(d_output)\n",
    "        \n",
    "        #d_input = torch.mm(self.weight.t(), d_output.t())\n",
    "        return dl_ds, self.weight\n",
    "    \n",
    "    def backward(self, dl_ds_next):  \n",
    "        #dl_dx = self.weights.t().mv(dl_ds_next)   # the problem is here! \n",
    "        dl_dx = w_next.t().mv(dl_ds_next)\n",
    "        dl_ds = Sigma().backward(self.s)*dl_dx \n",
    "        self.dl_dw.add_(dl_ds.view(-1, 1).mm(self.input.view(1, -1)))   \n",
    "        self.dl_db.add_(dl_ds)\n",
    "        return dl_ds\n",
    "    \n",
    "    def backward ( self , dl_ds_next) :\n",
    "        dl_ds = dl_ds_next\n",
    "        dl_dw = dl_ds_next.view(-1,1).mm(self.input.view(-1,1).t())\n",
    "        \n",
    "        self.dl_dws.add_(dl_dw) \n",
    "        self.dl_dbias.add(d_output)\n",
    "        #d_input = torch.mm(self.weight.t(), d_output.t())\n",
    "        return dl_ds\n",
    "    \n",
    "    def param ( self ) :\n",
    "        return [[self.weight,self.dl_dws], [self.bias, self.dl_dbias]] \n",
    "    \n",
    "    def update_params ( self, eta ):\n",
    "        self.weight = self.weight - eta * self.dl_dws\n",
    "        self.bias   = self.bias - eta * self.dl_dbias\n",
    "    \n",
    "\n",
    "\n",
    "class MyNet(Module):\n",
    "\n",
    "    def __init__(self): # , loss\n",
    "        super().__init__()\n",
    "        self.fc1 = Linear2(2,100)\n",
    "        self.fc2 = Linear2(100,100)\n",
    "        self.fc3 = Linear2(100,2)\n",
    "        self.act1 = Tanh()\n",
    "        self.act2 = Tanh()\n",
    "        self.act3 = Tanh()\n",
    "        #self.loss = loss\n",
    "        \n",
    "    #done by luca\n",
    "    def add(self, module):\n",
    "        self.modules.append(module)\n",
    "\n",
    "    def forward(self, input_): # , target\n",
    "        \n",
    "        s1 = self.fc1.forward(input_)\n",
    "        x1 = self.act1.forward(s1)\n",
    "        \n",
    "        s2 = self.fc2.forward(x1)\n",
    "        x2 = self.act2.forward(s2)\n",
    "        \n",
    "        s3 = self.fc3.forward(x2)\n",
    "        x3 = self.act3.forward(s3)\n",
    "        \n",
    "        return x3\n",
    "    \n",
    "    def backward(self, out):\n",
    "        \n",
    "        sigma3_p = self.act3.backward(out)\n",
    "        dl_ds3 = self.fc3.backward_last(sigma3_p)\n",
    "        \n",
    "        sigma2_p = self.act2.backward(dl_ds3)\n",
    "        dl_ds2  = self.fc2.backward(sigma2_p)\n",
    "        \n",
    "        sigma1_p = self.act1.backward(dl_ds2)\n",
    "        dl_ds1 = self.fc1.backward(sigma1_p)\n",
    "    \n",
    "    def zero_grads(self):\n",
    "        self.fc1.zero_grads()\n",
    "        self.fc2.zero_grads()\n",
    "        self.fc3.zero_grads()\n",
    "        \n",
    "            \n",
    "    def param ( self ):\n",
    "        params = []\n",
    "        params = params + self.fc1.param() + self.fc2.param() + self.fc3.param()\n",
    "        return params\n",
    "            \n",
    "    def update_params( self, eta ):\n",
    "        self.fc1.update_params(eta)\n",
    "        self.fc2.update_params(eta)\n",
    "        self.fc3.update_params(eta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prof version\n",
    "\n",
    "######################################################################\n",
    "\n",
    "def loss(v, t):\n",
    "    return (v - t).pow(2).sum()\n",
    "\n",
    "def dloss(v, t):\n",
    "    return 2 * (v - t)\n",
    "\n",
    "######################################################################\n",
    "# from F\n",
    "class Model(Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5)\n",
    "        self.conv2 = nn.Conv2d(20, 20, 5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        return F.relu(self.conv2(x))\n",
    "\n",
    "def linear(input, weight, bias=None):\n",
    "    \n",
    "    output = input.matmul(weight.t())\n",
    "    if bias is not None:\n",
    "        output += bias\n",
    "    return output\n",
    "\n",
    "\n",
    "class Linear(Module):\n",
    "    \n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.linear(input, self.weight, self.bias)\n",
    "\n",
    "######################################################################\n",
    "################# A first model ######################################\n",
    "def linear_forward(x,w,b):\n",
    "    s = w.mv(x) + b\n",
    "    x = sigma(s)\n",
    "    return s,x\n",
    "\n",
    "def linear_backward(x0, x, dl_ds_prev, dl_dw, dl_db, initial = False):\n",
    "    \n",
    "    dl_dx = w.t().mv(dl_ds_prev)\n",
    "    dl_ds = dsigma(s1) * dl_dx   \n",
    "    dl_dw.add_(dl_ds.view(-1, 1).mm(x0.view(1, -1)))\n",
    "    dl_db.add_(dl_ds)\n",
    "\n",
    "def forward_pass(ws, bs, x, test = False):\n",
    "    x0 = x\n",
    "    \n",
    "    s1, x1 = linear_forward(x0,ws[0],bs[0])\n",
    "    s2, x2 = linear_forward(x1,ws[1],bs[1])\n",
    "    \n",
    "    xs = [x1, x2]\n",
    "    ss = [s1, s2]\n",
    "    if test: \n",
    "        return xs[-1]\n",
    "    return x0, xs, ss\n",
    "\n",
    "\n",
    "def backward_pass(ws, bs,\n",
    "                  t,\n",
    "                  x, xs, ss,\n",
    "                  dl_dws, dl_dbs):\n",
    "    x0 = x\n",
    "    \n",
    "    dl_dx2 = dloss(xs[-1], t)\n",
    "    dl_ds2 = dsigma(ss[2-1]) * dl_dx2\n",
    "    dl_dws[2-1].add_(dl_ds2.view(-1, 1).mm(xs[1-1].view(1, -1)))\n",
    "    dl_dbs[2-1].add_(dl_ds2)\n",
    "    \n",
    "    dl_dx1 = ws[2-1].t().mv(dl_ds2) # w2\n",
    "    dl_ds1 = dsigma(ss[1-1]) * dl_dx1   \n",
    "    \n",
    "    dl_dws[1-1].add_(dl_ds1.view(-1, 1).mm(x0.view(1, -1)))\n",
    "    dl_dbs[1-1].add_(dl_ds1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_input.size():  torch.Size([1000, 2])\n",
      "-6.00436487729894e-06\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "bool value of non-empty torch.ByteTensor objects is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-33190ad6df85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# check wether the target was 1 or -1 --> verify if positif\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0mnb_train_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb_train_errors\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# if == -1 lets say :p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0macc_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         raise RuntimeError(\"bool value of non-empty \" + torch.typename(self) +\n\u001b[0;32m--> 140\u001b[0;31m                            \" objects is ambiguous\")\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0m__nonzero__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: bool value of non-empty torch.ByteTensor objects is ambiguous"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "zeta = 0.9\n",
    "train_input = train_input\n",
    "test_input = test_input\n",
    "\n",
    "nb_hidden=50\n",
    "print ('train_input.size(): ', train_input.size())\n",
    "nb_classes = 2 #train_target.size(1) \n",
    "nb_train_samples = train_input.size(0)\n",
    "\n",
    "eta = 0.1 / train_target.size(0)\n",
    "\n",
    "eps = 1e-6\n",
    "\n",
    "# weights and biases\n",
    "w1 = Tensor(nb_hidden, train_input.size(1)).normal_(0,1)\n",
    "b1 = Tensor(nb_hidden).normal_(0,1)\n",
    "w2 = Tensor(nb_classes, nb_hidden).normal_(0,eps)\n",
    "b2 = Tensor(nb_classes).normal_(0,eps)\n",
    "\n",
    "# lists\n",
    "ws = [w1, w2]\n",
    "bs = [b1, b2]\n",
    "# derivatives of the losse wrt weights and biases\n",
    "dl_dws = []\n",
    "dl_dbs = []\n",
    "for w in ws:\n",
    "    dl_dws.append(Tensor(w.size()))\n",
    "for b in bs:\n",
    "    dl_dbs.append(Tensor(b.size()))\n",
    "\n",
    "\n",
    "epochs = 250\n",
    "for k in range (0,epochs):\n",
    "    \n",
    "    acc_loss = 0\n",
    "    nb_train_errors = 0\n",
    "    \n",
    "    # set the storage to 0\n",
    "    for i in range(0, len(dl_dws)):\n",
    "        dl_dws[i].zero_()\n",
    "        dl_dbs[i].zero_()\n",
    "    \n",
    "    # for each sample run forward and backward pass\n",
    "    for n in range(0, nb_train_samples):\n",
    "        \n",
    "        # run forward pass\n",
    "        x0, xs, ss = forward_pass(ws, bs, train_input[n])\n",
    "        \n",
    "        # prediction is the maximum predicted class\n",
    "        \n",
    "        predicted = xs[-1].max(dim = 0)[1] # dim is the axis, 1 for taking index, 0 to just select the value\n",
    "        pred = predicted [0]\n",
    "        #print(predicted)\n",
    "        pred = xs[-1][0]\n",
    "        #print(xs[-1])\n",
    "        print(pred)\n",
    "        # check wether the target was 1 or -1 --> verify if positif\n",
    "        if train_target[n] != int(pred) : \n",
    "            nb_train_errors = nb_train_errors + 1 # if == -1 lets say :p \n",
    "        acc_loss += loss(Tensor(1).fill_(pred), train_target[n])\n",
    "        #acc_loss += loss(pred, train_target[n])\n",
    "\n",
    "        # run backward pass\n",
    "        backward_pass(ws, bs,\n",
    "                      train_target[n],\n",
    "                      x0, xs, ss,\n",
    "                      dl_dws, dl_dbs)\n",
    "    \n",
    "    # Gradient step\n",
    "    for i in range(0, len(ws)):\n",
    "        ws[i] = ws[i] - eta * dl_dws[i]\n",
    "        bs[i] = bs[i] - eta * dl_dbs[i]\n",
    "\n",
    "    # Test error\n",
    "    nb_test_errors = 0\n",
    "    for n in range(0, test_input.size(0)):\n",
    "        output = forward_pass(ws, bs, test_input[n], test=True)\n",
    "\n",
    "        pred = output.max(0)[1][0]\n",
    "        if test_target[n] != int(output[0]) : nb_test_errors = nb_test_errors + 1  \n",
    "\n",
    "\n",
    "    print(k,' --> acc_train_loss {:.02f} acc_train_error {:.02f}% test_error {:.02f}%'\n",
    "          .format(acc_loss,\n",
    "                  (100 * nb_train_errors) / train_input.size(0),\n",
    "                  (100 * nb_test_errors) / test_input.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
