{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "#from torch import LongTensor\n",
    "import torch\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3989422804014327"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/math.sqrt(2*math.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 2]) torch.Size([1000, 1])\n",
      "0.133\n"
     ]
    }
   ],
   "source": [
    "def generate_disc_set(nb):\n",
    "    input = Tensor(nb, 2).uniform_(0,1)\n",
    "    R = 1/math.sqrt(2*math.pi) # Radius of the disk\n",
    "    target = (R - input.pow(2).sum(1).sqrt()).sign()#.long()\n",
    "    target.add_(1).div_(2) # to transform [-1,1] into [0,1]\n",
    "    #target = input.pow(2).sum(1).mul(-1).add(1 / 2/ math.pi).sign().add(1).div(2).long() # prof version\n",
    "    return input, target\n",
    "\n",
    "\n",
    "train_input, train_target = generate_disc_set(1000)\n",
    "test_input, test_target = generate_disc_set(1000)\n",
    "\n",
    "mu, std = train_input.mean(),train_input.std()\n",
    "train_input.sub_(mu).div_(std)\n",
    "mu, std = test_input.mean(),test_input.std()\n",
    "test_input.sub_(mu).div_(std)\n",
    "\n",
    "#train_input, train_target = Variable(train_input), Variable(train_target)\n",
    "#test_input, test_target = Variable(test_input), Variable(test_target)\n",
    "\n",
    "# one hot?\n",
    "#one_hot_targets = np.eye(2)[train_target]\n",
    "#train_target = one_hot_targets\n",
    "#train_target = Tensor(train_target)\n",
    "\n",
    "mini_batch_size = 100\n",
    "print (train_input.size(), train_target.size())\n",
    "#print(train_input[0:10],train_target[0:10])\n",
    "print(train_target.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9 %\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "def add_squared(data):\n",
    "    square = np.power(data,2)[:,0]+np.power(data,2)[:,1].reshape(1,-1)\n",
    "    square = np.transpose(square)\n",
    "    return np.append(data,square,axis=1)\n",
    "\n",
    "# train the model\n",
    "X = train_input.numpy()\n",
    "X = add_squared(X)\n",
    "Y = train_target.numpy()\n",
    "clf = linear_model.SGDClassifier(max_iter=5000)\n",
    "clf.fit(X, Y.ravel())\n",
    "\n",
    "# test the function\n",
    "test = test_input.numpy()\n",
    "test = add_squared(test)\n",
    "Y_test = test_target.numpy()\n",
    "nb_errors = 0\n",
    "for i,x in enumerate(clf.predict(test)):\n",
    "    if x!= Y_test[i]:\n",
    "        nb_errors += 1\n",
    "print(nb_errors/test.shape[0]*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return x.tanh()\n",
    "\n",
    "def d_tanh(x):\n",
    "    return (x.exp() + x.mul(-1).exp()).pow(-2)\n",
    "\n",
    "def relu(x):\n",
    "    if x>0 : return x\n",
    "    else : return x.fill(0)\n",
    "    \n",
    "def d_relu(x):\n",
    "    if x>o: return x.fill(1)\n",
    "    else : return x.fill(0)\n",
    "\n",
    "def mse(x,t):\n",
    "    return (x - t).pow(2).sum()\n",
    "\n",
    "#def dsigma_relu(x):\n",
    "\n",
    "sigma = tanh\n",
    "dsigma = d_tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### suggested structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Module ( object ) :\n",
    "    def forward ( self , * input ) :\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward ( self , * gradwrtoutput ) :\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def param ( self ) :\n",
    "        return [] \n",
    "    \n",
    "class Linear(Module):\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = torch.Tensor(out_features, in_features)\n",
    "        self.bias   = torch.Tensor(out_features)\n",
    "        \n",
    "        self.d_weight = torch.Tensor(out_features, in_features).fill_(0.0)\n",
    "        print(self.weight.size())\n",
    "        self.d_bias   = torch.Tensor(out_features).fill_(0.0)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1)) # *sqrt(3)?\n",
    "        self.weight.uniform_(-stdv, stdv)\n",
    "        self.bias.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = torch.mv(self.weight, input.view(-1))\n",
    "        #self.input = torch.add(self.input,self.bias)\n",
    "        return self.input\n",
    "        \n",
    "    def backward ( self ,d_output ) :\n",
    "        tmp = d_output.view(-1, 1) * (self.input.view(1, -1))\n",
    "        print(tmp.size())\n",
    "        self.d_weight.add_(tmp) \n",
    "        self.d_bias.add(d_output)\n",
    "        d_input = self.weight.t() * d_output #mv()\n",
    "        return d_input\n",
    "    \n",
    "   # self._gradient.add_(d_dy.view(-1,1)*self._input.view(1,-1))        \n",
    "    \n",
    "def backward_pass(w1, b1, w2, b2,\n",
    "              t,\n",
    "              x, s1, x1, s2, x2,\n",
    "              dl_dw1, dl_db1, dl_dw2, dl_db2):\n",
    "    x0 = x\n",
    "    dl_dx2 = dloss(x2, t)\n",
    "    dl_ds2 = dsigma(s2) * dl_dx2\n",
    "    dl_dx1 = w2.t().mv(dl_ds2)\n",
    "    dl_ds1 = dsigma(s1) * dl_dx1\n",
    "\n",
    "    dl_dw2.add_(dl_ds2.view(-1, 1).mm(x1.view(1, -1)))\n",
    "    dl_db2.add_(dl_ds2)\n",
    "    dl_dw1.add_(dl_ds1.view(-1, 1).mm(x0.view(1, -1)))\n",
    "    dl_db1.add_(dl_ds1)\n",
    "\n",
    "class Tanh(Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return self.input.tanh()\n",
    "    \n",
    "    def backward(self, d_output):\n",
    "        x = self.input\n",
    "        d_input = d_tanh(x) * d_output\n",
    "        return d_input\n",
    "     \n",
    "\n",
    "\n",
    "class LossMSE(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,x,t):\n",
    "        self.input=x\n",
    "        d_input = mse(self.input,t)\n",
    "        return d_input\n",
    "        \n",
    "    def backward(self):\n",
    "        d_output = 2 * self.input\n",
    "        return d_output\n",
    "    \n",
    "class Sequential(Module):\n",
    "\n",
    "    def __init__(self,modules):\n",
    "        super().__init__()\n",
    "        self.modules = modules\n",
    "        self.loss = LossMSE()\n",
    "        \n",
    "    \n",
    "    #done by luca\n",
    "    def add(self, module):\n",
    "        self.modules.append(module)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        for module in self.modules:\n",
    "            input = module.forward(input) # module.forward?\n",
    "        return input, self.loss.forward(input, target)\n",
    "    \n",
    "    def backward(self):\n",
    "        out = self.loss.backward()# arguments??\n",
    "        n = len(self.modules)-1 # -1 for the loss?\n",
    "        for i in range(0,n):\n",
    "            out = self.modules[n-i].backward(out)\n",
    "        return out\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 2])\n",
      "torch.Size([2, 20])\n"
     ]
    }
   ],
   "source": [
    "hidden = 20\n",
    "lay1 = Linear(2,hidden)\n",
    "lay2 = Linear(hidden,2)\n",
    "layers = [lay1,lay2]\n",
    "net = Sequential(modules = layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " 1.00000e-02 *\n",
       "  -6.2781\n",
       "   6.9773\n",
       " [torch.FloatTensor of size 2], 1.743702232837677)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.forward(Tensor([0.2 , 0.3]),Tensor([-1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "inconsistent tensor size at /opt/conda/conda-bld/libtorch_1493853448612/work/pytorch-0.1.12/torch/lib/TH/generic/THTensorMath.c:831",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3ff0dae4f603>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-7e9a5b183db2>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;31m# -1 for the loss?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-7e9a5b183db2>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, d_output)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_weight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_bias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0md_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0md_output\u001b[0m \u001b[0;31m#mv()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: inconsistent tensor size at /opt/conda/conda-bld/libtorch_1493853448612/work/pytorch-0.1.12/torch/lib/TH/generic/THTensorMath.c:831"
     ]
    }
   ],
   "source": [
    "net.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-69157f70c632>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-69157f70c632>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    class net(Module)\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "a = 2\n",
    "def f(x,a=2):\n",
    "    return a*x\n",
    "a=3\n",
    "f(5)\n",
    "\n",
    "class net(Module)\n",
    "    def __init__(self,module_list, loss):\n",
    "        \n",
    "        self.modules = module_list[0]\n",
    "        self.loss = loss\n",
    "    \n",
    "    def forward(self,x):\n",
    "        for module in self.modules:\n",
    "            x = module.forward(x)\n",
    "        \n",
    "    def backward(self):\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.00000e-02 *\n",
      "  3.5325\n",
      "  0.7400\n",
      "[torch.FloatTensor of size 2]\n",
      " \n",
      "1.00000e-02 *\n",
      "  1.7663\n",
      "  0.2467\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aaa = Tensor([2.0,3.0])\n",
    "m1 = Tanh()\n",
    "m2 = Tanh()\n",
    "m1.forward(aaa)\n",
    "m2.forward(aaa)\n",
    "\n",
    "print(m1.backward(aaa),d_tanh(aaa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "1.00000e-02 *\n",
       "  3.5325\n",
       "  0.7400\n",
       "[torch.FloatTensor of size 2]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.backward(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "backward() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0d8765ead659>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLossMSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maaa\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maaa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msigma_tanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maaa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: backward() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "aaa = Tensor([2.0,3.0])\n",
    "t = Tensor([0,0])\n",
    "m = LossMSE()\n",
    "m.forward(aaa,t)\n",
    "print(m.backward(aaa),sigma_tanh(aaa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/576169/understanding-python-super-with-init-methods for using super()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prof version\n",
    "\n",
    "######################################################################\n",
    "\n",
    "def loss(v, t):\n",
    "    return (v - t).pow(2).sum()\n",
    "\n",
    "def dloss(v, t):\n",
    "    return 2 * (v - t)\n",
    "\n",
    "######################################################################\n",
    "# from F\n",
    "class Model(Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5)\n",
    "        self.conv2 = nn.Conv2d(20, 20, 5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        return F.relu(self.conv2(x))\n",
    "\n",
    "def linear(input, weight, bias=None):\n",
    "    \n",
    "    output = input.matmul(weight.t())\n",
    "    if bias is not None:\n",
    "        output += bias\n",
    "    return output\n",
    "\n",
    "\n",
    "class Linear(Module):\n",
    "    \n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.linear(input, self.weight, self.bias)\n",
    "\n",
    "######################################################################\n",
    "################# A first model ######################################\n",
    "def linear_forward(x,w,b):\n",
    "    s = w.mv(x) + b\n",
    "    x = sigma(s)\n",
    "    return s,x\n",
    "\n",
    "def linear_backward(x0, x, dl_ds_prev, dl_dw, dl_db, initial = False):\n",
    "    \n",
    "    dl_dx = w.t().mv(dl_ds_prev)\n",
    "    dl_ds = dsigma(s1) * dl_dx   \n",
    "    dl_dw.add_(dl_ds.view(-1, 1).mm(x0.view(1, -1)))\n",
    "    dl_db.add_(dl_ds)\n",
    "\n",
    "def forward_pass(ws, bs, x, test = False):\n",
    "    x0 = x\n",
    "    \n",
    "    s1, x1 = linear_forward(x0,ws[0],bs[0])\n",
    "    s2, x2 = linear_forward(x1,ws[1],bs[1])\n",
    "    \n",
    "    xs = [x1, x2]\n",
    "    ss = [s1, s2]\n",
    "    if test: \n",
    "        return xs[-1]\n",
    "    return x0, xs, ss\n",
    "\n",
    "\n",
    "def backward_pass(ws, bs,\n",
    "                  t,\n",
    "                  x, xs, ss,\n",
    "                  dl_dws, dl_dbs):\n",
    "    x0 = x\n",
    "    \n",
    "    dl_dx2 = dloss(xs[-1], t)\n",
    "    dl_ds2 = dsigma(ss[2-1]) * dl_dx2\n",
    "    dl_dws[2-1].add_(dl_ds2.view(-1, 1).mm(xs[1-1].view(1, -1)))\n",
    "    dl_dbs[2-1].add_(dl_ds2)\n",
    "    \n",
    "    dl_dx1 = ws[2-1].t().mv(dl_ds2) # w2\n",
    "    dl_ds1 = dsigma(ss[1-1]) * dl_dx1   \n",
    "    \n",
    "    dl_dws[1-1].add_(dl_ds1.view(-1, 1).mm(x0.view(1, -1)))\n",
    "    dl_dbs[1-1].add_(dl_ds1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_input.size():  torch.Size([1000, 2])\n",
      "-1.0527702443141607e-06\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "bool value of non-empty torch.ByteTensor objects is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-33190ad6df85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# check wether the target was 1 or -1 --> verify if positif\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0mnb_train_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb_train_errors\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# if == -1 lets say :p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0macc_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         raise RuntimeError(\"bool value of non-empty \" + torch.typename(self) +\n\u001b[0;32m--> 140\u001b[0;31m                            \" objects is ambiguous\")\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0m__nonzero__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: bool value of non-empty torch.ByteTensor objects is ambiguous"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "zeta = 0.9\n",
    "train_input = train_input\n",
    "test_input = test_input\n",
    "\n",
    "nb_hidden=50\n",
    "print ('train_input.size(): ', train_input.size())\n",
    "nb_classes = 2 #train_target.size(1) \n",
    "nb_train_samples = train_input.size(0)\n",
    "\n",
    "eta = 0.1 / train_target.size(0)\n",
    "\n",
    "eps = 1e-6\n",
    "\n",
    "# weights and biases\n",
    "w1 = Tensor(nb_hidden, train_input.size(1)).normal_(0,1)\n",
    "b1 = Tensor(nb_hidden).normal_(0,1)\n",
    "w2 = Tensor(nb_classes, nb_hidden).normal_(0,eps)\n",
    "b2 = Tensor(nb_classes).normal_(0,eps)\n",
    "\n",
    "# lists\n",
    "ws = [w1, w2]\n",
    "bs = [b1, b2]\n",
    "# derivatives of the losse wrt weights and biases\n",
    "dl_dws = []\n",
    "dl_dbs = []\n",
    "for w in ws:\n",
    "    dl_dws.append(Tensor(w.size()))\n",
    "for b in bs:\n",
    "    dl_dbs.append(Tensor(b.size()))\n",
    "\n",
    "\n",
    "epochs = 250\n",
    "for k in range (0,epochs):\n",
    "    \n",
    "    acc_loss = 0\n",
    "    nb_train_errors = 0\n",
    "    \n",
    "    # set the storage to 0\n",
    "    for i in range(0, len(dl_dws)):\n",
    "        dl_dws[i].zero_()\n",
    "        dl_dbs[i].zero_()\n",
    "    \n",
    "    # for each sample run forward and backward pass\n",
    "    for n in range(0, nb_train_samples):\n",
    "        \n",
    "        # run forward pass\n",
    "        x0, xs, ss = forward_pass(ws, bs, train_input[n])\n",
    "        \n",
    "        # prediction is the maximum predicted class\n",
    "        \n",
    "        predicted = xs[-1].max(dim = 0)[1] # dim is the axis, 1 for taking index, 0 to just select the value\n",
    "        pred = predicted [0]\n",
    "        #print(predicted)\n",
    "        pred = xs[-1][0]\n",
    "        #print(xs[-1])\n",
    "        print(pred)\n",
    "        # check wether the target was 1 or -1 --> verify if positif\n",
    "        if train_target[n] != int(pred) : \n",
    "            nb_train_errors = nb_train_errors + 1 # if == -1 lets say :p \n",
    "        acc_loss += loss(Tensor(1).fill_(pred), train_target[n])\n",
    "        #acc_loss += loss(pred, train_target[n])\n",
    "\n",
    "        # run backward pass\n",
    "        backward_pass(ws, bs,\n",
    "                      train_target[n],\n",
    "                      x0, xs, ss,\n",
    "                      dl_dws, dl_dbs)\n",
    "    \n",
    "    # Gradient step\n",
    "    for i in range(0, len(ws)):\n",
    "        ws[i] = ws[i] - eta * dl_dws[i]\n",
    "        bs[i] = bs[i] - eta * dl_dbs[i]\n",
    "\n",
    "    # Test error\n",
    "    nb_test_errors = 0\n",
    "    for n in range(0, test_input.size(0)):\n",
    "        output = forward_pass(ws, bs, test_input[n], test=True)\n",
    "\n",
    "        pred = output.max(0)[1][0]\n",
    "        if test_target[n] != int(output[0]) : nb_test_errors = nb_test_errors + 1  \n",
    "\n",
    "\n",
    "    print(k,' --> acc_train_loss {:.02f} acc_train_error {:.02f}% test_error {:.02f}%'\n",
    "          .format(acc_loss,\n",
    "                  (100 * nb_train_errors) / train_input.size(0),\n",
    "                  (100 * nb_test_errors) / test_input.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
