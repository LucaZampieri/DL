{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: Variable containing:\n",
      " 0.6986\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "dx None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = Variable(Tensor([[0.4, 0.6],[0.2,0.3],[0.5, 0.6]]),requires_grad=True)\n",
    "t = Variable(torch.Tensor([[0,1],[1,0],[0,1]]))\n",
    "criterion = torch.nn.BCELoss()\n",
    "loss = criterion(x,t)\n",
    "print('loss:',loss)\n",
    "dx = loss.backward()\n",
    "print('dx',dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Minimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5e2ff3eb6dec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mBfgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMinimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"\"\"BFGS (Broyden-Fletcher-Goldfarb-Shanno) is one of the most well-knwon\n\u001b[1;32m      3\u001b[0m     \u001b[0mquasi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mNewton\u001b[0m \u001b[0mmethods\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mmain\u001b[0m \u001b[0midea\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mto\u001b[0m \u001b[0miteratively\u001b[0m \u001b[0mconstruct\u001b[0m \u001b[0man\u001b[0m \u001b[0mapproximate\u001b[0m \u001b[0minverse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mHessian\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m^\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0m_t\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mby\u001b[0m \u001b[0ma\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Minimizer' is not defined"
     ]
    }
   ],
   "source": [
    "class Bfgs(Minimizer):\n",
    "    \"\"\"BFGS (Broyden-Fletcher-Goldfarb-Shanno) is one of the most well-knwon\n",
    "    quasi-Newton methods. The main idea is to iteratively construct an approximate inverse\n",
    "    Hessian :math:`B^{-1}_t` by a rank-2 update:\n",
    "        .. math::\n",
    "            B^{-1}_{t+1} = B^{-1}_t + (1 + \\\\frac{y_t^TB^{-1}_ty_t}{y_t^Ts_t})\\\\frac{s_ts_t^T}{s_t^Ty_t} - \\\\frac{s_ty_t^TB^{-1}_t + B^{-1}_ty_ts_t^T}{s_t^Ty_t},\n",
    "    where :math:`y_t = f(\\\\theta_{t+1}) - f(\\\\theta_{t})` and :math:`s_t = \\\\theta_{t+1} - \\\\theta_t`.\n",
    "    The storage requirements for BFGS scale quadratically with the number of\n",
    "    variables. For detailed derivations, see [nocedal2006a]_, chapter 6.\n",
    "    .. [nocedal2006a]  Nocedal, J. and Wright, S. (2006),\n",
    "        Numerical Optimization, 2nd edition, Springer.\n",
    "    Attributes\n",
    "    ----------\n",
    "    wrt : array_like\n",
    "        Current solution to the problem. Can be given as a first argument to \\\n",
    "        ``.f`` and ``.fprime``.\n",
    "    f : Callable\n",
    "        The object function.\n",
    "    fprime : Callable\n",
    "        First derivative of the objective function. Returns an array of the \\\n",
    "        same shape as ``.wrt``.\n",
    "    initial_inv_hessian : array_like\n",
    "        The initial estimate of the approximiate Hessian.\n",
    "    line_search : LineSearch object.\n",
    "        Line search object to perform line searches with.\n",
    "    args : iterable\n",
    "        Iterator over arguments which ``fprime`` will be called with.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, wrt, f, fprime, initial_inv_hessian=None,\n",
    "                 line_search=None, args=None):\n",
    "        \"\"\"Create a BFGS object.\n",
    "        Parameters\n",
    "        ----------\n",
    "        wrt : array_like\n",
    "            Array that represents the solution. Will be operated upon in\n",
    "            place.  ``f`` and ``fprime`` should accept this array as a first argument.\n",
    "        f : callable\n",
    "            The objective function.\n",
    "        fprime : callable\n",
    "            Callable that given a solution vector as first parameter and *args\n",
    "            and **kwargs drawn from the iterations ``args`` returns a\n",
    "            search direction, such as a gradient.\n",
    "        initial_inv_hessian : array_like\n",
    "            The initial estimate of the approximiate Hessian.\n",
    "        line_search : LineSearch object.\n",
    "            Line search object to perform line searches with.\n",
    "        args : iterable\n",
    "            Iterator over arguments which ``fprime`` will be called with.\n",
    "        \"\"\"\n",
    "        super(Bfgs, self).__init__(wrt, args=args)\n",
    "        self.f = f\n",
    "        self.fprime = fprime\n",
    "        self.inv_hessian = initial_inv_hessian\n",
    "\n",
    "        if line_search is not None:\n",
    "            self.line_search = line_search\n",
    "        else:\n",
    "            self.line_search = WolfeLineSearch(wrt, self.f, self.fprime)\n",
    "\n",
    "    def set_from_info(self, info):\n",
    "        raise NotImplemented('nobody has found the time to implement this yet')\n",
    "\n",
    "    def extended_info(self, **kwargs):\n",
    "        raise NotImplemented('nobody has found the time to implement this yet')\n",
    "\n",
    "    def find_direction(self, grad_m1, grad, step, inv_hessian):\n",
    "        H = self.inv_hessian\n",
    "        grad_diff = grad - grad_m1\n",
    "        ys = np.inner(grad_diff, step)\n",
    "        Hy = np.dot(H, grad_diff)\n",
    "        yHy = np.inner(grad_diff, Hy)\n",
    "        H += (ys + yHy) * np.outer(step, step) / ys ** 2\n",
    "        H -= (np.outer(Hy, step) + np.outer(step, Hy)) / ys\n",
    "        direction = -np.dot(H, grad)\n",
    "        return direction, {'gradient_diff': grad_diff}\n",
    "\n",
    "    def __iter__(self):\n",
    "        args, kwargs = next(self.args)\n",
    "        grad = self.fprime(self.wrt, *args, **kwargs)\n",
    "        grad_m1 = scipy.zeros(grad.shape)\n",
    "\n",
    "        if self.inv_hessian is None:\n",
    "            self.inv_hessian = scipy.eye(grad.shape[0])\n",
    "\n",
    "        for i, (next_args, next_kwargs) in enumerate(self.args):\n",
    "            if i == 0:\n",
    "                direction, info = -grad, {}\n",
    "            else:\n",
    "                direction, info = self.find_direction(\n",
    "                    grad_m1, grad, step, self.inv_hessian)\n",
    "\n",
    "            if not is_nonzerofinite(direction):\n",
    "                # TODO: inform the user here.\n",
    "                break\n",
    "\n",
    "            step_length = self.line_search.search(\n",
    "                direction, None, args, kwargs)\n",
    "\n",
    "            if step_length != 0:\n",
    "                step = step_length * direction\n",
    "                self.wrt += step\n",
    "            else:\n",
    "                self.logfunc(\n",
    "                    {'message': 'step length is 0--need to bail out.'})\n",
    "                break\n",
    "\n",
    "            # Prepare everything for the next loop.\n",
    "            args, kwargs = next_args, next_kwargs\n",
    "            # TODO: not all line searches have .grad!\n",
    "            grad_m1[:], grad[:] = grad, self.line_search.grad\n",
    "\n",
    "            info.update({\n",
    "                'step_length': step_length,\n",
    "                'n_iter': i,\n",
    "                'args': args,\n",
    "                'kwargs': kwargs,\n",
    "            })\n",
    "yield info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.1.post2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "print (torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ciao \n",
      "    0\n",
      "    0\n",
      "    0\n",
      "  ⋮   \n",
      "    0\n",
      "    0\n",
      "    0\n",
      "[torch.FloatTensor of size 1000x1]\n",
      "\n",
      "\n",
      "   -1     1\n",
      "   -1     1\n",
      "   -1     1\n",
      "     ⋮      \n",
      "   -1     1\n",
      "   -1     1\n",
      "   -1     1\n",
      "[torch.FloatTensor of size 1000x2]\n",
      "\n",
      "ciao \n",
      "    1\n",
      "    0\n",
      "    0\n",
      "  ⋮   \n",
      "    1\n",
      "    0\n",
      "    0\n",
      "[torch.FloatTensor of size 1000x1]\n",
      "\n",
      "\n",
      "    1    -1\n",
      "   -1     1\n",
      "   -1     1\n",
      "     ⋮      \n",
      "    1    -1\n",
      "   -1     1\n",
      "   -1     1\n",
      "[torch.FloatTensor of size 1000x2]\n",
      "\n",
      "\n",
      "-1\n",
      " 1\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def disk(nb_points) :\n",
    "    radius = 1/np.sqrt(2*np.pi)\n",
    "    inp = Tensor(nb_points,2).uniform_(0,1)\n",
    "    ratio = torch.floor(torch.norm(inp,p=2,dim=1)/radius)\n",
    "    target = 1-torch.clamp(ratio,min=0,max=1)\n",
    "    t = Tensor(nb_points,2)\n",
    "    print('ciao',target)\n",
    "    for i in range(nb_points):\n",
    "        if target[i].numpy() == 0:\n",
    "            t[i,:] = Tensor([-1,1])\n",
    "        elif target[i].numpy() == 1:\n",
    "            t[i,:] = Tensor([1,-1])\n",
    "    print(t)    \n",
    "    return inp, t\n",
    "\n",
    "nb_points = 1000\n",
    "# create train set and respective labels\n",
    "train_input , train_target = disk(nb_points)\n",
    "# create test set and respective labels\n",
    "test_input , test_target = disk(nb_points)\n",
    "\n",
    "\"\"\"\n",
    "target=torch.zeros(train_target.shape[0],2)-1\n",
    "target[:,0][train_target==0]=1\n",
    "target[:,1][train_target==1]=1\n",
    "\"\"\"\n",
    "#print(train_input, train_target, test_input, test_target)\n",
    "\n",
    "# data normalization\n",
    "# , std_train = train_input.mean() , train_input.std()\n",
    "#train_input.sub_(mu_train).div_(std_train)\n",
    "#mu_test , std_test = test_input.mean() , test_input.std()\n",
    "#test_input.sub_(mu_test).div_(std_test)\n",
    "# later see whether it is convenient to normalize component-wise with broadcasting (pag 66 week5)\n",
    "\n",
    "# then do sanity check to see if the two classes are balanced\n",
    "print(train_target[-1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Module ( object ) :\n",
    "    def forward ( self , * input ) :\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward ( self , * gradwrtoutput ) :\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def param ( self ) :\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return x.tanh()\n",
    "\n",
    "def dtanh(x):\n",
    "    return 1-torch.tanh(x).pow(2)\n",
    "\n",
    "\n",
    "class Sigma(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,input):\n",
    "        return tanh(input)\n",
    "    \n",
    "    def backward(self, output):\n",
    "        #return dtanh(input)*output\n",
    "        return 4 * (output.exp() + output.mul(-1).exp()).pow(-2)\n",
    "    \n",
    "    # here you need to add \"def param\" too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return max(0,x)\n",
    "\n",
    "def drelu(x):\n",
    "    if x>0:\n",
    "        out = 1\n",
    "    elif x<0:\n",
    "        out = 0\n",
    "    return out\n",
    "\n",
    "class Relu(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,input):\n",
    "        self.input = input\n",
    "        return relu(input)\n",
    "    \n",
    "    def backward(self,output):\n",
    "        return drelu(output) #to check, probably incorrect\n",
    "    \n",
    "    # here you need to add \"def param\" too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(v, t):\n",
    "    return (v - t).pow(2).sum()\n",
    "\n",
    "def dloss(v, t):\n",
    "    return 2 * (v - t)\n",
    "\n",
    "class Loss(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        self.input = input\n",
    "        return loss(input, target)\n",
    "    \n",
    "    def backward(self, output):\n",
    "        return dloss(output,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# definition of Linear\n",
    "class Linear(Module):\n",
    "    # in_features: size of each input sample\n",
    "    # out_features: size of each output sample\n",
    "    # bias: If set to False, the layer will not learn an additive bias. Default: ``True``\n",
    "\n",
    "    # Attributes:\n",
    "    # weight: the learnable weights of the module of shape (out_features x in_features)`\n",
    "    # bias:   the learnable bias of the module of shape `(out_features)`\n",
    "\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weights = Tensor(out_features,in_features).normal_(0,1e-6)\n",
    "        self.bias = Tensor(out_features).zero_()\n",
    "        self.dl_dw = Tensor(out_features,in_features).zero_()\n",
    "        self.dl_db = Tensor(out_features).zero_()\n",
    "                \n",
    "    def forward(self,input):\n",
    "        self.input = input\n",
    "        #print('ecco i pesi:',self.weights)\n",
    "        self.s = torch.mv(self.weights,input)\n",
    "        return torch.mv(self.weights,input)\n",
    "    \n",
    "    def backward_last(self, output, target): # output would be x3 \n",
    "        dl_dx = dloss(output, target)\n",
    "        dl_ds = Sigma().backward(self.s)*dl_dx \n",
    "        self.dl_dw.add_(dl_ds.view(-1, 1).mm(self.input.view(1, -1)))  \n",
    "        self.dl_db.add_(dl_ds)\n",
    "        #print('last layer',self.dl_dw,'dl_ds', dl_ds)\n",
    "        return dl_ds, self.weights\n",
    "    \n",
    "    def backward(self, dl_ds_next, w_next):  \n",
    "        #dl_dx = self.weights.t().mv(dl_ds_next)   # the problem is here! \n",
    "        dl_dx = w_next.t().mv(dl_ds_next)\n",
    "        dl_ds = Sigma().backward(self.s)*dl_dx \n",
    "        self.dl_dw.add_(dl_ds.view(-1, 1).mm(self.input.view(1, -1)))   \n",
    "        self.dl_db.add_(dl_ds)\n",
    "        return dl_ds, self.weights      \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(Module):\n",
    "    def __init__(self):\n",
    "        self.fc1 = Linear(input_size,hidden_units)  # first hidden layer\n",
    "        self.fc2 = Linear(hidden_units,hidden_units) # second hidden layer\n",
    "        self.fc3 = Linear(hidden_units,output_size) # third hidden layer\n",
    "   \n",
    "    def forward(self,x):\n",
    "        s1 = self.fc1.forward(x)\n",
    "        #print('fc1 forward',x)\n",
    "        x1 = Sigma().forward(s1)\n",
    "        #print('sigma 1st time',x)\n",
    "        s2 = self.fc2.forward(x1)\n",
    "        #print('fc2 forward',x)\n",
    "        x2 = Sigma().forward(s2)\n",
    "        #print('sigma 2nd time',x)\n",
    "        s3 = self.fc3.forward(x2)\n",
    "        #print('fc3 forward',x)\n",
    "        x3 = Sigma().forward(s3)\n",
    "        return x3\n",
    "    \n",
    "    def backward(self, t, x3):  \n",
    "        # last layer\n",
    "        dl_ds3, w3 = self.fc3.backward_last(output = x3, target = t)\n",
    "        # previous layers\n",
    "        dl_ds2, w2 = self.fc2.backward(dl_ds_next = dl_ds3, w_next = w3)\n",
    "        dl_ds1, w1 = self.fc1.backward(dl_ds_next = dl_ds2, w_next = w2)\n",
    "        #print('dl_dw3',dl_dw3, 'dl_dw2', dl_dw2, 'dl_dw1', dl_dw1)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a network with 2 input units, 2 output units, 3 hidden layers with 25 units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# network parameters\n",
    "input_size = 2\n",
    "output_size = 2\n",
    "hidden_units = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-1\n",
       " 1\n",
       "[torch.FloatTensor of size 2]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 acc_train_loss 2000.00 acc_train_error 11.50% , magnitude x3 -2.4e-18\n",
      "epoch 1 acc_train_loss 2000.00 acc_train_error 11.50% , magnitude x3 -1.1e-17\n",
      "epoch 2 acc_train_loss 2000.00 acc_train_error 11.50% , magnitude x3 -2.1e-17\n",
      "epoch 3 acc_train_loss 2000.00 acc_train_error 11.50% , magnitude x3 -3.6e-17\n",
      "epoch 4 acc_train_loss 2000.00 acc_train_error 11.50% , magnitude x3 -5.7e-17\n",
      "epoch 5 acc_train_loss 2000.00 acc_train_error 11.50% , magnitude x3 -9e-17\n",
      "epoch 6 acc_train_loss 2000.00 acc_train_error 11.50% , magnitude x3 -1.4e-16\n",
      "epoch 7 acc_train_loss 2000.00 acc_train_error 11.50% , magnitude x3 -2.4e-16\n",
      "epoch 8 acc_train_loss 2000.00 acc_train_error 11.50% , magnitude x3 -4.2e-16\n",
      "epoch 9 acc_train_loss 2000.00 acc_train_error 11.50% , magnitude x3 -7.8e-16\n",
      "epoch 10 acc_train_loss 2000.00 acc_train_error 11.50% , magnitude x3 -1.7e-15\n",
      "epoch 11 acc_train_loss 2000.00 acc_train_error 11.50% , magnitude x3 -4.1e-15\n",
      "epoch 12 acc_train_loss 2000.00 acc_train_error 11.50% , magnitude x3 -1.3e-14\n",
      "epoch 13 acc_train_loss 2000.00 acc_train_error 11.50% , magnitude x3 -6.4e-14\n",
      "epoch 14 acc_train_loss 2000.00 acc_train_error 11.50% , magnitude x3 -6.5e-13\n",
      "epoch 15 acc_train_loss 2000.00 acc_train_error 11.50% , magnitude x3 -2.8e-11\n",
      "epoch 16 acc_train_loss 2000.00 acc_train_error 11.50% , magnitude x3 -2.6e-08\n",
      "epoch 17 acc_train_loss 1887.25 acc_train_error 11.50% , magnitude x3 -0.016\n",
      "epoch 18 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 19 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 20 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 21 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 22 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 23 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 24 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 25 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 26 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 27 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 28 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 29 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 30 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 31 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 32 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 33 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 34 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 35 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 36 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 37 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 38 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 39 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 40 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 41 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 42 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 43 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 44 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 45 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 46 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 47 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 48 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 49 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 50 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 51 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 52 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 53 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 54 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 55 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 56 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 57 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 58 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 59 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 60 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 61 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 62 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 63 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 64 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 65 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 66 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 67 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 68 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 69 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 70 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 71 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 72 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 73 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 74 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 75 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 76 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 77 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 78 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 79 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 80 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 81 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 82 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 83 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 84 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 85 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 86 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 87 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 88 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 89 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 90 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 91 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 92 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 93 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 94 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 95 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 96 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 97 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 98 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 99 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n"
     ]
    }
   ],
   "source": [
    "# training set\n",
    "train_input = train_input\n",
    "train_target = train_target\n",
    "# network parameters\n",
    "model = Net()\n",
    "input_size = 2\n",
    "output_size = 2\n",
    "hidden_units = 25\n",
    "# training parameters\n",
    "lr = 10\n",
    "nb_epochs = 100\n",
    "\n",
    "for k in range(0, nb_epochs):\n",
    "\n",
    "    # Back-prop\n",
    "\n",
    "    acc_loss = 0\n",
    "    nb_train_errors = 0\n",
    "\n",
    "    model.fc1.dl_dw.zero_()\n",
    "    model.fc1.dl_db.zero_()\n",
    "    model.fc2.dl_dw.zero_()\n",
    "    model.fc2.dl_db.zero_()\n",
    "    model.fc3.dl_dw.zero_()\n",
    "    model.fc3.dl_db.zero_()\n",
    "    \n",
    "    #print('cycle',k,'before',model.fc1.dl_dw)\n",
    "    for n in range(0, train_input.size(0)):\n",
    "    #for n in range(0, 25):\n",
    "        x = train_input[n]\n",
    "        t = train_target[n]\n",
    "        x3 = model.forward(x)\n",
    "                       \n",
    "        pred = x3.max(0)[1][0]  # the result is the index (0 or 1) of the position where the max value is\n",
    "        targ = train_target[n,:].max(0)[1][0]\n",
    "        if targ != pred:\n",
    "            nb_train_errors = nb_train_errors + 1 \n",
    "            \n",
    "        #print('sample num',n,'x3',x3,'pred x3',pred,'targ',targ,'err',nb_train_errors)\n",
    "        \n",
    "        acc_loss += loss(x3, t)  \n",
    "        #if k > 35 : \n",
    "            #myloss = loss(x3,t)\n",
    "            #print('sample',n,'x3',x3,'target',t,'myloss',myloss,'acc_loss',acc_loss)\n",
    "        #acc_loss += (x3 - t).pow(2).sum()\n",
    "        \n",
    "        \n",
    "        model.backward(t, x3)\n",
    "    #print('dl_dw1',dl_dw1)\n",
    "    \n",
    "    # Gradient step\n",
    "    #print('cycle',k,'after',model.fc1.dl_dw)\n",
    "    model.fc1.weights = model.fc1.weights - lr * model.fc1.dl_dw\n",
    "    model.fc1.bias = model.fc1.bias - lr * model.fc1.dl_db\n",
    "    model.fc2.weights = model.fc2.weights - lr * model.fc2.dl_dw\n",
    "    model.fc2.bias = model.fc2.bias - lr * model.fc2.dl_db\n",
    "    model.fc3.weights = model.fc3.weights - lr * model.fc3.dl_dw\n",
    "    model.fc3.bias = model.fc3.bias - lr * model.fc3.dl_db\n",
    "    #print('cycle',k,'weights for layer 1 are', model.fc1.weights)\n",
    "    \n",
    "    print('epoch {:d} acc_train_loss {:.02f} acc_train_error {:.02f}% , magnitude x3 {:.2g}'.format(k,acc_loss,\n",
    "                                                                            (100 * nb_train_errors) / train_input.size(0) , x3[0]))\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"   \\n# Test error\\n\\n    nb_test_errors = 0\\n\\n    for n in range(0, test_input.size(0)):\\n        _, _, _, _, x3 = model.forward(x)\\n\\n        pred = x3.max(0)[1][0]\\n        if test_target[n, pred] < 0: nb_test_errors = nb_test_errors + 1\\n\\n    print('{:d} acc_train_loss {:.02f} acc_train_error {:.02f}% test_error {:.02f}%'\\n          .format(k,\\n                  acc_loss,\\n                  (100 * nb_train_errors) / train_input.size(0),\\n                  (100 * nb_test_errors) / test_input.size(0)))\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"   \n",
    "# Test error\n",
    "\n",
    "    nb_test_errors = 0\n",
    "\n",
    "    for n in range(0, test_input.size(0)):\n",
    "        _, _, _, _, x3 = model.forward(x)\n",
    "\n",
    "        pred = x3.max(0)[1][0]\n",
    "        if test_target[n, pred] < 0: nb_test_errors = nb_test_errors + 1\n",
    "\n",
    "    print('{:d} acc_train_loss {:.02f} acc_train_error {:.02f}% test_error {:.02f}%'\n",
    "          .format(k,\n",
    "                  acc_loss,\n",
    "                  (100 * nb_train_errors) / train_input.size(0),\n",
    "                  (100 * nb_test_errors) / test_input.size(0)))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
