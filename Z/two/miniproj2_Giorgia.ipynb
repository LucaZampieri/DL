{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1.12\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "print (torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ciao \n",
      "    0\n",
      "    0\n",
      "    0\n",
      "  ⋮   \n",
      "    0\n",
      "    0\n",
      "    0\n",
      "[torch.FloatTensor of size 1000x1]\n",
      "\n",
      "\n",
      "   -1     1\n",
      "   -1     1\n",
      "   -1     1\n",
      "     ⋮      \n",
      "   -1     1\n",
      "   -1     1\n",
      "   -1     1\n",
      "[torch.FloatTensor of size 1000x2]\n",
      "\n",
      "ciao \n",
      "    1\n",
      "    0\n",
      "    0\n",
      "  ⋮   \n",
      "    1\n",
      "    0\n",
      "    0\n",
      "[torch.FloatTensor of size 1000x1]\n",
      "\n",
      "\n",
      "    1    -1\n",
      "   -1     1\n",
      "   -1     1\n",
      "     ⋮      \n",
      "    1    -1\n",
      "   -1     1\n",
      "   -1     1\n",
      "[torch.FloatTensor of size 1000x2]\n",
      "\n",
      "\n",
      "-1\n",
      " 1\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def disk(nb_points) :\n",
    "    radius = 1/np.sqrt(2*np.pi)\n",
    "    inp = Tensor(nb_points,2).uniform_(0,1)\n",
    "    ratio = torch.floor(torch.norm(inp,p=2,dim=1)/radius)\n",
    "    target = 1-torch.clamp(ratio,min=0,max=1)\n",
    "    t = Tensor(nb_points,2)\n",
    "    print('ciao',target)\n",
    "    for i in range(nb_points):\n",
    "        if target[i].numpy() == 0:\n",
    "            t[i,:] = Tensor([-1,1])\n",
    "        elif target[i].numpy() == 1:\n",
    "            t[i,:] = Tensor([1,-1])\n",
    "    print(t)    \n",
    "    return inp, t\n",
    "\n",
    "nb_points = 1000\n",
    "# create train set and respective labels\n",
    "train_input , train_target = disk(nb_points)\n",
    "# create test set and respective labels\n",
    "test_input , test_target = disk(nb_points)\n",
    "\n",
    "\"\"\"\n",
    "target=torch.zeros(train_target.shape[0],2)-1\n",
    "target[:,0][train_target==0]=1\n",
    "target[:,1][train_target==1]=1\n",
    "\"\"\"\n",
    "#print(train_input, train_target, test_input, test_target)\n",
    "\n",
    "# data normalization\n",
    "# , std_train = train_input.mean() , train_input.std()\n",
    "#train_input.sub_(mu_train).div_(std_train)\n",
    "#mu_test , std_test = test_input.mean() , test_input.std()\n",
    "#test_input.sub_(mu_test).div_(std_test)\n",
    "# later see whether it is convenient to normalize component-wise with broadcasting (pag 66 week5)\n",
    "\n",
    "# then do sanity check to see if the two classes are balanced\n",
    "print(train_target[-1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Module ( object ) :\n",
    "    def forward ( self , * input ) :\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward ( self , * gradwrtoutput ) :\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def param ( self ) :\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return x.tanh()\n",
    "\n",
    "def dtanh(x):\n",
    "    return 1-torch.tanh(x).pow(2)\n",
    "\n",
    "\n",
    "class Sigma(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,input):\n",
    "        return tanh(input)\n",
    "    \n",
    "    def backward(self, output):\n",
    "        #return dtanh(input)*output\n",
    "        return 4 * (output.exp() + output.mul(-1).exp()).pow(-2)\n",
    "    \n",
    "    # here you need to add \"def param\" too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return max(0,x)\n",
    "\n",
    "def drelu(x):\n",
    "    if x>0:\n",
    "        out = 1\n",
    "    elif x<0:\n",
    "        out = 0\n",
    "    return out\n",
    "\n",
    "class Relu(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,input):\n",
    "        self.input = input\n",
    "        return relu(input)\n",
    "    \n",
    "    def backward(self,output):\n",
    "        return drelu(output) #to check, probably incorrect\n",
    "    \n",
    "    # here you need to add \"def param\" too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(v, t):\n",
    "    return (v - t).pow(2).sum()\n",
    "\n",
    "def dloss(v, t):\n",
    "    return 2 * (v - t)\n",
    "\n",
    "class Loss(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        self.input = input\n",
    "        return loss(input, target)\n",
    "    \n",
    "    def backward(self, output):\n",
    "        return dloss(output,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# definition of Linear\n",
    "class Linear(Module):\n",
    "    # in_features: size of each input sample\n",
    "    # out_features: size of each output sample\n",
    "    # bias: If set to False, the layer will not learn an additive bias. Default: ``True``\n",
    "\n",
    "    # Attributes:\n",
    "    # weight: the learnable weights of the module of shape (out_features x in_features)`\n",
    "    # bias:   the learnable bias of the module of shape `(out_features)`\n",
    "\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weights = Tensor(out_features,in_features).normal_(0,1e-6)\n",
    "        self.bias = Tensor(out_features).zero_()\n",
    "        self.dl_dw = Tensor(out_features,in_features).zero_()\n",
    "        self.dl_db = Tensor(out_features).zero_()\n",
    "                \n",
    "    def forward(self,input):\n",
    "        self.input = input\n",
    "        #print('ecco i pesi:',self.weights)\n",
    "        self.s = torch.mv(self.weights,input)\n",
    "        return torch.mv(self.weights,input)\n",
    "    \n",
    "    def backward_last(self, output, target): # output would be x3 \n",
    "        dl_dx = dloss(output, target)\n",
    "        dl_ds = Sigma().backward(self.s)*dl_dx \n",
    "        self.dl_dw.add_(dl_ds.view(-1, 1).mm(self.input.view(1, -1)))  \n",
    "        self.dl_db.add_(dl_ds)\n",
    "        #print('last layer',self.dl_dw,'dl_ds', dl_ds)\n",
    "        return dl_ds, self.weights\n",
    "    \n",
    "    def backward(self, dl_ds_next, w_next):  \n",
    "        #dl_dx = self.weights.t().mv(dl_ds_next)   # the problem is here! \n",
    "        dl_dx = w_next.t().mv(dl_ds_next)\n",
    "        dl_ds = Sigma().backward(self.s)*dl_dx \n",
    "        self.dl_dw.add_(dl_ds.view(-1, 1).mm(self.input.view(1, -1)))   \n",
    "        self.dl_db.add_(dl_ds)\n",
    "        return dl_ds, self.weights      \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(Module):\n",
    "    def __init__(self):\n",
    "        self.fc1 = Linear(input_size,hidden_units)  # first hidden layer\n",
    "        self.fc2 = Linear(hidden_units,hidden_units) # second hidden layer\n",
    "        self.fc3 = Linear(hidden_units,output_size) # third hidden layer\n",
    "   \n",
    "    def forward(self,x):\n",
    "        s1 = self.fc1.forward(x)\n",
    "        #print('fc1 forward',x)\n",
    "        x1 = Sigma().forward(s1)\n",
    "        #print('sigma 1st time',x)\n",
    "        s2 = self.fc2.forward(x1)\n",
    "        #print('fc2 forward',x)\n",
    "        x2 = Sigma().forward(s2)\n",
    "        #print('sigma 2nd time',x)\n",
    "        s3 = self.fc3.forward(x2)\n",
    "        #print('fc3 forward',x)\n",
    "        x3 = Sigma().forward(s3)\n",
    "        return x3\n",
    "    \n",
    "    def backward(self, t, x3):  \n",
    "        # last layer\n",
    "        dl_ds3, w3 = self.fc3.backward_last(output = x3, target = t)\n",
    "        # previous layers\n",
    "        dl_ds2, w2 = self.fc2.backward(dl_ds_next = dl_ds3, w_next = w3)\n",
    "        dl_ds1, w1 = self.fc1.backward(dl_ds_next = dl_ds2, w_next = w2)\n",
    "        #print('dl_dw3',dl_dw3, 'dl_dw2', dl_dw2, 'dl_dw1', dl_dw1)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a network with 2 input units, 2 output units, 3 hidden layers with 25 units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# network parameters\n",
    "input_size = 2\n",
    "output_size = 2\n",
    "hidden_units = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-1\n",
       " 1\n",
       "[torch.FloatTensor of size 2]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 acc_train_loss 2000.00 acc_train_error 11.50% , magnitude x3 -2.4e-18\n",
      "epoch 1 acc_train_loss 2000.00 acc_train_error 11.50% , magnitude x3 -1.1e-17\n",
      "epoch 2 acc_train_loss 2000.00 acc_train_error 11.50% , magnitude x3 -2.1e-17\n",
      "epoch 3 acc_train_loss 2000.00 acc_train_error 11.50% , magnitude x3 -3.6e-17\n",
      "epoch 4 acc_train_loss 2000.00 acc_train_error 11.50% , magnitude x3 -5.7e-17\n",
      "epoch 5 acc_train_loss 2000.00 acc_train_error 11.50% , magnitude x3 -9e-17\n",
      "epoch 6 acc_train_loss 2000.00 acc_train_error 11.50% , magnitude x3 -1.4e-16\n",
      "epoch 7 acc_train_loss 2000.00 acc_train_error 11.50% , magnitude x3 -2.4e-16\n",
      "epoch 8 acc_train_loss 2000.00 acc_train_error 11.50% , magnitude x3 -4.2e-16\n",
      "epoch 9 acc_train_loss 2000.00 acc_train_error 11.50% , magnitude x3 -7.8e-16\n",
      "epoch 10 acc_train_loss 2000.00 acc_train_error 11.50% , magnitude x3 -1.7e-15\n",
      "epoch 11 acc_train_loss 2000.00 acc_train_error 11.50% , magnitude x3 -4.1e-15\n",
      "epoch 12 acc_train_loss 2000.00 acc_train_error 11.50% , magnitude x3 -1.3e-14\n",
      "epoch 13 acc_train_loss 2000.00 acc_train_error 11.50% , magnitude x3 -6.4e-14\n",
      "epoch 14 acc_train_loss 2000.00 acc_train_error 11.50% , magnitude x3 -6.5e-13\n",
      "epoch 15 acc_train_loss 2000.00 acc_train_error 11.50% , magnitude x3 -2.8e-11\n",
      "epoch 16 acc_train_loss 2000.00 acc_train_error 11.50% , magnitude x3 -2.6e-08\n",
      "epoch 17 acc_train_loss 1887.25 acc_train_error 11.50% , magnitude x3 -0.016\n",
      "epoch 18 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 19 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 20 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 21 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 22 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 23 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 24 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 25 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 26 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 27 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 28 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 29 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 30 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 31 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 32 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 33 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 34 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 35 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 36 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 37 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 38 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 39 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 40 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 41 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 42 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 43 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 44 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 45 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 46 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 47 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 48 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 49 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 50 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 51 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 52 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 53 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 54 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 55 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 56 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 57 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 58 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 59 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 60 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 61 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 62 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 63 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 64 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 65 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 66 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 67 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 68 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 69 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 70 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 71 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 72 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 73 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 74 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 75 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 76 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 77 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 78 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 79 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 80 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 81 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 82 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 83 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 84 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 85 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 86 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 87 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 88 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 89 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 90 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 91 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 92 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 93 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 94 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 95 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 96 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 97 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 98 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n",
      "epoch 99 acc_train_loss 920.00 acc_train_error 11.50% , magnitude x3 -1\n"
     ]
    }
   ],
   "source": [
    "# training set\n",
    "train_input = train_input\n",
    "train_target = train_target\n",
    "# network parameters\n",
    "model = Net()\n",
    "input_size = 2\n",
    "output_size = 2\n",
    "hidden_units = 25\n",
    "# training parameters\n",
    "lr = 10\n",
    "nb_epochs = 100\n",
    "\n",
    "for k in range(0, nb_epochs):\n",
    "\n",
    "    # Back-prop\n",
    "\n",
    "    acc_loss = 0\n",
    "    nb_train_errors = 0\n",
    "\n",
    "    model.fc1.dl_dw.zero_()\n",
    "    model.fc1.dl_db.zero_()\n",
    "    model.fc2.dl_dw.zero_()\n",
    "    model.fc2.dl_db.zero_()\n",
    "    model.fc3.dl_dw.zero_()\n",
    "    model.fc3.dl_db.zero_()\n",
    "    \n",
    "    #print('cycle',k,'before',model.fc1.dl_dw)\n",
    "    for n in range(0, train_input.size(0)):\n",
    "    #for n in range(0, 25):\n",
    "        x = train_input[n]\n",
    "        t = train_target[n]\n",
    "        x3 = model.forward(x)\n",
    "                       \n",
    "        pred = x3.max(0)[1][0]  # the result is the index (0 or 1) of the position where the max value is\n",
    "        targ = train_target[n,:].max(0)[1][0]\n",
    "        if targ != pred:\n",
    "            nb_train_errors = nb_train_errors + 1 \n",
    "            \n",
    "        #print('sample num',n,'x3',x3,'pred x3',pred,'targ',targ,'err',nb_train_errors)\n",
    "        \n",
    "        acc_loss += loss(x3, t)  \n",
    "        #if k > 35 : \n",
    "            #myloss = loss(x3,t)\n",
    "            #print('sample',n,'x3',x3,'target',t,'myloss',myloss,'acc_loss',acc_loss)\n",
    "        #acc_loss += (x3 - t).pow(2).sum()\n",
    "        \n",
    "        \n",
    "        model.backward(t, x3)\n",
    "    #print('dl_dw1',dl_dw1)\n",
    "    \n",
    "    # Gradient step\n",
    "    #print('cycle',k,'after',model.fc1.dl_dw)\n",
    "    model.fc1.weights = model.fc1.weights - lr * model.fc1.dl_dw\n",
    "    model.fc1.bias = model.fc1.bias - lr * model.fc1.dl_db\n",
    "    model.fc2.weights = model.fc2.weights - lr * model.fc2.dl_dw\n",
    "    model.fc2.bias = model.fc2.bias - lr * model.fc2.dl_db\n",
    "    model.fc3.weights = model.fc3.weights - lr * model.fc3.dl_dw\n",
    "    model.fc3.bias = model.fc3.bias - lr * model.fc3.dl_db\n",
    "    #print('cycle',k,'weights for layer 1 are', model.fc1.weights)\n",
    "    \n",
    "    print('epoch {:d} acc_train_loss {:.02f} acc_train_error {:.02f}% , magnitude x3 {:.2g}'.format(k,acc_loss,\n",
    "                                                                            (100 * nb_train_errors) / train_input.size(0) , x3[0]))\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"   \\n# Test error\\n\\n    nb_test_errors = 0\\n\\n    for n in range(0, test_input.size(0)):\\n        _, _, _, _, x3 = model.forward(x)\\n\\n        pred = x3.max(0)[1][0]\\n        if test_target[n, pred] < 0: nb_test_errors = nb_test_errors + 1\\n\\n    print('{:d} acc_train_loss {:.02f} acc_train_error {:.02f}% test_error {:.02f}%'\\n          .format(k,\\n                  acc_loss,\\n                  (100 * nb_train_errors) / train_input.size(0),\\n                  (100 * nb_test_errors) / test_input.size(0)))\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"   \n",
    "# Test error\n",
    "\n",
    "    nb_test_errors = 0\n",
    "\n",
    "    for n in range(0, test_input.size(0)):\n",
    "        _, _, _, _, x3 = model.forward(x)\n",
    "\n",
    "        pred = x3.max(0)[1][0]\n",
    "        if test_target[n, pred] < 0: nb_test_errors = nb_test_errors + 1\n",
    "\n",
    "    print('{:d} acc_train_loss {:.02f} acc_train_error {:.02f}% test_error {:.02f}%'\n",
    "          .format(k,\n",
    "                  acc_loss,\n",
    "                  (100 * nb_train_errors) / train_input.size(0),\n",
    "                  (100 * nb_test_errors) / test_input.size(0)))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
